nohup: ignoring input
W0127 03:21:02.631000 3622551 site-packages/torch/distributed/run.py:793] 
W0127 03:21:02.631000 3622551 site-packages/torch/distributed/run.py:793] *****************************************
W0127 03:21:02.631000 3622551 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0127 03:21:02.631000 3622551 site-packages/torch/distributed/run.py:793] *****************************************
Config (path: configs/260124/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': None, 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 1000, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 2, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 200, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260124/flux2klein_fulltune_5000.py'}
Config (path: configs/260124/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': None, 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 1000, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 2, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 200, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260124/flux2klein_fulltune_5000.py'}Config (path: configs/260124/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': None, 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 1000, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 2, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 200, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260124/flux2klein_fulltune_5000.py'}

Config (path: configs/260124/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': None, 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 1000, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 2, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 200, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260124/flux2klein_fulltune_5000.py'}
01/27/2026 03:21:10 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/27/2026 03:21:10 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/27/2026 03:21:10 - INFO - __main__ - [INFO] Using model type: Flux2Klein
01/27/2026 03:21:10 - INFO - __main__ - [INFO] Skipping text encoder load (parquet dataset + DeepSpeed mode)
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory - ============================================================
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory - 🏭 Model Factory Initialized
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Model Type: Flux2Klein
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Pretrained Path: black-forest-labs/FLUX.2-klein-base-9B
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Cache Dir: None
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    VAE Class: AutoencoderKLFlux2
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Transformer Class: Flux2Transformer2DModel
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Text Encoder Class: Qwen3ForCausalLM
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory -    Pipeline Class: Flux2KleinPipeline
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory - ============================================================
01/27/2026 03:21:10 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading tokenizer: Qwen2TokenizerFast
01/27/2026 03:21:10 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/27/2026 03:21:10 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/27/2026 03:21:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading text encoder: Qwen3ForCausalLM
01/27/2026 03:21:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Skipping text encoder load (parquet dataset mode)
01/27/2026 03:21:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading VAE: AutoencoderKLFlux2
All model checkpoint weights were used when initializing AutoencoderKLFlux2.

All the weights of AutoencoderKLFlux2 were initialized from the model checkpoint at black-forest-labs/FLUX.2-klein-base-9B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKLFlux2 for predictions without further training.
01/27/2026 03:21:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading transformer: Flux2Transformer2DModel
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 33554.43it/s]
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 15224.33it/s]
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 5919.98it/s]
Instantiating Flux2Transformer2DModel model under default dtype torch.bfloat16.
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 26546.23it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.25it/s]
All model checkpoint weights were used when initializing Flux2Transformer2DModel.

All the weights of Flux2Transformer2DModel were initialized from the model checkpoint at black-forest-labs/FLUX.2-klein-base-9B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Flux2Transformer2DModel for predictions without further training.
01/27/2026 03:21:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] Fine-tuning the full model ...
01/27/2026 03:21:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] Enabling gradient checkpointing for transformer
01/27/2026 03:21:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading scheduler: FlowMatchEulerDiscreteScheduler
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:369: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:370: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:369: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:370: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:369: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:370: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
01/27/2026 03:21:13 - INFO - OpenSciDraw.utils.model_factory - [INFO] VAE scale factor: 16
01/27/2026 03:21:13 - INFO - __main__ - [INFO] DeepSpeed detected - keeping transformer in bf16 for ZeRO-3
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:369: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:370: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
01/27/2026 03:21:13 - INFO - __main__ - [INFO] Configuring model devices and offloading
01/27/2026 03:21:13 - INFO - __main__ - [INFO] Using parquet dataset - VAE and text encoder remain on CPU
01/27/2026 03:21:13 - INFO - __main__ - [INFO] DeepSpeed mode: transformer stays on CPU, ZeRO-3 will handle placement
01/27/2026 03:21:13 - INFO - __main__ - [INFO] Gradient checkpointing enabled
01/27/2026 03:21:13 - INFO - __main__ - [INFO] Number of trainable parameters: 9078.58M
01/27/2026 03:21:13 - INFO - __main__ - [INFO] Loading dataset
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...


🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...
⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...

⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:01, 46.64it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:01, 46.11it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:01, 46.04it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:01, 46.20it/s]Scanning Parquet Files:  12%|█▏        | 10/84 [00:00<00:01, 47.92it/s]Scanning Parquet Files:  13%|█▎        | 11/84 [00:00<00:01, 49.94it/s]Scanning Parquet Files:  13%|█▎        | 11/84 [00:00<00:01, 49.47it/s]Scanning Parquet Files:  13%|█▎        | 11/84 [00:00<00:01, 49.48it/s]Scanning Parquet Files:  18%|█▊        | 15/84 [00:00<00:01, 47.85it/s]Scanning Parquet Files:  20%|██        | 17/84 [00:00<00:01, 52.80it/s]Scanning Parquet Files:  20%|██        | 17/84 [00:00<00:01, 52.16it/s]Scanning Parquet Files:  20%|██        | 17/84 [00:00<00:01, 52.21it/s]Scanning Parquet Files:  27%|██▋       | 23/84 [00:00<00:01, 54.69it/s]Scanning Parquet Files:  27%|██▋       | 23/84 [00:00<00:01, 54.45it/s]Scanning Parquet Files:  26%|██▌       | 22/84 [00:00<00:01, 52.16it/s]Scanning Parquet Files:  29%|██▊       | 24/84 [00:00<00:01, 56.02it/s]Scanning Parquet Files:  35%|███▍      | 29/84 [00:00<00:00, 56.35it/s]Scanning Parquet Files:  37%|███▋      | 31/84 [00:00<00:00, 60.39it/s]Scanning Parquet Files:  36%|███▌      | 30/84 [00:00<00:00, 57.30it/s]Scanning Parquet Files:  36%|███▌      | 30/84 [00:00<00:00, 57.14it/s]Scanning Parquet Files:  43%|████▎     | 36/84 [00:00<00:00, 48.12it/s]Scanning Parquet Files:  43%|████▎     | 36/84 [00:00<00:00, 48.05it/s]Scanning Parquet Files:  42%|████▏     | 35/84 [00:00<00:01, 44.48it/s]Scanning Parquet Files:  45%|████▌     | 38/84 [00:00<00:01, 40.65it/s]Scanning Parquet Files:  48%|████▊     | 40/84 [00:00<00:01, 40.03it/s]Scanning Parquet Files:  50%|█████     | 42/84 [00:00<00:01, 39.03it/s]Scanning Parquet Files:  50%|█████     | 42/84 [00:00<00:01, 38.82it/s]Scanning Parquet Files:  51%|█████     | 43/84 [00:00<00:01, 38.04it/s]Scanning Parquet Files:  54%|█████▎    | 45/84 [00:01<00:01, 34.74it/s]Scanning Parquet Files:  56%|█████▌    | 47/84 [00:01<00:01, 35.96it/s]Scanning Parquet Files:  56%|█████▌    | 47/84 [00:01<00:01, 34.03it/s]Scanning Parquet Files:  57%|█████▋    | 48/84 [00:01<00:00, 36.48it/s]Scanning Parquet Files:  58%|█████▊    | 49/84 [00:01<00:01, 34.49it/s]Scanning Parquet Files:  62%|██████▏   | 52/84 [00:01<00:00, 37.96it/s]Scanning Parquet Files:  63%|██████▎   | 53/84 [00:01<00:00, 38.74it/s]Scanning Parquet Files:  64%|██████▍   | 54/84 [00:01<00:00, 40.61it/s]Scanning Parquet Files:  64%|██████▍   | 54/84 [00:01<00:00, 37.58it/s]Scanning Parquet Files:  68%|██████▊   | 57/84 [00:01<00:00, 40.36it/s]Scanning Parquet Files:  70%|███████   | 59/84 [00:01<00:00, 42.58it/s]Scanning Parquet Files:  71%|███████▏  | 60/84 [00:01<00:00, 44.44it/s]Scanning Parquet Files:  75%|███████▌  | 63/84 [00:01<00:00, 45.00it/s]Scanning Parquet Files:  70%|███████   | 59/84 [00:01<00:00, 39.87it/s]Scanning Parquet Files:  76%|███████▌  | 64/84 [00:01<00:00, 42.12it/s]Scanning Parquet Files:  79%|███████▊  | 66/84 [00:01<00:00, 45.55it/s]Scanning Parquet Files:  82%|████████▏ | 69/84 [00:01<00:00, 48.65it/s]Scanning Parquet Files:  79%|███████▊  | 66/84 [00:01<00:00, 45.17it/s]Scanning Parquet Files:  83%|████████▎ | 70/84 [00:01<00:00, 46.23it/s]Scanning Parquet Files:  86%|████████▌ | 72/84 [00:01<00:00, 49.16it/s]Scanning Parquet Files:  89%|████████▉ | 75/84 [00:01<00:00, 48.02it/s]Scanning Parquet Files:  85%|████████▍ | 71/84 [00:01<00:00, 44.92it/s]Scanning Parquet Files:  93%|█████████▎| 78/84 [00:01<00:00, 51.67it/s]Scanning Parquet Files:  92%|█████████▏| 77/84 [00:01<00:00, 50.37it/s]Scanning Parquet Files:  93%|█████████▎| 78/84 [00:01<00:00, 49.05it/s]Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 47.42it/s]
Scanning Parquet Files:  98%|█████████▊| 82/84 [00:01<00:00, 49.64it/s]Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 47.14it/s]
✅ Loaded 190237 samples.
Scanning Parquet Files:  99%|█████████▉| 83/84 [00:01<00:00, 51.10it/s]Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 46.65it/s]
✅ Loaded 190237 samples.
✅ Loaded 190237 samples.
Scanning Parquet Files:  99%|█████████▉| 83/84 [00:01<00:00, 49.18it/s]Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 44.87it/s]
✅ Loaded 190237 samples.
Filtered dataset: 190132 samples remaining.
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
01/27/2026 03:21:17 - INFO - __main__ - [INFO] Set DeepSpeed train_micro_batch_size_per_gpu to 1
01/27/2026 03:21:17 - INFO - __main__ - [INFO] resume_from_checkpoint is None, but checking for existing checkpoints in /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000...
01/27/2026 03:21:17 - INFO - __main__ - [INFO] Auto-detected checkpoint: /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/checkpoint-4000
01/27/2026 03:21:17 - INFO - __main__ - [INFO] Loading transformer weights from checkpoint: /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/checkpoint-4000/transformer
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
01/27/2026 03:26:21 - INFO - __main__ - [INFO] Successfully loaded transformer weights from checkpoint (safetensors)
Before initializing optimizer states
MA 25.37 GB         Max_MA 29.59 GB         CA 29.6 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 36.19 GB, percent = 4.2%
After initializing optimizer states
MA 25.37 GB         Max_MA 33.82 GB         CA 38.06 GB         Max_CA 38 GB 
CPU Virtual Memory:  used = 37.44 GB, percent = 4.3%
After initializing ZeRO optimizer
MA 25.37 GB         Max_MA 25.37 GB         CA 38.06 GB         Max_CA 38 GB 
CPU Virtual Memory:  used = 37.61 GB, percent = 4.3%
01/27/2026 03:26:43 - INFO - __main__ - ***** Running training *****
01/27/2026 03:26:43 - INFO - __main__ -   Num examples = 190132
01/27/2026 03:26:43 - INFO - __main__ -   Num Epochs = 2
01/27/2026 03:26:43 - INFO - __main__ -   Instantaneous batch size per device = 1
01/27/2026 03:26:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
01/27/2026 03:26:43 - INFO - __main__ -   Gradient Accumulation steps = 4
01/27/2026 03:26:43 - INFO - __main__ -   Total optimization steps = 5000
01/27/2026 03:26:43 - INFO - __main__ - [INFO] Loading accelerator state from: /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/checkpoint-4000/accelerator
01/27/2026 03:26:43 - INFO - __main__ - [INFO] Patched torch.load to use weights_only=False for DeepSpeed compatibility
01/27/2026 03:26:43 - INFO - accelerate.accelerator - Loading states from /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/checkpoint-4000/accelerator
01/27/2026 03:26:43 - INFO - accelerate.accelerator - Loading DeepSpeed Model and Optimizer
01/27/2026 03:30:01 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer loaded from input dir /home/v-yuxluo/data/experiments/flux2klein_fulltune_5000/checkpoint-4000/accelerator/pytorch_model
01/27/2026 03:30:01 - INFO - accelerate.checkpointing - All model weights loaded successfully
01/27/2026 03:30:01 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
01/27/2026 03:30:01 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
01/27/2026 03:30:01 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
01/27/2026 03:30:01 - INFO - accelerate.checkpointing - All random states loaded successfully
01/27/2026 03:30:01 - INFO - accelerate.accelerator - Loading in 0 custom states
01/27/2026 03:30:01 - INFO - __main__ - [INFO] Resuming from step 4000, epoch 1
Steps:  80%|████████  | 4000/5000 [00:00<?, ?it/s]01/27/2026 03:30:01 - INFO - __main__ - [INFO] Using training iteration function: Flux2Klein_fulltune_train_iteration
01/27/2026 03:30:02 - INFO - __main__ - [INFO] Using validation function: Flux2Klein_fulltune_validation_func_parquet
01/27/2026 03:30:02 - INFO - __main__ - [INFO] Validation every 200 steps
wandb: Loaded settings from
wandb:   /home/v-yuxluo/.config/wandb/settings
wandb: [wandb.login()] Loaded credentials for https://microsoft-research.wandb.io from /home/v-yuxluo/.netrc.
wandb: Currently logged in as: v-yuxluo to https://microsoft-research.wandb.io. Use `wandb login --relogin` to force relogin
wandb: setting up run p5iq7wq4
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/v-yuxluo/WORK_local/ArXivQwenImage/wandb/run-20260127_033003-p5iq7wq4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flux2klein_9b_fulltune_5000steps
wandb: ⭐️ View project at https://microsoft-research.wandb.io/v-yuxluo/Flux2Klein-FullTune
wandb: 🚀 View run at https://microsoft-research.wandb.io/v-yuxluo/Flux2Klein-FullTune/runs/p5iq7wq4
01/27/2026 03:30:04 - INFO - __main__ - 
======================================================================
01/27/2026 03:30:04 - INFO - __main__ - Starting Training Loop
01/27/2026 03:30:04 - INFO - __main__ - ======================================================================
01/27/2026 03:30:04 - INFO - __main__ - [INFO] Skipping 4120 batches (1030 steps * 4 GA) in epoch 1 to resume from step 4000

[Step 4000] Training Debug Info:
  Loss: 0.388733
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0282, std: 0.9414
  Noise mean: 0.0049, std: 1.0000
  Target mean: -0.0233, std: 1.3750
  Model pred mean: -0.0295, std: 1.2188
  Sigmas: [0.8203125]... (timesteps: [822.0])

[Step 4000] Training Debug Info:
  Loss: 0.490124
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: -0.0086, std: 0.9414
  Noise mean: -0.0013, std: 1.0000
  Target mean: 0.0073, std: 1.3750
  Model pred mean: 0.0026, std: 1.1797
  Sigmas: [0.90234375]... (timesteps: [901.0])

[Step 4000] Training Debug Info:
  Loss: 0.690181
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0132, std: 0.9297
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0128, std: 1.3672
  Model pred mean: -0.0143, std: 1.0859
  Sigmas: [0.47265625]... (timesteps: [472.0])

[Step 4000] Training Debug Info:
  Loss: 0.839846
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0173, std: 0.8984
  Noise mean: -0.0020, std: 1.0000
  Target mean: 0.0153, std: 1.3438
  Model pred mean: 0.0178, std: 0.9844
  Sigmas: [0.41015625]... (timesteps: [411.0])
Steps:  80%|████████  | 4001/5000 [00:17<4:47:30, 17.27s/it]Steps:  80%|████████  | 4001/5000 [00:17<4:47:30, 17.27s/it, loss=0.8398, lr=1.17e-06]Steps:  80%|████████  | 4002/5000 [00:29<3:54:20, 14.09s/it, loss=0.8398, lr=1.17e-06]Steps:  80%|████████  | 4002/5000 [00:29<3:54:20, 14.09s/it, loss=0.4242, lr=1.17e-06]Steps:  80%|████████  | 4003/5000 [00:41<3:39:16, 13.20s/it, loss=0.4242, lr=1.17e-06]Steps:  80%|████████  | 4003/5000 [00:41<3:39:16, 13.20s/it, loss=0.6411, lr=1.16e-06]Steps:  80%|████████  | 4004/5000 [00:53<3:32:54, 12.83s/it, loss=0.6411, lr=1.16e-06]Steps:  80%|████████  | 4004/5000 [00:53<3:32:54, 12.83s/it, loss=0.4309, lr=1.16e-06]Steps:  80%|████████  | 4005/5000 [01:05<3:27:05, 12.49s/it, loss=0.4309, lr=1.16e-06]Steps:  80%|████████  | 4005/5000 [01:05<3:27:05, 12.49s/it, loss=0.4393, lr=1.16e-06]Steps:  80%|████████  | 4006/5000 [01:17<3:23:43, 12.30s/it, loss=0.4393, lr=1.16e-06]Steps:  80%|████████  | 4006/5000 [01:17<3:23:43, 12.30s/it, loss=1.1318, lr=1.16e-06]Steps:  80%|████████  | 4007/5000 [01:29<3:20:51, 12.14s/it, loss=1.1318, lr=1.16e-06]Steps:  80%|████████  | 4007/5000 [01:29<3:20:51, 12.14s/it, loss=0.9339, lr=1.15e-06]Steps:  80%|████████  | 4008/5000 [01:41<3:19:28, 12.07s/it, loss=0.9339, lr=1.15e-06]Steps:  80%|████████  | 4008/5000 [01:41<3:19:28, 12.07s/it, loss=0.4576, lr=1.15e-06]Steps:  80%|████████  | 4009/5000 [01:53<3:18:48, 12.04s/it, loss=0.4576, lr=1.15e-06]Steps:  80%|████████  | 4009/5000 [01:53<3:18:48, 12.04s/it, loss=1.0575, lr=1.15e-06]Steps:  80%|████████  | 4010/5000 [02:04<3:17:42, 11.98s/it, loss=1.0575, lr=1.15e-06]Steps:  80%|████████  | 4010/5000 [02:04<3:17:42, 11.98s/it, loss=1.1289, lr=1.15e-06]
[Step 4010] Training Debug Info:
  Loss: 0.365116
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0167, std: 0.8984
  Noise mean: -0.0025, std: 1.0000
  Target mean: -0.0192, std: 1.3438
  Model pred mean: -0.0201, std: 1.2031
  Sigmas: [0.91015625]... (timesteps: [911.0])

[Step 4010] Training Debug Info:
  Loss: 0.460800
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0059, std: 0.9180
  Noise mean: -0.0013, std: 1.0000
  Target mean: -0.0072, std: 1.3594
  Model pred mean: -0.0072, std: 1.1719
  Sigmas: [0.66796875]... (timesteps: [668.0])

[Step 4010] Training Debug Info:
  Loss: 1.155670
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0070, std: 0.8477
  Noise mean: 0.0020, std: 1.0000
  Target mean: 0.0090, std: 1.3125
  Model pred mean: 0.0057, std: 0.7500
  Sigmas: [0.09912109375]... (timesteps: [99.0])

[Step 4010] Training Debug Info:
  Loss: 1.088995
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0172, std: 0.9023
  Noise mean: -0.0017, std: 1.0000
  Target mean: -0.0188, std: 1.3438
  Model pred mean: -0.0166, std: 0.8516
  Sigmas: [0.2431640625]... (timesteps: [243.0])
Steps:  80%|████████  | 4011/5000 [02:16<3:17:35, 11.99s/it, loss=1.1289, lr=1.15e-06]Steps:  80%|████████  | 4011/5000 [02:16<3:17:35, 11.99s/it, loss=1.0890, lr=1.15e-06]Steps:  80%|████████  | 4012/5000 [02:28<3:17:00, 11.96s/it, loss=1.0890, lr=1.15e-06]Steps:  80%|████████  | 4012/5000 [02:28<3:17:00, 11.96s/it, loss=1.0437, lr=1.14e-06]Steps:  80%|████████  | 4013/5000 [02:40<3:16:56, 11.97s/it, loss=1.0437, lr=1.14e-06]Steps:  80%|████████  | 4013/5000 [02:40<3:16:56, 11.97s/it, loss=0.4896, lr=1.14e-06]Steps:  80%|████████  | 4014/5000 [02:52<3:16:26, 11.95s/it, loss=0.4896, lr=1.14e-06]Steps:  80%|████████  | 4014/5000 [02:52<3:16:26, 11.95s/it, loss=0.4471, lr=1.14e-06]Steps:  80%|████████  | 4015/5000 [03:04<3:15:43, 11.92s/it, loss=0.4471, lr=1.14e-06]Steps:  80%|████████  | 4015/5000 [03:04<3:15:43, 11.92s/it, loss=0.4036, lr=1.14e-06]Steps:  80%|████████  | 4016/5000 [03:16<3:15:11, 11.90s/it, loss=0.4036, lr=1.14e-06]Steps:  80%|████████  | 4016/5000 [03:16<3:15:11, 11.90s/it, loss=1.1172, lr=1.13e-06]Steps:  80%|████████  | 4017/5000 [03:28<3:15:15, 11.92s/it, loss=1.1172, lr=1.13e-06]Steps:  80%|████████  | 4017/5000 [03:28<3:15:15, 11.92s/it, loss=1.1672, lr=1.13e-06]Steps:  80%|████████  | 4018/5000 [03:40<3:15:28, 11.94s/it, loss=1.1672, lr=1.13e-06]Steps:  80%|████████  | 4018/5000 [03:40<3:15:28, 11.94s/it, loss=0.9027, lr=1.13e-06]Steps:  80%|████████  | 4019/5000 [03:52<3:15:01, 11.93s/it, loss=0.9027, lr=1.13e-06]Steps:  80%|████████  | 4019/5000 [03:52<3:15:01, 11.93s/it, loss=1.0442, lr=1.13e-06]Steps:  80%|████████  | 4020/5000 [04:04<3:15:06, 11.95s/it, loss=1.0442, lr=1.13e-06]Steps:  80%|████████  | 4020/5000 [04:04<3:15:06, 11.95s/it, loss=0.4688, lr=1.13e-06]
[Step 4020] Training Debug Info:
  Loss: 0.679811
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0011, std: 0.9180
  Noise mean: -0.0005, std: 0.9961
  Target mean: -0.0016, std: 1.3594
  Model pred mean: 0.0033, std: 1.0781
  Sigmas: [0.50390625]... (timesteps: [504.0])

[Step 4020] Training Debug Info:
  Loss: 0.367162
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0425, std: 0.9297
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0439, std: 1.3672
  Model pred mean: -0.0369, std: 1.2266
  Sigmas: [0.8203125]... (timesteps: [821.0])

[Step 4020] Training Debug Info:
  Loss: 0.647469
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0002, std: 0.9219
  Noise mean: 0.0018, std: 1.0000
  Target mean: 0.0020, std: 1.3594
  Model pred mean: -0.0007, std: 1.0938
  Sigmas: [0.546875]... (timesteps: [548.0])

[Step 4020] Training Debug Info:
  Loss: 0.633256
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0021, std: 0.9414
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0017, std: 1.3750
  Model pred mean: -0.0018, std: 1.1172
  Sigmas: [0.50390625]... (timesteps: [504.0])
Steps:  80%|████████  | 4021/5000 [04:16<3:14:50, 11.94s/it, loss=0.4688, lr=1.13e-06]Steps:  80%|████████  | 4021/5000 [04:16<3:14:50, 11.94s/it, loss=0.6333, lr=1.12e-06]Steps:  80%|████████  | 4022/5000 [04:28<3:14:54, 11.96s/it, loss=0.6333, lr=1.12e-06]Steps:  80%|████████  | 4022/5000 [04:28<3:14:54, 11.96s/it, loss=0.7387, lr=1.12e-06]Steps:  80%|████████  | 4023/5000 [04:40<3:14:10, 11.93s/it, loss=0.7387, lr=1.12e-06]Steps:  80%|████████  | 4023/5000 [04:40<3:14:10, 11.93s/it, loss=0.5867, lr=1.12e-06]Steps:  80%|████████  | 4024/5000 [04:51<3:13:44, 11.91s/it, loss=0.5867, lr=1.12e-06]Steps:  80%|████████  | 4024/5000 [04:51<3:13:44, 11.91s/it, loss=1.1582, lr=1.12e-06]Steps:  80%|████████  | 4025/5000 [05:03<3:13:31, 11.91s/it, loss=1.1582, lr=1.12e-06]Steps:  80%|████████  | 4025/5000 [05:03<3:13:31, 11.91s/it, loss=0.5447, lr=1.11e-06]Steps:  81%|████████  | 4026/5000 [05:15<3:13:04, 11.89s/it, loss=0.5447, lr=1.11e-06]Steps:  81%|████████  | 4026/5000 [05:15<3:13:04, 11.89s/it, loss=0.4849, lr=1.11e-06]Steps:  81%|████████  | 4027/5000 [05:27<3:13:02, 11.90s/it, loss=0.4849, lr=1.11e-06]Steps:  81%|████████  | 4027/5000 [05:27<3:13:02, 11.90s/it, loss=1.1092, lr=1.11e-06]Steps:  81%|████████  | 4028/5000 [05:39<3:12:55, 11.91s/it, loss=1.1092, lr=1.11e-06]Steps:  81%|████████  | 4028/5000 [05:39<3:12:55, 11.91s/it, loss=1.0181, lr=1.11e-06]Steps:  81%|████████  | 4029/5000 [05:51<3:12:43, 11.91s/it, loss=1.0181, lr=1.11e-06]Steps:  81%|████████  | 4029/5000 [05:51<3:12:43, 11.91s/it, loss=0.5849, lr=1.11e-06]Steps:  81%|████████  | 4030/5000 [06:03<3:12:12, 11.89s/it, loss=0.5849, lr=1.11e-06]Steps:  81%|████████  | 4030/5000 [06:03<3:12:12, 11.89s/it, loss=0.8604, lr=1.10e-06]
[Step 4030] Training Debug Info:
  Loss: 0.613353
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0042, std: 0.9297
  Noise mean: -0.0029, std: 1.0000
  Target mean: -0.0071, std: 1.3672
  Model pred mean: 0.0001, std: 1.1172
  Sigmas: [0.54296875]... (timesteps: [542.0])

[Step 4030] Training Debug Info:
  Loss: 0.372624
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0060, std: 0.9062
  Noise mean: -0.0033, std: 0.9961
  Target mean: -0.0093, std: 1.3438
  Model pred mean: -0.0059, std: 1.1953
  Sigmas: [0.859375]... (timesteps: [859.0])

[Step 4030] Training Debug Info:
  Loss: 0.376361
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0272, std: 0.9219
  Noise mean: 0.0008, std: 1.0000
  Target mean: 0.0280, std: 1.3594
  Model pred mean: 0.0258, std: 1.2109
  Sigmas: [0.79296875]... (timesteps: [793.0])

[Step 4030] Training Debug Info:
  Loss: 0.404936
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0029, std: 0.9062
  Noise mean: 0.0020, std: 1.0000
  Target mean: -0.0009, std: 1.3516
  Model pred mean: -0.0057, std: 1.1875
  Sigmas: [0.796875]... (timesteps: [798.0])
Steps:  81%|████████  | 4031/5000 [06:15<3:12:35, 11.92s/it, loss=0.8604, lr=1.10e-06]Steps:  81%|████████  | 4031/5000 [06:15<3:12:35, 11.92s/it, loss=0.4049, lr=1.10e-06]Steps:  81%|████████  | 4032/5000 [06:27<3:12:14, 11.92s/it, loss=0.4049, lr=1.10e-06]Steps:  81%|████████  | 4032/5000 [06:27<3:12:14, 11.92s/it, loss=1.0888, lr=1.10e-06]Steps:  81%|████████  | 4033/5000 [06:39<3:11:46, 11.90s/it, loss=1.0888, lr=1.10e-06]Steps:  81%|████████  | 4033/5000 [06:39<3:11:46, 11.90s/it, loss=0.8346, lr=1.10e-06]Steps:  81%|████████  | 4034/5000 [06:50<3:11:35, 11.90s/it, loss=0.8346, lr=1.10e-06]Steps:  81%|████████  | 4034/5000 [06:50<3:11:35, 11.90s/it, loss=0.4152, lr=1.09e-06]Steps:  81%|████████  | 4035/5000 [07:02<3:11:33, 11.91s/it, loss=0.4152, lr=1.09e-06]Steps:  81%|████████  | 4035/5000 [07:02<3:11:33, 11.91s/it, loss=1.0434, lr=1.09e-06]Steps:  81%|████████  | 4036/5000 [07:14<3:11:18, 11.91s/it, loss=1.0434, lr=1.09e-06]Steps:  81%|████████  | 4036/5000 [07:14<3:11:18, 11.91s/it, loss=0.5276, lr=1.09e-06]Steps:  81%|████████  | 4037/5000 [07:26<3:11:10, 11.91s/it, loss=0.5276, lr=1.09e-06]Steps:  81%|████████  | 4037/5000 [07:26<3:11:10, 11.91s/it, loss=1.0261, lr=1.09e-06]Steps:  81%|████████  | 4038/5000 [07:38<3:10:57, 11.91s/it, loss=1.0261, lr=1.09e-06]Steps:  81%|████████  | 4038/5000 [07:38<3:10:57, 11.91s/it, loss=0.5931, lr=1.09e-06]Steps:  81%|████████  | 4039/5000 [07:50<3:10:34, 11.90s/it, loss=0.5931, lr=1.09e-06]Steps:  81%|████████  | 4039/5000 [07:50<3:10:34, 11.90s/it, loss=0.7937, lr=1.08e-06]Steps:  81%|████████  | 4040/5000 [08:02<3:10:50, 11.93s/it, loss=0.7937, lr=1.08e-06]Steps:  81%|████████  | 4040/5000 [08:02<3:10:50, 11.93s/it, loss=0.3904, lr=1.08e-06]
[Step 4040] Training Debug Info:
  Loss: 0.593225
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0014, std: 0.9102
  Noise mean: -0.0012, std: 1.0000
  Target mean: 0.0002, std: 1.3516
  Model pred mean: -0.0091, std: 1.0938
  Sigmas: [0.98828125]... (timesteps: [989.0])

[Step 4040] Training Debug Info:
  Loss: 0.894630
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0145, std: 0.9219
  Noise mean: -0.0026, std: 1.0000
  Target mean: 0.0118, std: 1.3672
  Model pred mean: 0.0143, std: 0.9805
  Sigmas: [0.40234375]... (timesteps: [402.0])

[Step 4040] Training Debug Info:
  Loss: 1.070849
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0167, std: 0.9297
  Noise mean: 0.0020, std: 1.0000
  Target mean: -0.0147, std: 1.3594
  Model pred mean: -0.0192, std: 0.8906
  Sigmas: [0.134765625]... (timesteps: [135.0])

[Step 4040] Training Debug Info:
  Loss: 1.078806
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0167, std: 0.8945
  Noise mean: -0.0021, std: 1.0000
  Target mean: 0.0146, std: 1.3438
  Model pred mean: 0.0183, std: 0.8516
  Sigmas: [0.326171875]... (timesteps: [327.0])
Steps:  81%|████████  | 4041/5000 [08:14<3:10:29, 11.92s/it, loss=0.3904, lr=1.08e-06]Steps:  81%|████████  | 4041/5000 [08:14<3:10:29, 11.92s/it, loss=1.0788, lr=1.08e-06]Steps:  81%|████████  | 4042/5000 [08:26<3:10:15, 11.92s/it, loss=1.0788, lr=1.08e-06]Steps:  81%|████████  | 4042/5000 [08:26<3:10:15, 11.92s/it, loss=0.6190, lr=1.08e-06]Steps:  81%|████████  | 4043/5000 [08:38<3:09:49, 11.90s/it, loss=0.6190, lr=1.08e-06]Steps:  81%|████████  | 4043/5000 [08:38<3:09:49, 11.90s/it, loss=1.1392, lr=1.08e-06]Steps:  81%|████████  | 4044/5000 [08:50<3:09:37, 11.90s/it, loss=1.1392, lr=1.08e-06]Steps:  81%|████████  | 4044/5000 [08:50<3:09:37, 11.90s/it, loss=0.5598, lr=1.07e-06]Steps:  81%|████████  | 4045/5000 [09:01<3:09:21, 11.90s/it, loss=0.5598, lr=1.07e-06]Steps:  81%|████████  | 4045/5000 [09:01<3:09:21, 11.90s/it, loss=0.9110, lr=1.07e-06]Steps:  81%|████████  | 4046/5000 [09:13<3:09:15, 11.90s/it, loss=0.9110, lr=1.07e-06]Steps:  81%|████████  | 4046/5000 [09:13<3:09:15, 11.90s/it, loss=1.0584, lr=1.07e-06]Steps:  81%|████████  | 4047/5000 [09:25<3:08:53, 11.89s/it, loss=1.0584, lr=1.07e-06]Steps:  81%|████████  | 4047/5000 [09:25<3:08:53, 11.89s/it, loss=1.0733, lr=1.07e-06]Steps:  81%|████████  | 4048/5000 [09:37<3:08:51, 11.90s/it, loss=1.0733, lr=1.07e-06]Steps:  81%|████████  | 4048/5000 [09:37<3:08:51, 11.90s/it, loss=0.5985, lr=1.06e-06]Steps:  81%|████████  | 4049/5000 [09:49<3:09:01, 11.93s/it, loss=0.5985, lr=1.06e-06]Steps:  81%|████████  | 4049/5000 [09:49<3:09:01, 11.93s/it, loss=0.9887, lr=1.06e-06]Steps:  81%|████████  | 4050/5000 [10:01<3:08:24, 11.90s/it, loss=0.9887, lr=1.06e-06]Steps:  81%|████████  | 4050/5000 [10:01<3:08:24, 11.90s/it, loss=1.1574, lr=1.06e-06]
[Step 4050] Training Debug Info:
  Loss: 0.362265
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0153, std: 0.9023
  Noise mean: -0.0017, std: 1.0000
  Target mean: 0.0135, std: 1.3516
  Model pred mean: 0.0127, std: 1.2109
  Sigmas: [0.86328125]... (timesteps: [862.0])

[Step 4050] Training Debug Info:
  Loss: 0.462398
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0073, std: 0.9258
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0063, std: 1.3672
  Model pred mean: -0.0061, std: 1.1875
  Sigmas: [0.67578125]... (timesteps: [675.0])

[Step 4050] Training Debug Info:
  Loss: 0.482963
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0113, std: 0.9414
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0115, std: 1.3750
  Model pred mean: -0.0172, std: 1.1797
  Sigmas: [0.90234375]... (timesteps: [901.0])

[Step 4050] Training Debug Info:
  Loss: 0.378436
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0211, std: 0.9062
  Noise mean: 0.0014, std: 0.9961
  Target mean: -0.0198, std: 1.3438
  Model pred mean: -0.0254, std: 1.2031
  Sigmas: [0.875]... (timesteps: [876.0])
Steps:  81%|████████  | 4051/5000 [10:13<3:08:08, 11.90s/it, loss=1.1574, lr=1.06e-06]Steps:  81%|████████  | 4051/5000 [10:13<3:08:08, 11.90s/it, loss=0.3784, lr=1.06e-06]Steps:  81%|████████  | 4052/5000 [10:25<3:07:55, 11.89s/it, loss=0.3784, lr=1.06e-06]Steps:  81%|████████  | 4052/5000 [10:25<3:07:55, 11.89s/it, loss=0.8721, lr=1.06e-06]Steps:  81%|████████  | 4053/5000 [10:37<3:07:57, 11.91s/it, loss=0.8721, lr=1.06e-06]Steps:  81%|████████  | 4053/5000 [10:37<3:07:57, 11.91s/it, loss=0.6362, lr=1.05e-06]Steps:  81%|████████  | 4054/5000 [10:49<3:07:46, 11.91s/it, loss=0.6362, lr=1.05e-06]Steps:  81%|████████  | 4054/5000 [10:49<3:07:46, 11.91s/it, loss=0.3560, lr=1.05e-06]Steps:  81%|████████  | 4055/5000 [11:01<3:07:56, 11.93s/it, loss=0.3560, lr=1.05e-06]Steps:  81%|████████  | 4055/5000 [11:01<3:07:56, 11.93s/it, loss=0.9801, lr=1.05e-06]Steps:  81%|████████  | 4056/5000 [11:13<3:07:52, 11.94s/it, loss=0.9801, lr=1.05e-06]Steps:  81%|████████  | 4056/5000 [11:13<3:07:52, 11.94s/it, loss=1.0629, lr=1.05e-06]Steps:  81%|████████  | 4057/5000 [11:24<3:07:37, 11.94s/it, loss=1.0629, lr=1.05e-06]Steps:  81%|████████  | 4057/5000 [11:24<3:07:37, 11.94s/it, loss=0.5032, lr=1.04e-06]Steps:  81%|████████  | 4058/5000 [11:36<3:07:35, 11.95s/it, loss=0.5032, lr=1.04e-06]Steps:  81%|████████  | 4058/5000 [11:36<3:07:35, 11.95s/it, loss=0.4777, lr=1.04e-06]Steps:  81%|████████  | 4059/5000 [11:48<3:07:27, 11.95s/it, loss=0.4777, lr=1.04e-06]Steps:  81%|████████  | 4059/5000 [11:48<3:07:27, 11.95s/it, loss=0.4857, lr=1.04e-06]Steps:  81%|████████  | 4060/5000 [12:00<3:06:49, 11.92s/it, loss=0.4857, lr=1.04e-06]Steps:  81%|████████  | 4060/5000 [12:00<3:06:49, 11.92s/it, loss=0.3815, lr=1.04e-06]
[Step 4060] Training Debug Info:
  Loss: 0.894545
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0059, std: 0.9453
  Noise mean: 0.0012, std: 1.0000
  Target mean: 0.0071, std: 1.3750
  Model pred mean: 0.0053, std: 0.9961
  Sigmas: [0.3359375]... (timesteps: [336.0])

[Step 4060] Training Debug Info:
  Loss: 0.541842
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0066, std: 0.9297
  Noise mean: 0.0024, std: 1.0000
  Target mean: 0.0090, std: 1.3672
  Model pred mean: 0.0095, std: 1.1484
  Sigmas: [0.6171875]... (timesteps: [618.0])

[Step 4060] Training Debug Info:
  Loss: 0.722019
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0150, std: 0.9062
  Noise mean: 0.0042, std: 1.0000
  Target mean: -0.0108, std: 1.3516
  Model pred mean: -0.0159, std: 1.0469
  Sigmas: [0.4765625]... (timesteps: [477.0])

[Step 4060] Training Debug Info:
  Loss: 0.679801
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0422, std: 0.9492
  Noise mean: -0.0016, std: 1.0000
  Target mean: -0.0437, std: 1.3750
  Model pred mean: -0.0461, std: 1.1016
  Sigmas: [0.4375]... (timesteps: [437.0])
Steps:  81%|████████  | 4061/5000 [12:12<3:06:25, 11.91s/it, loss=0.3815, lr=1.04e-06]Steps:  81%|████████  | 4061/5000 [12:12<3:06:25, 11.91s/it, loss=0.6798, lr=1.04e-06]Steps:  81%|████████  | 4062/5000 [12:24<3:06:10, 11.91s/it, loss=0.6798, lr=1.04e-06]Steps:  81%|████████  | 4062/5000 [12:24<3:06:10, 11.91s/it, loss=0.3575, lr=1.03e-06]Steps:  81%|████████▏ | 4063/5000 [12:36<3:05:59, 11.91s/it, loss=0.3575, lr=1.03e-06]Steps:  81%|████████▏ | 4063/5000 [12:36<3:05:59, 11.91s/it, loss=0.4523, lr=1.03e-06]Steps:  81%|████████▏ | 4064/5000 [12:48<3:05:32, 11.89s/it, loss=0.4523, lr=1.03e-06]Steps:  81%|████████▏ | 4064/5000 [12:48<3:05:32, 11.89s/it, loss=0.3584, lr=1.03e-06]Steps:  81%|████████▏ | 4065/5000 [13:00<3:05:18, 11.89s/it, loss=0.3584, lr=1.03e-06]Steps:  81%|████████▏ | 4065/5000 [13:00<3:05:18, 11.89s/it, loss=0.9488, lr=1.03e-06]Steps:  81%|████████▏ | 4066/5000 [13:12<3:05:06, 11.89s/it, loss=0.9488, lr=1.03e-06]Steps:  81%|████████▏ | 4066/5000 [13:12<3:05:06, 11.89s/it, loss=0.3910, lr=1.03e-06]Steps:  81%|████████▏ | 4067/5000 [13:24<3:05:43, 11.94s/it, loss=0.3910, lr=1.03e-06]Steps:  81%|████████▏ | 4067/5000 [13:24<3:05:43, 11.94s/it, loss=0.8691, lr=1.02e-06]Steps:  81%|████████▏ | 4068/5000 [13:35<3:04:55, 11.90s/it, loss=0.8691, lr=1.02e-06]Steps:  81%|████████▏ | 4068/5000 [13:35<3:04:55, 11.90s/it, loss=0.5149, lr=1.02e-06]Steps:  81%|████████▏ | 4069/5000 [13:47<3:04:18, 11.88s/it, loss=0.5149, lr=1.02e-06]Steps:  81%|████████▏ | 4069/5000 [13:47<3:04:18, 11.88s/it, loss=0.5036, lr=1.02e-06]Steps:  81%|████████▏ | 4070/5000 [13:59<3:04:37, 11.91s/it, loss=0.5036, lr=1.02e-06]Steps:  81%|████████▏ | 4070/5000 [13:59<3:04:37, 11.91s/it, loss=0.4851, lr=1.02e-06]
[Step 4070] Training Debug Info:
  Loss: 0.952107
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0075, std: 0.9453
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0080, std: 1.3750
  Model pred mean: -0.0069, std: 0.9688
  Sigmas: [0.271484375]... (timesteps: [272.0])

[Step 4070] Training Debug Info:
  Loss: 0.350255
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0117, std: 0.8828
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0134, std: 1.3359
  Model pred mean: 0.0167, std: 1.1875
  Sigmas: [0.90625]... (timesteps: [908.0])

[Step 4070] Training Debug Info:
  Loss: 0.507354
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0040, std: 0.9375
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0022, std: 1.3750
  Model pred mean: -0.0019, std: 1.1719
  Sigmas: [0.55859375]... (timesteps: [560.0])

[Step 4070] Training Debug Info:
  Loss: 0.510008
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0302, std: 0.9297
  Noise mean: -0.0002, std: 0.9961
  Target mean: -0.0304, std: 1.3594
  Model pred mean: -0.0282, std: 1.1641
  Sigmas: [0.578125]... (timesteps: [578.0])
Steps:  81%|████████▏ | 4071/5000 [14:11<3:04:23, 11.91s/it, loss=0.4851, lr=1.02e-06]Steps:  81%|████████▏ | 4071/5000 [14:11<3:04:23, 11.91s/it, loss=0.5100, lr=1.02e-06]Steps:  81%|████████▏ | 4072/5000 [14:23<3:04:14, 11.91s/it, loss=0.5100, lr=1.02e-06]Steps:  81%|████████▏ | 4072/5000 [14:23<3:04:14, 11.91s/it, loss=0.3807, lr=1.01e-06]Steps:  81%|████████▏ | 4073/5000 [14:35<3:04:03, 11.91s/it, loss=0.3807, lr=1.01e-06]Steps:  81%|████████▏ | 4073/5000 [14:35<3:04:03, 11.91s/it, loss=1.0360, lr=1.01e-06]Steps:  81%|████████▏ | 4074/5000 [14:47<3:03:27, 11.89s/it, loss=1.0360, lr=1.01e-06]Steps:  81%|████████▏ | 4074/5000 [14:47<3:03:27, 11.89s/it, loss=0.4321, lr=1.01e-06]Steps:  82%|████████▏ | 4075/5000 [14:59<3:03:02, 11.87s/it, loss=0.4321, lr=1.01e-06]Steps:  82%|████████▏ | 4075/5000 [14:59<3:03:02, 11.87s/it, loss=0.4210, lr=1.01e-06]Steps:  82%|████████▏ | 4076/5000 [15:11<3:03:27, 11.91s/it, loss=0.4210, lr=1.01e-06]Steps:  82%|████████▏ | 4076/5000 [15:11<3:03:27, 11.91s/it, loss=1.1514, lr=1.00e-06]Steps:  82%|████████▏ | 4077/5000 [15:23<3:03:10, 11.91s/it, loss=1.1514, lr=1.00e-06]Steps:  82%|████████▏ | 4077/5000 [15:23<3:03:10, 11.91s/it, loss=1.0271, lr=1.00e-06]Steps:  82%|████████▏ | 4078/5000 [15:34<3:03:00, 11.91s/it, loss=1.0271, lr=1.00e-06]Steps:  82%|████████▏ | 4078/5000 [15:34<3:03:00, 11.91s/it, loss=0.8521, lr=1.00e-06]Steps:  82%|████████▏ | 4079/5000 [15:46<3:02:47, 11.91s/it, loss=0.8521, lr=1.00e-06]Steps:  82%|████████▏ | 4079/5000 [15:46<3:02:47, 11.91s/it, loss=1.1612, lr=9.98e-07]Steps:  82%|████████▏ | 4080/5000 [15:58<3:02:25, 11.90s/it, loss=1.1612, lr=9.98e-07]Steps:  82%|████████▏ | 4080/5000 [15:58<3:02:25, 11.90s/it, loss=1.0857, lr=9.96e-07]
[Step 4080] Training Debug Info:
  Loss: 0.352010
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0195, std: 0.8828
  Noise mean: 0.0026, std: 1.0000
  Target mean: 0.0221, std: 1.3359
  Model pred mean: 0.0244, std: 1.1953
  Sigmas: [0.87890625]... (timesteps: [879.0])

[Step 4080] Training Debug Info:
  Loss: 0.441564
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0008, std: 0.8906
  Noise mean: -0.0003, std: 1.0000
  Target mean: -0.0012, std: 1.3438
  Model pred mean: -0.0067, std: 1.1719
  Sigmas: [0.9375]... (timesteps: [939.0])

[Step 4080] Training Debug Info:
  Loss: 0.429291
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0586, std: 0.9180
  Noise mean: -0.0024, std: 1.0000
  Target mean: -0.0610, std: 1.3594
  Model pred mean: -0.0571, std: 1.1875
  Sigmas: [0.6640625]... (timesteps: [665.0])

[Step 4080] Training Debug Info:
  Loss: 0.485214
  Latent shape: torch.Size([1, 32, 132, 66]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0129, std: 0.9141
  Noise mean: -0.0018, std: 1.0000
  Target mean: 0.0111, std: 1.3516
  Model pred mean: 0.0119, std: 1.1562
  Sigmas: [0.640625]... (timesteps: [640.0])
Steps:  82%|████████▏ | 4081/5000 [16:10<3:02:10, 11.89s/it, loss=1.0857, lr=9.96e-07]Steps:  82%|████████▏ | 4081/5000 [16:10<3:02:10, 11.89s/it, loss=0.4852, lr=9.94e-07]Steps:  82%|████████▏ | 4082/5000 [16:22<3:01:58, 11.89s/it, loss=0.4852, lr=9.94e-07]Steps:  82%|████████▏ | 4082/5000 [16:22<3:01:58, 11.89s/it, loss=0.4079, lr=9.92e-07]Steps:  82%|████████▏ | 4083/5000 [16:34<3:01:49, 11.90s/it, loss=0.4079, lr=9.92e-07]Steps:  82%|████████▏ | 4083/5000 [16:34<3:01:49, 11.90s/it, loss=0.8899, lr=9.90e-07]Steps:  82%|████████▏ | 4084/5000 [16:46<3:01:45, 11.91s/it, loss=0.8899, lr=9.90e-07]Steps:  82%|████████▏ | 4084/5000 [16:46<3:01:45, 11.91s/it, loss=0.9739, lr=9.88e-07]Steps:  82%|████████▏ | 4085/5000 [16:58<3:01:48, 11.92s/it, loss=0.9739, lr=9.88e-07]Steps:  82%|████████▏ | 4085/5000 [16:58<3:01:48, 11.92s/it, loss=0.6765, lr=9.86e-07]Steps:  82%|████████▏ | 4086/5000 [17:10<3:01:33, 11.92s/it, loss=0.6765, lr=9.86e-07]Steps:  82%|████████▏ | 4086/5000 [17:10<3:01:33, 11.92s/it, loss=1.1304, lr=9.84e-07]Steps:  82%|████████▏ | 4087/5000 [17:22<3:00:56, 11.89s/it, loss=1.1304, lr=9.84e-07]Steps:  82%|████████▏ | 4087/5000 [17:22<3:00:56, 11.89s/it, loss=0.9409, lr=9.82e-07]Steps:  82%|████████▏ | 4088/5000 [17:33<3:00:45, 11.89s/it, loss=0.9409, lr=9.82e-07]Steps:  82%|████████▏ | 4088/5000 [17:33<3:00:45, 11.89s/it, loss=0.6073, lr=9.80e-07]Steps:  82%|████████▏ | 4089/5000 [17:45<3:00:43, 11.90s/it, loss=0.6073, lr=9.80e-07]Steps:  82%|████████▏ | 4089/5000 [17:45<3:00:43, 11.90s/it, loss=0.6038, lr=9.78e-07]Steps:  82%|████████▏ | 4090/5000 [17:57<3:00:33, 11.91s/it, loss=0.6038, lr=9.78e-07]Steps:  82%|████████▏ | 4090/5000 [17:57<3:00:33, 11.91s/it, loss=0.4289, lr=9.76e-07]
[Step 4090] Training Debug Info:
  Loss: 1.121649
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0054, std: 0.8789
  Noise mean: 0.0011, std: 1.0000
  Target mean: 0.0065, std: 1.3359
  Model pred mean: 0.0053, std: 0.8086
  Sigmas: [0.0751953125]... (timesteps: [75.0])

[Step 4090] Training Debug Info:
  Loss: 0.372961
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0347, std: 0.8906
  Noise mean: 0.0013, std: 1.0000
  Target mean: 0.0359, std: 1.3438
  Model pred mean: 0.0366, std: 1.1953
  Sigmas: [0.84765625]... (timesteps: [848.0])

[Step 4090] Training Debug Info:
  Loss: 1.073296
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0022, std: 0.9375
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0020, std: 1.3672
  Model pred mean: -0.0026, std: 0.8984
  Sigmas: [0.06494140625]... (timesteps: [65.0])

[Step 4090] Training Debug Info:
  Loss: 0.961091
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0251, std: 0.9414
  Noise mean: 0.0009, std: 0.9961
  Target mean: 0.0260, std: 1.3750
  Model pred mean: 0.0253, std: 0.9648
  Sigmas: [0.30859375]... (timesteps: [308.0])
Steps:  82%|████████▏ | 4091/5000 [18:09<3:00:11, 11.89s/it, loss=0.4289, lr=9.76e-07]Steps:  82%|████████▏ | 4091/5000 [18:09<3:00:11, 11.89s/it, loss=0.9611, lr=9.73e-07]Steps:  82%|████████▏ | 4092/5000 [18:21<3:00:05, 11.90s/it, loss=0.9611, lr=9.73e-07]Steps:  82%|████████▏ | 4092/5000 [18:21<3:00:05, 11.90s/it, loss=0.5881, lr=9.71e-07]Steps:  82%|████████▏ | 4093/5000 [18:33<2:59:35, 11.88s/it, loss=0.5881, lr=9.71e-07]Steps:  82%|████████▏ | 4093/5000 [18:33<2:59:35, 11.88s/it, loss=0.6967, lr=9.69e-07]Steps:  82%|████████▏ | 4094/5000 [18:45<3:00:04, 11.93s/it, loss=0.6967, lr=9.69e-07]Steps:  82%|████████▏ | 4094/5000 [18:45<3:00:04, 11.93s/it, loss=0.8616, lr=9.67e-07]Steps:  82%|████████▏ | 4095/5000 [18:57<3:00:00, 11.93s/it, loss=0.8616, lr=9.67e-07]Steps:  82%|████████▏ | 4095/5000 [18:57<3:00:00, 11.93s/it, loss=1.0767, lr=9.65e-07]Steps:  82%|████████▏ | 4096/5000 [19:09<2:59:34, 11.92s/it, loss=1.0767, lr=9.65e-07]Steps:  82%|████████▏ | 4096/5000 [19:09<2:59:34, 11.92s/it, loss=1.1425, lr=9.63e-07]Steps:  82%|████████▏ | 4097/5000 [19:21<2:59:26, 11.92s/it, loss=1.1425, lr=9.63e-07]Steps:  82%|████████▏ | 4097/5000 [19:21<2:59:26, 11.92s/it, loss=0.5222, lr=9.61e-07]Steps:  82%|████████▏ | 4098/5000 [19:33<2:59:06, 11.91s/it, loss=0.5222, lr=9.61e-07]Steps:  82%|████████▏ | 4098/5000 [19:33<2:59:06, 11.91s/it, loss=0.3962, lr=9.59e-07]Steps:  82%|████████▏ | 4099/5000 [19:45<2:59:06, 11.93s/it, loss=0.3962, lr=9.59e-07]Steps:  82%|████████▏ | 4099/5000 [19:45<2:59:06, 11.93s/it, loss=0.9840, lr=9.57e-07]Steps:  82%|████████▏ | 4100/5000 [19:56<2:58:48, 11.92s/it, loss=0.9840, lr=9.57e-07]Steps:  82%|████████▏ | 4100/5000 [19:56<2:58:48, 11.92s/it, loss=0.5313, lr=9.55e-07]01/27/2026 03:49:58 - INFO - __main__ - 
[Step 4100] ✅ Loss in normal range (0.5313)
01/27/2026 03:49:58 - INFO - __main__ -   Loss avg (last 100): 0.7434
01/27/2026 03:49:58 - INFO - __main__ -   Loss range: [0.3560, 1.1672]

[Step 4100] Training Debug Info:
  Loss: 0.911557
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: -0.0097, std: 0.8750
  Noise mean: -0.0028, std: 1.0000
  Target mean: 0.0070, std: 1.3281
  Model pred mean: 0.0107, std: 0.9219
  Sigmas: [0.423828125]... (timesteps: [424.0])

[Step 4100] Training Debug Info:
  Loss: 1.005094
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0400, std: 0.9414
  Noise mean: -0.0024, std: 1.0000
  Target mean: -0.0425, std: 1.3750
  Model pred mean: -0.0400, std: 0.9414
  Sigmas: [0.00099945068359375]... (timesteps: [1.0])

[Step 4100] Training Debug Info:
  Loss: 0.660018
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0032, std: 0.9141
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0038, std: 1.3594
  Model pred mean: 0.0012, std: 1.0938
  Sigmas: [0.99609375]... (timesteps: [998.0])

[Step 4100] Training Debug Info:
  Loss: 0.565803
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0079, std: 0.8672
  Noise mean: -0.0013, std: 1.0000
  Target mean: -0.0092, std: 1.3203
  Model pred mean: -0.0093, std: 1.0938
  Sigmas: [0.61328125]... (timesteps: [615.0])
Steps:  82%|████████▏ | 4101/5000 [20:08<2:58:18, 11.90s/it, loss=0.5313, lr=9.55e-07]Steps:  82%|████████▏ | 4101/5000 [20:08<2:58:18, 11.90s/it, loss=0.5658, lr=9.53e-07]Steps:  82%|████████▏ | 4102/5000 [20:20<2:58:00, 11.89s/it, loss=0.5658, lr=9.53e-07]Steps:  82%|████████▏ | 4102/5000 [20:20<2:58:00, 11.89s/it, loss=0.6035, lr=9.51e-07]Steps:  82%|████████▏ | 4103/5000 [20:32<2:58:12, 11.92s/it, loss=0.6035, lr=9.51e-07]Steps:  82%|████████▏ | 4103/5000 [20:32<2:58:12, 11.92s/it, loss=1.0265, lr=9.49e-07]Steps:  82%|████████▏ | 4104/5000 [20:44<2:57:53, 11.91s/it, loss=1.0265, lr=9.49e-07]Steps:  82%|████████▏ | 4104/5000 [20:44<2:57:53, 11.91s/it, loss=0.4746, lr=9.47e-07]Steps:  82%|████████▏ | 4105/5000 [20:56<2:57:44, 11.92s/it, loss=0.4746, lr=9.47e-07]Steps:  82%|████████▏ | 4105/5000 [20:56<2:57:44, 11.92s/it, loss=1.1129, lr=9.45e-07]Steps:  82%|████████▏ | 4106/5000 [21:08<2:57:36, 11.92s/it, loss=1.1129, lr=9.45e-07]Steps:  82%|████████▏ | 4106/5000 [21:08<2:57:36, 11.92s/it, loss=1.0877, lr=9.43e-07]Steps:  82%|████████▏ | 4107/5000 [21:20<2:57:20, 11.92s/it, loss=1.0877, lr=9.43e-07]Steps:  82%|████████▏ | 4107/5000 [21:20<2:57:20, 11.92s/it, loss=0.4592, lr=9.41e-07]Steps:  82%|████████▏ | 4108/5000 [21:32<2:56:59, 11.90s/it, loss=0.4592, lr=9.41e-07]Steps:  82%|████████▏ | 4108/5000 [21:32<2:56:59, 11.90s/it, loss=0.5446, lr=9.39e-07]Steps:  82%|████████▏ | 4109/5000 [21:43<2:56:08, 11.86s/it, loss=0.5446, lr=9.39e-07]Steps:  82%|████████▏ | 4109/5000 [21:43<2:56:08, 11.86s/it, loss=1.0881, lr=9.37e-07]Steps:  82%|████████▏ | 4110/5000 [21:55<2:55:58, 11.86s/it, loss=1.0881, lr=9.37e-07]Steps:  82%|████████▏ | 4110/5000 [21:55<2:55:58, 11.86s/it, loss=1.0762, lr=9.34e-07]
[Step 4110] Training Debug Info:
  Loss: 1.167760
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0007, std: 0.9023
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0005, std: 1.3438
  Model pred mean: -0.0002, std: 0.8047
  Sigmas: [0.1650390625]... (timesteps: [165.0])

[Step 4110] Training Debug Info:
  Loss: 0.754172
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0051, std: 0.8984
  Noise mean: 0.0007, std: 1.0000
  Target mean: 0.0058, std: 1.3438
  Model pred mean: 0.0059, std: 1.0234
  Sigmas: [0.482421875]... (timesteps: [483.0])

[Step 4110] Training Debug Info:
  Loss: 0.864788
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0096, std: 0.8906
  Noise mean: -0.0019, std: 1.0000
  Target mean: 0.0077, std: 1.3359
  Model pred mean: 0.0082, std: 0.9609
  Sigmas: [0.427734375]... (timesteps: [427.0])

[Step 4110] Training Debug Info:
  Loss: 0.489243
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0452, std: 0.8984
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0454, std: 1.3438
  Model pred mean: -0.0398, std: 1.1562
  Sigmas: [0.92578125]... (timesteps: [926.0])
Steps:  82%|████████▏ | 4111/5000 [22:07<2:55:55, 11.87s/it, loss=1.0762, lr=9.34e-07]Steps:  82%|████████▏ | 4111/5000 [22:07<2:55:55, 11.87s/it, loss=0.4892, lr=9.32e-07]Steps:  82%|████████▏ | 4112/5000 [22:19<2:56:09, 11.90s/it, loss=0.4892, lr=9.32e-07]Steps:  82%|████████▏ | 4112/5000 [22:19<2:56:09, 11.90s/it, loss=0.9362, lr=9.30e-07]Steps:  82%|████████▏ | 4113/5000 [22:31<2:56:01, 11.91s/it, loss=0.9362, lr=9.30e-07]Steps:  82%|████████▏ | 4113/5000 [22:31<2:56:01, 11.91s/it, loss=0.4317, lr=9.28e-07]Steps:  82%|████████▏ | 4114/5000 [22:43<2:55:54, 11.91s/it, loss=0.4317, lr=9.28e-07]Steps:  82%|████████▏ | 4114/5000 [22:43<2:55:54, 11.91s/it, loss=0.4042, lr=9.26e-07]Steps:  82%|████████▏ | 4115/5000 [22:55<2:55:19, 11.89s/it, loss=0.4042, lr=9.26e-07]Steps:  82%|████████▏ | 4115/5000 [22:55<2:55:19, 11.89s/it, loss=0.6481, lr=9.24e-07]Steps:  82%|████████▏ | 4116/5000 [23:07<2:54:56, 11.87s/it, loss=0.6481, lr=9.24e-07]Steps:  82%|████████▏ | 4116/5000 [23:07<2:54:56, 11.87s/it, loss=0.5406, lr=9.22e-07]Steps:  82%|████████▏ | 4117/5000 [23:19<2:54:49, 11.88s/it, loss=0.5406, lr=9.22e-07]Steps:  82%|████████▏ | 4117/5000 [23:19<2:54:49, 11.88s/it, loss=1.1196, lr=9.20e-07]Steps:  82%|████████▏ | 4118/5000 [23:31<2:54:42, 11.89s/it, loss=1.1196, lr=9.20e-07]Steps:  82%|████████▏ | 4118/5000 [23:31<2:54:42, 11.89s/it, loss=0.8316, lr=9.18e-07]Steps:  82%|████████▏ | 4119/5000 [23:42<2:54:52, 11.91s/it, loss=0.8316, lr=9.18e-07]Steps:  82%|████████▏ | 4119/5000 [23:42<2:54:52, 11.91s/it, loss=1.0245, lr=9.16e-07]Steps:  82%|████████▏ | 4120/5000 [23:54<2:54:48, 11.92s/it, loss=1.0245, lr=9.16e-07]Steps:  82%|████████▏ | 4120/5000 [23:54<2:54:48, 11.92s/it, loss=1.0304, lr=9.14e-07]
[Step 4120] Training Debug Info:
  Loss: 0.563632
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0110, std: 0.9336
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0119, std: 1.3672
  Model pred mean: -0.0096, std: 1.1406
  Sigmas: [0.59375]... (timesteps: [593.0])

[Step 4120] Training Debug Info:
  Loss: 0.681711
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0184, std: 0.9219
  Noise mean: -0.0000, std: 1.0000
  Target mean: -0.0184, std: 1.3594
  Model pred mean: -0.0221, std: 1.0859
  Sigmas: [0.96484375]... (timesteps: [966.0])

[Step 4120] Training Debug Info:
  Loss: 0.408309
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0008, std: 0.9062
  Noise mean: -0.0004, std: 1.0000
  Target mean: 0.0004, std: 1.3516
  Model pred mean: 0.0016, std: 1.1875
  Sigmas: [0.765625]... (timesteps: [765.0])

[Step 4120] Training Debug Info:
  Loss: 0.422871
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0016, std: 0.9727
  Noise mean: 0.0030, std: 1.0000
  Target mean: 0.0014, std: 1.3984
  Model pred mean: 0.0011, std: 1.2344
  Sigmas: [0.8125]... (timesteps: [812.0])
Steps:  82%|████████▏ | 4121/5000 [24:07<2:55:17, 11.97s/it, loss=1.0304, lr=9.14e-07]Steps:  82%|████████▏ | 4121/5000 [24:07<2:55:17, 11.97s/it, loss=0.4229, lr=9.12e-07]Steps:  82%|████████▏ | 4122/5000 [24:18<2:54:52, 11.95s/it, loss=0.4229, lr=9.12e-07]Steps:  82%|████████▏ | 4122/5000 [24:18<2:54:52, 11.95s/it, loss=1.0638, lr=9.10e-07]Steps:  82%|████████▏ | 4123/5000 [24:30<2:54:34, 11.94s/it, loss=1.0638, lr=9.10e-07]Steps:  82%|████████▏ | 4123/5000 [24:30<2:54:34, 11.94s/it, loss=0.6597, lr=9.08e-07]Steps:  82%|████████▏ | 4124/5000 [24:42<2:54:00, 11.92s/it, loss=0.6597, lr=9.08e-07]Steps:  82%|████████▏ | 4124/5000 [24:42<2:54:00, 11.92s/it, loss=0.8090, lr=9.06e-07]Steps:  82%|████████▎ | 4125/5000 [24:54<2:53:31, 11.90s/it, loss=0.8090, lr=9.06e-07]Steps:  82%|████████▎ | 4125/5000 [24:54<2:53:31, 11.90s/it, loss=0.4086, lr=9.04e-07]Steps:  83%|████████▎ | 4126/5000 [25:06<2:52:54, 11.87s/it, loss=0.4086, lr=9.04e-07]Steps:  83%|████████▎ | 4126/5000 [25:06<2:52:54, 11.87s/it, loss=1.0030, lr=9.02e-07]Steps:  83%|████████▎ | 4127/5000 [25:18<2:52:40, 11.87s/it, loss=1.0030, lr=9.02e-07]Steps:  83%|████████▎ | 4127/5000 [25:18<2:52:40, 11.87s/it, loss=0.4408, lr=9.00e-07]Steps:  83%|████████▎ | 4128/5000 [25:30<2:52:18, 11.86s/it, loss=0.4408, lr=9.00e-07]Steps:  83%|████████▎ | 4128/5000 [25:30<2:52:18, 11.86s/it, loss=1.1395, lr=8.98e-07]Steps:  83%|████████▎ | 4129/5000 [25:41<2:52:10, 11.86s/it, loss=1.1395, lr=8.98e-07]Steps:  83%|████████▎ | 4129/5000 [25:41<2:52:10, 11.86s/it, loss=0.6409, lr=8.96e-07]Steps:  83%|████████▎ | 4130/5000 [25:53<2:52:24, 11.89s/it, loss=0.6409, lr=8.96e-07]Steps:  83%|████████▎ | 4130/5000 [25:53<2:52:24, 11.89s/it, loss=0.8052, lr=8.94e-07]
[Step 4130] Training Debug Info:
  Loss: 0.896653
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0369, std: 0.9297
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0361, std: 1.3672
  Model pred mean: -0.0361, std: 0.9805
  Sigmas: [0.37109375]... (timesteps: [372.0])

[Step 4130] Training Debug Info:
  Loss: 1.147310
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0157, std: 0.9023
  Noise mean: -0.0007, std: 1.0000
  Target mean: -0.0164, std: 1.3438
  Model pred mean: -0.0150, std: 0.8164
  Sigmas: [0.10693359375]... (timesteps: [107.0])

[Step 4130] Training Debug Info:
  Loss: 0.382581
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0223, std: 0.8906
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0206, std: 1.3359
  Model pred mean: -0.0232, std: 1.1953
  Sigmas: [0.87890625]... (timesteps: [880.0])

[Step 4130] Training Debug Info:
  Loss: 1.149974
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0074, std: 0.8828
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0056, std: 1.3281
  Model pred mean: -0.0083, std: 0.7930
  Sigmas: [0.11083984375]... (timesteps: [111.0])
Steps:  83%|████████▎ | 4131/5000 [26:05<2:52:15, 11.89s/it, loss=0.8052, lr=8.94e-07]Steps:  83%|████████▎ | 4131/5000 [26:05<2:52:15, 11.89s/it, loss=1.1500, lr=8.92e-07]Steps:  83%|████████▎ | 4132/5000 [26:17<2:52:01, 11.89s/it, loss=1.1500, lr=8.92e-07]Steps:  83%|████████▎ | 4132/5000 [26:17<2:52:01, 11.89s/it, loss=1.0739, lr=8.90e-07]Steps:  83%|████████▎ | 4133/5000 [26:29<2:51:55, 11.90s/it, loss=1.0739, lr=8.90e-07]Steps:  83%|████████▎ | 4133/5000 [26:29<2:51:55, 11.90s/it, loss=1.1947, lr=8.88e-07]Steps:  83%|████████▎ | 4134/5000 [26:41<2:51:43, 11.90s/it, loss=1.1947, lr=8.88e-07]Steps:  83%|████████▎ | 4134/5000 [26:41<2:51:43, 11.90s/it, loss=1.0384, lr=8.86e-07]Steps:  83%|████████▎ | 4135/5000 [26:53<2:51:32, 11.90s/it, loss=1.0384, lr=8.86e-07]Steps:  83%|████████▎ | 4135/5000 [26:53<2:51:32, 11.90s/it, loss=0.4117, lr=8.84e-07]Steps:  83%|████████▎ | 4136/5000 [27:05<2:51:38, 11.92s/it, loss=0.4117, lr=8.84e-07]Steps:  83%|████████▎ | 4136/5000 [27:05<2:51:38, 11.92s/it, loss=0.4033, lr=8.82e-07]Steps:  83%|████████▎ | 4137/5000 [27:17<2:51:26, 11.92s/it, loss=0.4033, lr=8.82e-07]Steps:  83%|████████▎ | 4137/5000 [27:17<2:51:26, 11.92s/it, loss=0.8140, lr=8.80e-07]Steps:  83%|████████▎ | 4138/5000 [27:29<2:50:46, 11.89s/it, loss=0.8140, lr=8.80e-07]Steps:  83%|████████▎ | 4138/5000 [27:29<2:50:46, 11.89s/it, loss=1.1633, lr=8.78e-07]Steps:  83%|████████▎ | 4139/5000 [27:41<2:51:19, 11.94s/it, loss=1.1633, lr=8.78e-07]Steps:  83%|████████▎ | 4139/5000 [27:41<2:51:19, 11.94s/it, loss=1.0964, lr=8.76e-07]Steps:  83%|████████▎ | 4140/5000 [27:53<2:50:58, 11.93s/it, loss=1.0964, lr=8.76e-07]Steps:  83%|████████▎ | 4140/5000 [27:53<2:50:58, 11.93s/it, loss=0.3454, lr=8.74e-07]
[Step 4140] Training Debug Info:
  Loss: 0.423107
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0366, std: 0.9531
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0347, std: 1.3828
  Model pred mean: -0.0327, std: 1.2188
  Sigmas: [0.6796875]... (timesteps: [680.0])

[Step 4140] Training Debug Info:
  Loss: 0.596104
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0137, std: 0.9141
  Noise mean: -0.0013, std: 1.0000
  Target mean: -0.0150, std: 1.3594
  Model pred mean: -0.0118, std: 1.1172
  Sigmas: [0.546875]... (timesteps: [547.0])

[Step 4140] Training Debug Info:
  Loss: 0.705690
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0040, std: 0.9180
  Noise mean: -0.0000, std: 1.0000
  Target mean: -0.0040, std: 1.3594
  Model pred mean: -0.0004, std: 1.0703
  Sigmas: [0.5234375]... (timesteps: [525.0])

[Step 4140] Training Debug Info:
  Loss: 1.013617
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0286, std: 0.9336
  Noise mean: -0.0027, std: 1.0000
  Target mean: 0.0259, std: 1.3672
  Model pred mean: 0.0293, std: 0.9219
  Sigmas: [0.006988525390625]... (timesteps: [7.0])
Steps:  83%|████████▎ | 4141/5000 [28:04<2:50:49, 11.93s/it, loss=0.3454, lr=8.74e-07]Steps:  83%|████████▎ | 4141/5000 [28:04<2:50:49, 11.93s/it, loss=1.0136, lr=8.72e-07]Steps:  83%|████████▎ | 4142/5000 [28:16<2:50:31, 11.92s/it, loss=1.0136, lr=8.72e-07]Steps:  83%|████████▎ | 4142/5000 [28:16<2:50:31, 11.92s/it, loss=0.3871, lr=8.70e-07]Steps:  83%|████████▎ | 4143/5000 [28:28<2:50:31, 11.94s/it, loss=0.3871, lr=8.70e-07]Steps:  83%|████████▎ | 4143/5000 [28:28<2:50:31, 11.94s/it, loss=1.1189, lr=8.69e-07]Steps:  83%|████████▎ | 4144/5000 [28:40<2:50:12, 11.93s/it, loss=1.1189, lr=8.69e-07]Steps:  83%|████████▎ | 4144/5000 [28:40<2:50:12, 11.93s/it, loss=1.1288, lr=8.67e-07]Steps:  83%|████████▎ | 4145/5000 [28:52<2:49:51, 11.92s/it, loss=1.1288, lr=8.67e-07]Steps:  83%|████████▎ | 4145/5000 [28:52<2:49:51, 11.92s/it, loss=1.1656, lr=8.65e-07]Steps:  83%|████████▎ | 4146/5000 [29:04<2:49:32, 11.91s/it, loss=1.1656, lr=8.65e-07]Steps:  83%|████████▎ | 4146/5000 [29:04<2:49:32, 11.91s/it, loss=0.5331, lr=8.63e-07]Steps:  83%|████████▎ | 4147/5000 [29:16<2:49:15, 11.91s/it, loss=0.5331, lr=8.63e-07]Steps:  83%|████████▎ | 4147/5000 [29:16<2:49:15, 11.91s/it, loss=1.1244, lr=8.61e-07]Steps:  83%|████████▎ | 4148/5000 [29:28<2:49:36, 11.94s/it, loss=1.1244, lr=8.61e-07]Steps:  83%|████████▎ | 4148/5000 [29:28<2:49:36, 11.94s/it, loss=0.3802, lr=8.59e-07]Steps:  83%|████████▎ | 4149/5000 [29:40<2:49:14, 11.93s/it, loss=0.3802, lr=8.59e-07]Steps:  83%|████████▎ | 4149/5000 [29:40<2:49:14, 11.93s/it, loss=1.1358, lr=8.57e-07]Steps:  83%|████████▎ | 4150/5000 [29:52<2:48:57, 11.93s/it, loss=1.1358, lr=8.57e-07]Steps:  83%|████████▎ | 4150/5000 [29:52<2:48:57, 11.93s/it, loss=0.7086, lr=8.55e-07]
[Step 4150] Training Debug Info:
  Loss: 0.643172
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0354, std: 0.9102
  Noise mean: 0.0036, std: 1.0000
  Target mean: -0.0317, std: 1.3516
  Model pred mean: -0.0248, std: 1.0859
  Sigmas: [0.99609375]... (timesteps: [998.0])

[Step 4150] Training Debug Info:
  Loss: 1.107687
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0256, std: 0.9219
  Noise mean: -0.0012, std: 1.0000
  Target mean: 0.0244, std: 1.3594
  Model pred mean: 0.0269, std: 0.8594
  Sigmas: [0.078125]... (timesteps: [78.0])

[Step 4150] Training Debug Info:
  Loss: 0.429964
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0420, std: 0.9375
  Noise mean: -0.0014, std: 0.9961
  Target mean: -0.0432, std: 1.3672
  Model pred mean: -0.0383, std: 1.1953
  Sigmas: [0.67578125]... (timesteps: [676.0])

[Step 4150] Training Debug Info:
  Loss: 0.505350
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0085, std: 0.9258
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0075, std: 1.3594
  Model pred mean: -0.0150, std: 1.1719
  Sigmas: [0.9296875]... (timesteps: [931.0])
Steps:  83%|████████▎ | 4151/5000 [30:04<2:48:49, 11.93s/it, loss=0.7086, lr=8.55e-07]Steps:  83%|████████▎ | 4151/5000 [30:04<2:48:49, 11.93s/it, loss=0.5053, lr=8.53e-07]Steps:  83%|████████▎ | 4152/5000 [30:16<2:48:01, 11.89s/it, loss=0.5053, lr=8.53e-07]Steps:  83%|████████▎ | 4152/5000 [30:16<2:48:01, 11.89s/it, loss=0.4193, lr=8.51e-07]Steps:  83%|████████▎ | 4153/5000 [30:27<2:47:51, 11.89s/it, loss=0.4193, lr=8.51e-07]Steps:  83%|████████▎ | 4153/5000 [30:27<2:47:51, 11.89s/it, loss=0.3643, lr=8.49e-07]Steps:  83%|████████▎ | 4154/5000 [30:39<2:47:46, 11.90s/it, loss=0.3643, lr=8.49e-07]Steps:  83%|████████▎ | 4154/5000 [30:39<2:47:46, 11.90s/it, loss=0.5859, lr=8.47e-07]Steps:  83%|████████▎ | 4155/5000 [30:51<2:47:41, 11.91s/it, loss=0.5859, lr=8.47e-07]Steps:  83%|████████▎ | 4155/5000 [30:51<2:47:41, 11.91s/it, loss=0.3657, lr=8.45e-07]Steps:  83%|████████▎ | 4156/5000 [31:03<2:47:13, 11.89s/it, loss=0.3657, lr=8.45e-07]Steps:  83%|████████▎ | 4156/5000 [31:03<2:47:13, 11.89s/it, loss=0.5197, lr=8.43e-07]Steps:  83%|████████▎ | 4157/5000 [31:15<2:47:35, 11.93s/it, loss=0.5197, lr=8.43e-07]Steps:  83%|████████▎ | 4157/5000 [31:15<2:47:35, 11.93s/it, loss=0.4092, lr=8.41e-07]Steps:  83%|████████▎ | 4158/5000 [31:27<2:47:12, 11.92s/it, loss=0.4092, lr=8.41e-07]Steps:  83%|████████▎ | 4158/5000 [31:27<2:47:12, 11.92s/it, loss=0.3605, lr=8.39e-07]Steps:  83%|████████▎ | 4159/5000 [31:39<2:46:45, 11.90s/it, loss=0.3605, lr=8.39e-07]Steps:  83%|████████▎ | 4159/5000 [31:39<2:46:45, 11.90s/it, loss=0.4764, lr=8.37e-07]Steps:  83%|████████▎ | 4160/5000 [31:51<2:46:29, 11.89s/it, loss=0.4764, lr=8.37e-07]Steps:  83%|████████▎ | 4160/5000 [31:51<2:46:29, 11.89s/it, loss=0.3736, lr=8.35e-07]
[Step 4160] Training Debug Info:
  Loss: 1.074332
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0011, std: 0.9258
  Noise mean: 0.0007, std: 1.0000
  Target mean: 0.0018, std: 1.3594
  Model pred mean: 0.0029, std: 0.8789
  Sigmas: [0.0458984375]... (timesteps: [46.0])

[Step 4160] Training Debug Info:
  Loss: 0.389888
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0078, std: 0.8750
  Noise mean: -0.0016, std: 1.0000
  Target mean: 0.0061, std: 1.3281
  Model pred mean: 0.0038, std: 1.1719
  Sigmas: [0.80859375]... (timesteps: [810.0])

[Step 4160] Training Debug Info:
  Loss: 0.869382
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0366, std: 0.9531
  Noise mean: 0.0034, std: 1.0000
  Target mean: -0.0332, std: 1.3828
  Model pred mean: -0.0361, std: 1.0234
  Sigmas: [0.359375]... (timesteps: [360.0])

[Step 4160] Training Debug Info:
  Loss: 1.005548
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0036, std: 0.8828
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0021, std: 1.3359
  Model pred mean: -0.0033, std: 0.8828
  Sigmas: [0.0030059814453125]... (timesteps: [3.0])
Steps:  83%|████████▎ | 4161/5000 [32:03<2:46:11, 11.88s/it, loss=0.3736, lr=8.35e-07]Steps:  83%|████████▎ | 4161/5000 [32:03<2:46:11, 11.88s/it, loss=1.0055, lr=8.33e-07]Steps:  83%|████████▎ | 4162/5000 [32:14<2:45:39, 11.86s/it, loss=1.0055, lr=8.33e-07]Steps:  83%|████████▎ | 4162/5000 [32:14<2:45:39, 11.86s/it, loss=0.4394, lr=8.32e-07]Steps:  83%|████████▎ | 4163/5000 [32:26<2:45:30, 11.86s/it, loss=0.4394, lr=8.32e-07]Steps:  83%|████████▎ | 4163/5000 [32:26<2:45:30, 11.86s/it, loss=0.4079, lr=8.30e-07]Steps:  83%|████████▎ | 4164/5000 [32:38<2:45:12, 11.86s/it, loss=0.4079, lr=8.30e-07]Steps:  83%|████████▎ | 4164/5000 [32:38<2:45:12, 11.86s/it, loss=0.4297, lr=8.28e-07]Steps:  83%|████████▎ | 4165/5000 [32:50<2:45:18, 11.88s/it, loss=0.4297, lr=8.28e-07]Steps:  83%|████████▎ | 4165/5000 [32:50<2:45:18, 11.88s/it, loss=0.6781, lr=8.26e-07]Steps:  83%|████████▎ | 4166/5000 [33:02<2:45:30, 11.91s/it, loss=0.6781, lr=8.26e-07]Steps:  83%|████████▎ | 4166/5000 [33:02<2:45:30, 11.91s/it, loss=1.0204, lr=8.24e-07]Steps:  83%|████████▎ | 4167/5000 [33:14<2:45:17, 11.91s/it, loss=1.0204, lr=8.24e-07]Steps:  83%|████████▎ | 4167/5000 [33:14<2:45:17, 11.91s/it, loss=0.8001, lr=8.22e-07]Steps:  83%|████████▎ | 4168/5000 [33:26<2:45:11, 11.91s/it, loss=0.8001, lr=8.22e-07]Steps:  83%|████████▎ | 4168/5000 [33:26<2:45:11, 11.91s/it, loss=1.0254, lr=8.20e-07]Steps:  83%|████████▎ | 4169/5000 [33:38<2:45:00, 11.91s/it, loss=1.0254, lr=8.20e-07]Steps:  83%|████████▎ | 4169/5000 [33:38<2:45:00, 11.91s/it, loss=1.1474, lr=8.18e-07]Steps:  83%|████████▎ | 4170/5000 [33:50<2:44:45, 11.91s/it, loss=1.1474, lr=8.18e-07]Steps:  83%|████████▎ | 4170/5000 [33:50<2:44:45, 11.91s/it, loss=1.1134, lr=8.16e-07]
[Step 4170] Training Debug Info:
  Loss: 0.981088
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0294, std: 0.9336
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0309, std: 1.3672
  Model pred mean: -0.0291, std: 0.9414
  Sigmas: [0.25390625]... (timesteps: [254.00001525878906])

[Step 4170] Training Debug Info:
  Loss: 0.390817
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0253, std: 0.9570
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0270, std: 1.3828
  Model pred mean: 0.0223, std: 1.2422
  Sigmas: [0.8203125]... (timesteps: [820.0])

[Step 4170] Training Debug Info:
  Loss: 1.142821
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0022, std: 0.9062
  Noise mean: -0.0014, std: 1.0000
  Target mean: 0.0008, std: 1.3516
  Model pred mean: 0.0030, std: 0.8242
  Sigmas: [0.173828125]... (timesteps: [174.0])

[Step 4170] Training Debug Info:
  Loss: 0.419786
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0054, std: 0.8633
  Noise mean: -0.0001, std: 0.9961
  Target mean: 0.0053, std: 1.3203
  Model pred mean: 0.0029, std: 1.1484
  Sigmas: [0.76953125]... (timesteps: [769.0])
Steps:  83%|████████▎ | 4171/5000 [34:02<2:44:30, 11.91s/it, loss=1.1134, lr=8.16e-07]Steps:  83%|████████▎ | 4171/5000 [34:02<2:44:30, 11.91s/it, loss=0.4198, lr=8.14e-07]Steps:  83%|████████▎ | 4172/5000 [34:14<2:44:19, 11.91s/it, loss=0.4198, lr=8.14e-07]Steps:  83%|████████▎ | 4172/5000 [34:14<2:44:19, 11.91s/it, loss=0.3602, lr=8.12e-07]Steps:  83%|████████▎ | 4173/5000 [34:25<2:44:04, 11.90s/it, loss=0.3602, lr=8.12e-07]Steps:  83%|████████▎ | 4173/5000 [34:25<2:44:04, 11.90s/it, loss=0.4783, lr=8.10e-07]Steps:  83%|████████▎ | 4174/5000 [34:37<2:43:36, 11.88s/it, loss=0.4783, lr=8.10e-07]Steps:  83%|████████▎ | 4174/5000 [34:37<2:43:36, 11.88s/it, loss=0.4963, lr=8.09e-07]Steps:  84%|████████▎ | 4175/5000 [34:49<2:43:59, 11.93s/it, loss=0.4963, lr=8.09e-07]Steps:  84%|████████▎ | 4175/5000 [34:49<2:43:59, 11.93s/it, loss=0.5471, lr=8.07e-07]Steps:  84%|████████▎ | 4176/5000 [35:01<2:43:39, 11.92s/it, loss=0.5471, lr=8.07e-07]Steps:  84%|████████▎ | 4176/5000 [35:01<2:43:39, 11.92s/it, loss=0.5041, lr=8.05e-07]Steps:  84%|████████▎ | 4177/5000 [35:13<2:43:09, 11.89s/it, loss=0.5041, lr=8.05e-07]Steps:  84%|████████▎ | 4177/5000 [35:13<2:43:09, 11.89s/it, loss=0.5704, lr=8.03e-07]Steps:  84%|████████▎ | 4178/5000 [35:25<2:42:48, 11.88s/it, loss=0.5704, lr=8.03e-07]Steps:  84%|████████▎ | 4178/5000 [35:25<2:42:48, 11.88s/it, loss=0.3773, lr=8.01e-07]Steps:  84%|████████▎ | 4179/5000 [35:37<2:42:35, 11.88s/it, loss=0.3773, lr=8.01e-07]Steps:  84%|████████▎ | 4179/5000 [35:37<2:42:35, 11.88s/it, loss=1.1325, lr=7.99e-07]Steps:  84%|████████▎ | 4180/5000 [35:49<2:42:13, 11.87s/it, loss=1.1325, lr=7.99e-07]Steps:  84%|████████▎ | 4180/5000 [35:49<2:42:13, 11.87s/it, loss=0.9820, lr=7.97e-07]
[Step 4180] Training Debug Info:
  Loss: 0.465527
  Latent shape: torch.Size([1, 32, 114, 78]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0223, std: 0.9102
  Noise mean: -0.0018, std: 1.0000
  Target mean: -0.0240, std: 1.3516
  Model pred mean: -0.0150, std: 1.1562
  Sigmas: [0.9375]... (timesteps: [938.0])

[Step 4180] Training Debug Info:
  Loss: 0.461809
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0112, std: 0.9023
  Noise mean: 0.0039, std: 1.0000
  Target mean: -0.0072, std: 1.3438
  Model pred mean: -0.0087, std: 1.1641
  Sigmas: [0.69921875]... (timesteps: [700.0])

[Step 4180] Training Debug Info:
  Loss: 0.515733
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0104, std: 0.9844
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0117, std: 1.4062
  Model pred mean: -0.0102, std: 1.2031
  Sigmas: [0.6171875]... (timesteps: [618.0])

[Step 4180] Training Debug Info:
  Loss: 0.421368
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0262, std: 0.9141
  Noise mean: -0.0000, std: 1.0000
  Target mean: 0.0262, std: 1.3516
  Model pred mean: 0.0237, std: 1.1875
  Sigmas: [0.80078125]... (timesteps: [799.0])
Steps:  84%|████████▎ | 4181/5000 [36:01<2:42:11, 11.88s/it, loss=0.9820, lr=7.97e-07]Steps:  84%|████████▎ | 4181/5000 [36:01<2:42:11, 11.88s/it, loss=0.4214, lr=7.95e-07]Steps:  84%|████████▎ | 4182/5000 [36:12<2:41:43, 11.86s/it, loss=0.4214, lr=7.95e-07]Steps:  84%|████████▎ | 4182/5000 [36:12<2:41:43, 11.86s/it, loss=1.1469, lr=7.93e-07]Steps:  84%|████████▎ | 4183/5000 [36:24<2:42:01, 11.90s/it, loss=1.1469, lr=7.93e-07]Steps:  84%|████████▎ | 4183/5000 [36:24<2:42:01, 11.90s/it, loss=1.1476, lr=7.92e-07]Steps:  84%|████████▎ | 4184/5000 [36:36<2:42:35, 11.96s/it, loss=1.1476, lr=7.92e-07]Steps:  84%|████████▎ | 4184/5000 [36:36<2:42:35, 11.96s/it, loss=1.1544, lr=7.90e-07]Steps:  84%|████████▎ | 4185/5000 [36:48<2:42:24, 11.96s/it, loss=1.1544, lr=7.90e-07]Steps:  84%|████████▎ | 4185/5000 [36:48<2:42:24, 11.96s/it, loss=0.8500, lr=7.88e-07]Steps:  84%|████████▎ | 4186/5000 [37:00<2:42:03, 11.95s/it, loss=0.8500, lr=7.88e-07]Steps:  84%|████████▎ | 4186/5000 [37:00<2:42:03, 11.95s/it, loss=0.7060, lr=7.86e-07]Steps:  84%|████████▎ | 4187/5000 [37:12<2:41:58, 11.95s/it, loss=0.7060, lr=7.86e-07]Steps:  84%|████████▎ | 4187/5000 [37:12<2:41:58, 11.95s/it, loss=0.5801, lr=7.84e-07]Steps:  84%|████████▍ | 4188/5000 [37:24<2:41:53, 11.96s/it, loss=0.5801, lr=7.84e-07]Steps:  84%|████████▍ | 4188/5000 [37:24<2:41:53, 11.96s/it, loss=0.4185, lr=7.82e-07]Steps:  84%|████████▍ | 4189/5000 [37:36<2:41:37, 11.96s/it, loss=0.4185, lr=7.82e-07]Steps:  84%|████████▍ | 4189/5000 [37:36<2:41:37, 11.96s/it, loss=1.0960, lr=7.80e-07]Steps:  84%|████████▍ | 4190/5000 [37:48<2:41:24, 11.96s/it, loss=1.0960, lr=7.80e-07]Steps:  84%|████████▍ | 4190/5000 [37:48<2:41:24, 11.96s/it, loss=0.3647, lr=7.78e-07]
[Step 4190] Training Debug Info:
  Loss: 0.417628
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0106, std: 0.9023
  Noise mean: -0.0015, std: 1.0000
  Target mean: 0.0091, std: 1.3438
  Model pred mean: 0.0098, std: 1.1875
  Sigmas: [0.75]... (timesteps: [751.0])

[Step 4190] Training Debug Info:
  Loss: 0.601171
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0119, std: 0.9453
  Noise mean: 0.0012, std: 1.0000
  Target mean: 0.0131, std: 1.3750
  Model pred mean: 0.0131, std: 1.1406
  Sigmas: [0.60546875]... (timesteps: [605.0])

[Step 4190] Training Debug Info:
  Loss: 0.584042
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0103, std: 0.8867
  Noise mean: 0.0017, std: 0.9961
  Target mean: 0.0120, std: 1.3359
  Model pred mean: 0.0061, std: 1.1016
  Sigmas: [0.96875]... (timesteps: [968.0])

[Step 4190] Training Debug Info:
  Loss: 1.045144
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0308, std: 0.9062
  Noise mean: 0.0011, std: 0.9961
  Target mean: -0.0297, std: 1.3438
  Model pred mean: -0.0317, std: 0.8789
  Sigmas: [0.031982421875]... (timesteps: [32.0])
Steps:  84%|████████▍ | 4191/5000 [38:00<2:40:58, 11.94s/it, loss=0.3647, lr=7.78e-07]Steps:  84%|████████▍ | 4191/5000 [38:00<2:40:58, 11.94s/it, loss=1.0451, lr=7.76e-07]Steps:  84%|████████▍ | 4192/5000 [38:12<2:40:41, 11.93s/it, loss=1.0451, lr=7.76e-07]Steps:  84%|████████▍ | 4192/5000 [38:12<2:40:41, 11.93s/it, loss=1.0128, lr=7.75e-07]Steps:  84%|████████▍ | 4193/5000 [38:24<2:40:41, 11.95s/it, loss=1.0128, lr=7.75e-07]Steps:  84%|████████▍ | 4193/5000 [38:24<2:40:41, 11.95s/it, loss=0.5085, lr=7.73e-07]Steps:  84%|████████▍ | 4194/5000 [38:36<2:40:07, 11.92s/it, loss=0.5085, lr=7.73e-07]Steps:  84%|████████▍ | 4194/5000 [38:36<2:40:07, 11.92s/it, loss=0.4795, lr=7.71e-07]Steps:  84%|████████▍ | 4195/5000 [38:48<2:39:53, 11.92s/it, loss=0.4795, lr=7.71e-07]Steps:  84%|████████▍ | 4195/5000 [38:48<2:39:53, 11.92s/it, loss=0.7651, lr=7.69e-07]Steps:  84%|████████▍ | 4196/5000 [39:00<2:39:47, 11.93s/it, loss=0.7651, lr=7.69e-07]Steps:  84%|████████▍ | 4196/5000 [39:00<2:39:47, 11.93s/it, loss=0.7542, lr=7.67e-07]Steps:  84%|████████▍ | 4197/5000 [39:12<2:39:41, 11.93s/it, loss=0.7542, lr=7.67e-07]Steps:  84%|████████▍ | 4197/5000 [39:12<2:39:41, 11.93s/it, loss=0.8347, lr=7.65e-07]Steps:  84%|████████▍ | 4198/5000 [39:23<2:39:13, 11.91s/it, loss=0.8347, lr=7.65e-07]Steps:  84%|████████▍ | 4198/5000 [39:23<2:39:13, 11.91s/it, loss=0.9924, lr=7.63e-07]Steps:  84%|████████▍ | 4199/5000 [39:35<2:38:53, 11.90s/it, loss=0.9924, lr=7.63e-07]Steps:  84%|████████▍ | 4199/5000 [39:35<2:38:53, 11.90s/it, loss=0.3643, lr=7.62e-07]Steps:  84%|████████▍ | 4200/5000 [39:47<2:38:46, 11.91s/it, loss=0.3643, lr=7.62e-07]Steps:  84%|████████▍ | 4200/5000 [39:47<2:38:46, 11.91s/it, loss=0.9744, lr=7.60e-07]01/27/2026 04:09:49 - INFO - __main__ - 
[Step 4200] ✅ Loss in normal range (0.9744)
01/27/2026 04:09:49 - INFO - __main__ -   Loss avg (last 100): 0.7421
01/27/2026 04:09:49 - INFO - __main__ -   Loss range: [0.3454, 1.1947]
01/27/2026 04:09:49 - INFO - __main__ - 
🔍 Running validation at step 4200...
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 4200 (parquet mode)...
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 4200...
01/27/2026 04:09:49 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/27/2026 04:09:49 - ERROR - __main__ - Validation failed: 'NoneType' object has no attribute 'parameters'

[Step 4200] Training Debug Info:
  Loss: 1.145105
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0008, std: 0.9258
  Noise mean: -0.0021, std: 1.0000
  Target mean: -0.0029, std: 1.3594
  Model pred mean: -0.0001, std: 0.8477
  Sigmas: [0.173828125]... (timesteps: [174.0])

[Step 4200] Training Debug Info:
  Loss: 0.383743
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0048, std: 0.9258
  Noise mean: 0.0014, std: 0.9961
  Target mean: -0.0033, std: 1.3594
  Model pred mean: -0.0009, std: 1.2109
  Sigmas: [0.76953125]... (timesteps: [771.0])

[Step 4200] Training Debug Info:
  Loss: 0.495023
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0008, std: 0.9180
  Noise mean: 0.0023, std: 1.0000
  Target mean: 0.0015, std: 1.3594
  Model pred mean: -0.0010, std: 1.1641
  Sigmas: [0.65234375]... (timesteps: [653.0])

[Step 4200] Training Debug Info:
  Loss: 0.367083
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0188, std: 0.8750
  Noise mean: -0.0027, std: 1.0000
  Target mean: 0.0161, std: 1.3281
  Model pred mean: 0.0117, std: 1.1797
  Sigmas: [0.8828125]... (timesteps: [881.0])
Steps:  84%|████████▍ | 4201/5000 [40:00<2:40:17, 12.04s/it, loss=0.9744, lr=7.60e-07]Steps:  84%|████████▍ | 4201/5000 [40:00<2:40:17, 12.04s/it, loss=0.3671, lr=7.58e-07]Steps:  84%|████████▍ | 4202/5000 [40:12<2:40:11, 12.04s/it, loss=0.3671, lr=7.58e-07]Steps:  84%|████████▍ | 4202/5000 [40:12<2:40:11, 12.04s/it, loss=0.7486, lr=7.56e-07]Steps:  84%|████████▍ | 4203/5000 [40:24<2:39:38, 12.02s/it, loss=0.7486, lr=7.56e-07]Steps:  84%|████████▍ | 4203/5000 [40:24<2:39:38, 12.02s/it, loss=0.6135, lr=7.54e-07]Steps:  84%|████████▍ | 4204/5000 [40:36<2:38:59, 11.98s/it, loss=0.6135, lr=7.54e-07]Steps:  84%|████████▍ | 4204/5000 [40:36<2:38:59, 11.98s/it, loss=0.4999, lr=7.52e-07]Steps:  84%|████████▍ | 4205/5000 [40:47<2:38:39, 11.97s/it, loss=0.4999, lr=7.52e-07]Steps:  84%|████████▍ | 4205/5000 [40:47<2:38:39, 11.97s/it, loss=1.0874, lr=7.51e-07]Steps:  84%|████████▍ | 4206/5000 [40:59<2:38:19, 11.96s/it, loss=1.0874, lr=7.51e-07]Steps:  84%|████████▍ | 4206/5000 [40:59<2:38:19, 11.96s/it, loss=0.8713, lr=7.49e-07]Steps:  84%|████████▍ | 4207/5000 [41:11<2:37:59, 11.95s/it, loss=0.8713, lr=7.49e-07]Steps:  84%|████████▍ | 4207/5000 [41:11<2:37:59, 11.95s/it, loss=0.6119, lr=7.47e-07]Steps:  84%|████████▍ | 4208/5000 [41:23<2:37:32, 11.93s/it, loss=0.6119, lr=7.47e-07]Steps:  84%|████████▍ | 4208/5000 [41:23<2:37:32, 11.93s/it, loss=1.0774, lr=7.45e-07]Steps:  84%|████████▍ | 4209/5000 [41:35<2:37:16, 11.93s/it, loss=1.0774, lr=7.45e-07]Steps:  84%|████████▍ | 4209/5000 [41:35<2:37:16, 11.93s/it, loss=0.3521, lr=7.43e-07]Steps:  84%|████████▍ | 4210/5000 [41:47<2:37:00, 11.92s/it, loss=0.3521, lr=7.43e-07]Steps:  84%|████████▍ | 4210/5000 [41:47<2:37:00, 11.92s/it, loss=0.5769, lr=7.41e-07]
[Step 4210] Training Debug Info:
  Loss: 1.074803
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0012, std: 0.9102
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0004, std: 1.3516
  Model pred mean: -0.0019, std: 0.8633
  Sigmas: [0.050048828125]... (timesteps: [50.0])

[Step 4210] Training Debug Info:
  Loss: 0.493839
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: -0.0077, std: 0.9375
  Noise mean: 0.0014, std: 1.0000
  Target mean: 0.0090, std: 1.3750
  Model pred mean: 0.0145, std: 1.1875
  Sigmas: [0.71484375]... (timesteps: [714.0])

[Step 4210] Training Debug Info:
  Loss: 0.481214
  Latent shape: torch.Size([1, 32, 114, 78]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0183, std: 0.9062
  Noise mean: -0.0004, std: 1.0000
  Target mean: -0.0187, std: 1.3516
  Model pred mean: -0.0192, std: 1.1641
  Sigmas: [0.6328125]... (timesteps: [633.0])

[Step 4210] Training Debug Info:
  Loss: 0.805084
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0057, std: 0.8711
  Noise mean: -0.0021, std: 1.0000
  Target mean: -0.0078, std: 1.3281
  Model pred mean: -0.0061, std: 0.9766
  Sigmas: [0.470703125]... (timesteps: [471.0])
Steps:  84%|████████▍ | 4211/5000 [41:59<2:37:07, 11.95s/it, loss=0.5769, lr=7.41e-07]Steps:  84%|████████▍ | 4211/5000 [41:59<2:37:07, 11.95s/it, loss=0.8051, lr=7.40e-07]Steps:  84%|████████▍ | 4212/5000 [42:11<2:36:44, 11.93s/it, loss=0.8051, lr=7.40e-07]Steps:  84%|████████▍ | 4212/5000 [42:11<2:36:44, 11.93s/it, loss=0.9433, lr=7.38e-07]Steps:  84%|████████▍ | 4213/5000 [42:23<2:36:25, 11.93s/it, loss=0.9433, lr=7.38e-07]Steps:  84%|████████▍ | 4213/5000 [42:23<2:36:25, 11.93s/it, loss=0.4997, lr=7.36e-07]Steps:  84%|████████▍ | 4214/5000 [42:35<2:36:14, 11.93s/it, loss=0.4997, lr=7.36e-07]Steps:  84%|████████▍ | 4214/5000 [42:35<2:36:14, 11.93s/it, loss=0.6055, lr=7.34e-07]Steps:  84%|████████▍ | 4215/5000 [42:47<2:36:03, 11.93s/it, loss=0.6055, lr=7.34e-07]Steps:  84%|████████▍ | 4215/5000 [42:47<2:36:03, 11.93s/it, loss=0.3817, lr=7.32e-07]Steps:  84%|████████▍ | 4216/5000 [42:59<2:35:48, 11.92s/it, loss=0.3817, lr=7.32e-07]Steps:  84%|████████▍ | 4216/5000 [42:59<2:35:48, 11.92s/it, loss=0.7690, lr=7.30e-07]Steps:  84%|████████▍ | 4217/5000 [43:10<2:35:22, 11.91s/it, loss=0.7690, lr=7.30e-07]Steps:  84%|████████▍ | 4217/5000 [43:10<2:35:22, 11.91s/it, loss=0.6844, lr=7.29e-07]Steps:  84%|████████▍ | 4218/5000 [43:22<2:35:07, 11.90s/it, loss=0.6844, lr=7.29e-07]Steps:  84%|████████▍ | 4218/5000 [43:22<2:35:07, 11.90s/it, loss=0.4722, lr=7.27e-07]Steps:  84%|████████▍ | 4219/5000 [43:34<2:35:04, 11.91s/it, loss=0.4722, lr=7.27e-07]Steps:  84%|████████▍ | 4219/5000 [43:34<2:35:04, 11.91s/it, loss=0.4889, lr=7.25e-07]Steps:  84%|████████▍ | 4220/5000 [43:46<2:35:13, 11.94s/it, loss=0.4889, lr=7.25e-07]Steps:  84%|████████▍ | 4220/5000 [43:46<2:35:13, 11.94s/it, loss=1.0199, lr=7.23e-07]
[Step 4220] Training Debug Info:
  Loss: 0.816726
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0181, std: 0.8945
  Noise mean: 0.0006, std: 0.9961
  Target mean: -0.0175, std: 1.3359
  Model pred mean: -0.0200, std: 0.9883
  Sigmas: [0.44921875]... (timesteps: [449.0])

[Step 4220] Training Debug Info:
  Loss: 0.793663
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0062, std: 0.9375
  Noise mean: -0.0034, std: 1.0000
  Target mean: -0.0096, std: 1.3750
  Model pred mean: -0.0040, std: 1.0469
  Sigmas: [0.423828125]... (timesteps: [424.0])

[Step 4220] Training Debug Info:
  Loss: 1.021999
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0034, std: 0.9258
  Noise mean: -0.0007, std: 1.0000
  Target mean: 0.0027, std: 1.3594
  Model pred mean: 0.0029, std: 0.9180
  Sigmas: [0.00799560546875]... (timesteps: [8.0])

[Step 4220] Training Debug Info:
  Loss: 0.625683
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0017, std: 0.8945
  Noise mean: -0.0031, std: 0.9961
  Target mean: -0.0048, std: 1.3359
  Model pred mean: -0.0024, std: 1.0781
  Sigmas: [0.55078125]... (timesteps: [552.0])
Steps:  84%|████████▍ | 4221/5000 [43:58<2:34:58, 11.94s/it, loss=1.0199, lr=7.23e-07]Steps:  84%|████████▍ | 4221/5000 [43:58<2:34:58, 11.94s/it, loss=0.6257, lr=7.21e-07]Steps:  84%|████████▍ | 4222/5000 [44:10<2:34:21, 11.90s/it, loss=0.6257, lr=7.21e-07]Steps:  84%|████████▍ | 4222/5000 [44:10<2:34:21, 11.90s/it, loss=0.4289, lr=7.20e-07]Steps:  84%|████████▍ | 4223/5000 [44:22<2:33:53, 11.88s/it, loss=0.4289, lr=7.20e-07]Steps:  84%|████████▍ | 4223/5000 [44:22<2:33:53, 11.88s/it, loss=1.0865, lr=7.18e-07]Steps:  84%|████████▍ | 4224/5000 [44:34<2:33:52, 11.90s/it, loss=1.0865, lr=7.18e-07]Steps:  84%|████████▍ | 4224/5000 [44:34<2:33:52, 11.90s/it, loss=0.6917, lr=7.16e-07]Steps:  84%|████████▍ | 4225/5000 [44:46<2:33:33, 11.89s/it, loss=0.6917, lr=7.16e-07]Steps:  84%|████████▍ | 4225/5000 [44:46<2:33:33, 11.89s/it, loss=0.5376, lr=7.14e-07]Steps:  85%|████████▍ | 4226/5000 [44:57<2:32:51, 11.85s/it, loss=0.5376, lr=7.14e-07]Steps:  85%|████████▍ | 4226/5000 [44:57<2:32:51, 11.85s/it, loss=0.6458, lr=7.12e-07]Steps:  85%|████████▍ | 4227/5000 [45:09<2:32:51, 11.87s/it, loss=0.6458, lr=7.12e-07]Steps:  85%|████████▍ | 4227/5000 [45:09<2:32:51, 11.87s/it, loss=0.9605, lr=7.11e-07]Steps:  85%|████████▍ | 4228/5000 [45:21<2:32:33, 11.86s/it, loss=0.9605, lr=7.11e-07]Steps:  85%|████████▍ | 4228/5000 [45:21<2:32:33, 11.86s/it, loss=0.3538, lr=7.09e-07]Steps:  85%|████████▍ | 4229/5000 [45:33<2:33:18, 11.93s/it, loss=0.3538, lr=7.09e-07]Steps:  85%|████████▍ | 4229/5000 [45:33<2:33:18, 11.93s/it, loss=0.6903, lr=7.07e-07]Steps:  85%|████████▍ | 4230/5000 [45:45<2:33:15, 11.94s/it, loss=0.6903, lr=7.07e-07]Steps:  85%|████████▍ | 4230/5000 [45:45<2:33:15, 11.94s/it, loss=0.5175, lr=7.05e-07]
[Step 4230] Training Debug Info:
  Loss: 1.076909
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0041, std: 0.8984
  Noise mean: 0.0030, std: 1.0000
  Target mean: -0.0010, std: 1.3438
  Model pred mean: -0.0042, std: 0.8555
  Sigmas: [0.294921875]... (timesteps: [294.0])

[Step 4230] Training Debug Info:
  Loss: 0.778233
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0165, std: 0.8789
  Noise mean: 0.0038, std: 1.0000
  Target mean: -0.0126, std: 1.3281
  Model pred mean: -0.0125, std: 0.9961
  Sigmas: [0.486328125]... (timesteps: [487.0])

[Step 4230] Training Debug Info:
  Loss: 1.175695
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0206, std: 0.8711
  Noise mean: -0.0000, std: 1.0000
  Target mean: 0.0206, std: 1.3281
  Model pred mean: 0.0220, std: 0.7617
  Sigmas: [0.1259765625]... (timesteps: [126.0])

[Step 4230] Training Debug Info:
  Loss: 0.599932
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0112, std: 0.9102
  Noise mean: -0.0021, std: 1.0000
  Target mean: -0.0133, std: 1.3516
  Model pred mean: -0.0166, std: 1.1250
  Sigmas: [0.94921875]... (timesteps: [951.0])
Steps:  85%|████████▍ | 4231/5000 [45:57<2:32:59, 11.94s/it, loss=0.5175, lr=7.05e-07]Steps:  85%|████████▍ | 4231/5000 [45:57<2:32:59, 11.94s/it, loss=0.5999, lr=7.03e-07]Steps:  85%|████████▍ | 4232/5000 [46:09<2:32:40, 11.93s/it, loss=0.5999, lr=7.03e-07]Steps:  85%|████████▍ | 4232/5000 [46:09<2:32:40, 11.93s/it, loss=0.5909, lr=7.02e-07]Steps:  85%|████████▍ | 4233/5000 [46:21<2:32:05, 11.90s/it, loss=0.5909, lr=7.02e-07]Steps:  85%|████████▍ | 4233/5000 [46:21<2:32:05, 11.90s/it, loss=1.0706, lr=7.00e-07]Steps:  85%|████████▍ | 4234/5000 [46:33<2:32:11, 11.92s/it, loss=1.0706, lr=7.00e-07]Steps:  85%|████████▍ | 4234/5000 [46:33<2:32:11, 11.92s/it, loss=1.0268, lr=6.98e-07]Steps:  85%|████████▍ | 4235/5000 [46:45<2:32:07, 11.93s/it, loss=1.0268, lr=6.98e-07]Steps:  85%|████████▍ | 4235/5000 [46:45<2:32:07, 11.93s/it, loss=0.4960, lr=6.96e-07]Steps:  85%|████████▍ | 4236/5000 [46:57<2:31:51, 11.93s/it, loss=0.4960, lr=6.96e-07]Steps:  85%|████████▍ | 4236/5000 [46:57<2:31:51, 11.93s/it, loss=0.8928, lr=6.95e-07]Steps:  85%|████████▍ | 4237/5000 [47:09<2:31:36, 11.92s/it, loss=0.8928, lr=6.95e-07]Steps:  85%|████████▍ | 4237/5000 [47:09<2:31:36, 11.92s/it, loss=1.1200, lr=6.93e-07]Steps:  85%|████████▍ | 4238/5000 [47:21<2:31:42, 11.95s/it, loss=1.1200, lr=6.93e-07]Steps:  85%|████████▍ | 4238/5000 [47:21<2:31:42, 11.95s/it, loss=1.1886, lr=6.91e-07]Steps:  85%|████████▍ | 4239/5000 [47:33<2:31:34, 11.95s/it, loss=1.1886, lr=6.91e-07]Steps:  85%|████████▍ | 4239/5000 [47:33<2:31:34, 11.95s/it, loss=0.4979, lr=6.89e-07]Steps:  85%|████████▍ | 4240/5000 [47:45<2:31:11, 11.94s/it, loss=0.4979, lr=6.89e-07]Steps:  85%|████████▍ | 4240/5000 [47:45<2:31:11, 11.94s/it, loss=0.5929, lr=6.87e-07]
[Step 4240] Training Debug Info:
  Loss: 0.873300
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0068, std: 0.9922
  Noise mean: -0.0044, std: 1.0000
  Target mean: -0.0112, std: 1.4062
  Model pred mean: -0.0067, std: 1.0547
  Sigmas: [0.373046875]... (timesteps: [374.0])

[Step 4240] Training Debug Info:
  Loss: 0.585308
  Latent shape: torch.Size([1, 32, 108, 78]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0013, std: 0.8984
  Noise mean: -0.0001, std: 1.0000
  Target mean: -0.0014, std: 1.3438
  Model pred mean: 0.0004, std: 1.0859
  Sigmas: [0.9765625]... (timesteps: [976.0])

[Step 4240] Training Debug Info:
  Loss: 0.722513
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0052, std: 0.8633
  Noise mean: 0.0025, std: 1.0000
  Target mean: 0.0077, std: 1.3203
  Model pred mean: 0.0066, std: 1.0078
  Sigmas: [0.53125]... (timesteps: [530.0])

[Step 4240] Training Debug Info:
  Loss: 0.527698
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0002, std: 0.9492
  Noise mean: 0.0019, std: 1.0000
  Target mean: 0.0021, std: 1.3750
  Model pred mean: -0.0014, std: 1.1719
  Sigmas: [0.890625]... (timesteps: [891.0])
Steps:  85%|████████▍ | 4241/5000 [47:57<2:30:58, 11.93s/it, loss=0.5929, lr=6.87e-07]Steps:  85%|████████▍ | 4241/5000 [47:57<2:30:58, 11.93s/it, loss=0.5277, lr=6.86e-07]Steps:  85%|████████▍ | 4242/5000 [48:08<2:30:35, 11.92s/it, loss=0.5277, lr=6.86e-07]Steps:  85%|████████▍ | 4242/5000 [48:08<2:30:35, 11.92s/it, loss=0.8616, lr=6.84e-07]Steps:  85%|████████▍ | 4243/5000 [48:20<2:30:11, 11.90s/it, loss=0.8616, lr=6.84e-07]Steps:  85%|████████▍ | 4243/5000 [48:20<2:30:11, 11.90s/it, loss=0.4070, lr=6.82e-07]Steps:  85%|████████▍ | 4244/5000 [48:32<2:30:09, 11.92s/it, loss=0.4070, lr=6.82e-07]Steps:  85%|████████▍ | 4244/5000 [48:32<2:30:09, 11.92s/it, loss=0.7928, lr=6.80e-07]Steps:  85%|████████▍ | 4245/5000 [48:44<2:29:43, 11.90s/it, loss=0.7928, lr=6.80e-07]Steps:  85%|████████▍ | 4245/5000 [48:44<2:29:43, 11.90s/it, loss=1.0678, lr=6.79e-07]Steps:  85%|████████▍ | 4246/5000 [48:56<2:29:30, 11.90s/it, loss=1.0678, lr=6.79e-07]Steps:  85%|████████▍ | 4246/5000 [48:56<2:29:30, 11.90s/it, loss=0.8673, lr=6.77e-07]Steps:  85%|████████▍ | 4247/5000 [49:08<2:29:54, 11.95s/it, loss=0.8673, lr=6.77e-07]Steps:  85%|████████▍ | 4247/5000 [49:08<2:29:54, 11.95s/it, loss=0.3919, lr=6.75e-07]Steps:  85%|████████▍ | 4248/5000 [49:20<2:29:29, 11.93s/it, loss=0.3919, lr=6.75e-07]Steps:  85%|████████▍ | 4248/5000 [49:20<2:29:29, 11.93s/it, loss=1.0474, lr=6.73e-07]Steps:  85%|████████▍ | 4249/5000 [49:32<2:29:05, 11.91s/it, loss=1.0474, lr=6.73e-07]Steps:  85%|████████▍ | 4249/5000 [49:32<2:29:05, 11.91s/it, loss=1.1090, lr=6.72e-07]Steps:  85%|████████▌ | 4250/5000 [49:44<2:28:47, 11.90s/it, loss=1.1090, lr=6.72e-07]Steps:  85%|████████▌ | 4250/5000 [49:44<2:28:47, 11.90s/it, loss=0.5119, lr=6.70e-07]
[Step 4250] Training Debug Info:
  Loss: 1.090327
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0330, std: 0.9648
  Noise mean: 0.0033, std: 1.0000
  Target mean: -0.0297, std: 1.3906
  Model pred mean: -0.0334, std: 0.9141
  Sigmas: [0.10986328125]... (timesteps: [110.0])

[Step 4250] Training Debug Info:
  Loss: 0.401452
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0189, std: 0.9727
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0210, std: 1.3984
  Model pred mean: -0.0225, std: 1.2422
  Sigmas: [0.73046875]... (timesteps: [730.0])

[Step 4250] Training Debug Info:
  Loss: 0.367938
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0134, std: 0.9219
  Noise mean: -0.0057, std: 1.0000
  Target mean: -0.0190, std: 1.3594
  Model pred mean: -0.0209, std: 1.2188
  Sigmas: [0.73828125]... (timesteps: [739.0])

[Step 4250] Training Debug Info:
  Loss: 1.158345
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0260, std: 0.9219
  Noise mean: 0.0005, std: 1.0000
  Target mean: 0.0266, std: 1.3594
  Model pred mean: 0.0242, std: 0.8320
  Sigmas: [0.1416015625]... (timesteps: [142.0])
Steps:  85%|████████▌ | 4251/5000 [49:56<2:28:30, 11.90s/it, loss=0.5119, lr=6.70e-07]Steps:  85%|████████▌ | 4251/5000 [49:56<2:28:30, 11.90s/it, loss=1.1583, lr=6.68e-07]Steps:  85%|████████▌ | 4252/5000 [50:07<2:28:30, 11.91s/it, loss=1.1583, lr=6.68e-07]Steps:  85%|████████▌ | 4252/5000 [50:07<2:28:30, 11.91s/it, loss=1.1075, lr=6.66e-07]Steps:  85%|████████▌ | 4253/5000 [50:19<2:28:31, 11.93s/it, loss=1.1075, lr=6.66e-07]Steps:  85%|████████▌ | 4253/5000 [50:19<2:28:31, 11.93s/it, loss=1.1329, lr=6.65e-07]Steps:  85%|████████▌ | 4254/5000 [50:31<2:28:30, 11.94s/it, loss=1.1329, lr=6.65e-07]Steps:  85%|████████▌ | 4254/5000 [50:31<2:28:30, 11.94s/it, loss=0.4643, lr=6.63e-07]Steps:  85%|████████▌ | 4255/5000 [50:43<2:28:06, 11.93s/it, loss=0.4643, lr=6.63e-07]Steps:  85%|████████▌ | 4255/5000 [50:43<2:28:06, 11.93s/it, loss=0.6715, lr=6.61e-07]Steps:  85%|████████▌ | 4256/5000 [50:55<2:28:28, 11.97s/it, loss=0.6715, lr=6.61e-07]Steps:  85%|████████▌ | 4256/5000 [50:55<2:28:28, 11.97s/it, loss=0.5357, lr=6.59e-07]Steps:  85%|████████▌ | 4257/5000 [51:07<2:28:10, 11.97s/it, loss=0.5357, lr=6.59e-07]Steps:  85%|████████▌ | 4257/5000 [51:07<2:28:10, 11.97s/it, loss=1.1594, lr=6.58e-07]Steps:  85%|████████▌ | 4258/5000 [51:19<2:27:53, 11.96s/it, loss=1.1594, lr=6.58e-07]Steps:  85%|████████▌ | 4258/5000 [51:19<2:27:53, 11.96s/it, loss=0.4343, lr=6.56e-07]Steps:  85%|████████▌ | 4259/5000 [51:31<2:27:23, 11.93s/it, loss=0.4343, lr=6.56e-07]Steps:  85%|████████▌ | 4259/5000 [51:31<2:27:23, 11.93s/it, loss=0.3856, lr=6.54e-07]Steps:  85%|████████▌ | 4260/5000 [51:43<2:27:02, 11.92s/it, loss=0.3856, lr=6.54e-07]Steps:  85%|████████▌ | 4260/5000 [51:43<2:27:02, 11.92s/it, loss=0.4559, lr=6.53e-07]
[Step 4260] Training Debug Info:
  Loss: 0.471815
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0498, std: 0.9258
  Noise mean: -0.0008, std: 1.0000
  Target mean: -0.0505, std: 1.3672
  Model pred mean: -0.0479, std: 1.1719
  Sigmas: [0.83984375]... (timesteps: [841.0])

[Step 4260] Training Debug Info:
  Loss: 0.890374
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0192, std: 0.8906
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0176, std: 1.3359
  Model pred mean: -0.0192, std: 0.9453
  Sigmas: [0.40625]... (timesteps: [406.0])

[Step 4260] Training Debug Info:
  Loss: 0.790462
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0064, std: 0.9297
  Noise mean: 0.0001, std: 1.0000
  Target mean: 0.0065, std: 1.3594
  Model pred mean: 0.0071, std: 1.0391
  Sigmas: [0.421875]... (timesteps: [422.0])

[Step 4260] Training Debug Info:
  Loss: 1.151834
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0142, std: 0.9102
  Noise mean: -0.0026, std: 1.0000
  Target mean: 0.0115, std: 1.3516
  Model pred mean: 0.0140, std: 0.8242
  Sigmas: [0.1796875]... (timesteps: [180.0])
Steps:  85%|████████▌ | 4261/5000 [51:55<2:26:43, 11.91s/it, loss=0.4559, lr=6.53e-07]Steps:  85%|████████▌ | 4261/5000 [51:55<2:26:43, 11.91s/it, loss=1.1518, lr=6.51e-07]Steps:  85%|████████▌ | 4262/5000 [52:07<2:26:41, 11.93s/it, loss=1.1518, lr=6.51e-07]Steps:  85%|████████▌ | 4262/5000 [52:07<2:26:41, 11.93s/it, loss=0.9863, lr=6.49e-07]Steps:  85%|████████▌ | 4263/5000 [52:19<2:26:17, 11.91s/it, loss=0.9863, lr=6.49e-07]Steps:  85%|████████▌ | 4263/5000 [52:19<2:26:17, 11.91s/it, loss=0.3560, lr=6.47e-07]Steps:  85%|████████▌ | 4264/5000 [52:31<2:26:00, 11.90s/it, loss=0.3560, lr=6.47e-07]Steps:  85%|████████▌ | 4264/5000 [52:31<2:26:00, 11.90s/it, loss=0.3799, lr=6.46e-07]Steps:  85%|████████▌ | 4265/5000 [52:43<2:26:04, 11.93s/it, loss=0.3799, lr=6.46e-07]Steps:  85%|████████▌ | 4265/5000 [52:43<2:26:04, 11.93s/it, loss=0.3631, lr=6.44e-07]Steps:  85%|████████▌ | 4266/5000 [52:55<2:25:45, 11.91s/it, loss=0.3631, lr=6.44e-07]Steps:  85%|████████▌ | 4266/5000 [52:55<2:25:45, 11.91s/it, loss=1.0747, lr=6.42e-07]Steps:  85%|████████▌ | 4267/5000 [53:06<2:25:37, 11.92s/it, loss=1.0747, lr=6.42e-07]Steps:  85%|████████▌ | 4267/5000 [53:06<2:25:37, 11.92s/it, loss=0.4877, lr=6.41e-07]Steps:  85%|████████▌ | 4268/5000 [53:18<2:25:25, 11.92s/it, loss=0.4877, lr=6.41e-07]Steps:  85%|████████▌ | 4268/5000 [53:18<2:25:25, 11.92s/it, loss=1.1070, lr=6.39e-07]Steps:  85%|████████▌ | 4269/5000 [53:30<2:24:55, 11.90s/it, loss=1.1070, lr=6.39e-07]Steps:  85%|████████▌ | 4269/5000 [53:30<2:24:55, 11.90s/it, loss=0.3486, lr=6.37e-07]Steps:  85%|████████▌ | 4270/5000 [53:42<2:25:02, 11.92s/it, loss=0.3486, lr=6.37e-07]Steps:  85%|████████▌ | 4270/5000 [53:42<2:25:02, 11.92s/it, loss=1.0212, lr=6.35e-07]
[Step 4270] Training Debug Info:
  Loss: 0.994315
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0111, std: 0.9336
  Noise mean: 0.0005, std: 1.0000
  Target mean: -0.0106, std: 1.3672
  Model pred mean: -0.0128, std: 0.9336
  Sigmas: [0.2578125]... (timesteps: [257.0])

[Step 4270] Training Debug Info:
  Loss: 0.428437
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0264, std: 0.9258
  Noise mean: -0.0031, std: 1.0000
  Target mean: -0.0294, std: 1.3594
  Model pred mean: -0.0269, std: 1.1953
  Sigmas: [0.72265625]... (timesteps: [723.0])

[Step 4270] Training Debug Info:
  Loss: 1.114068
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0075, std: 0.9258
  Noise mean: 0.0016, std: 0.9961
  Target mean: -0.0060, std: 1.3594
  Model pred mean: -0.0074, std: 0.8594
  Sigmas: [0.09814453125]... (timesteps: [98.0])

[Step 4270] Training Debug Info:
  Loss: 1.169441
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0023, std: 0.9062
  Noise mean: 0.0006, std: 1.0000
  Target mean: 0.0029, std: 1.3516
  Model pred mean: 0.0019, std: 0.8047
  Sigmas: [0.15625]... (timesteps: [156.0])
Steps:  85%|████████▌ | 4271/5000 [53:54<2:24:57, 11.93s/it, loss=1.0212, lr=6.35e-07]Steps:  85%|████████▌ | 4271/5000 [53:54<2:24:57, 11.93s/it, loss=1.1694, lr=6.34e-07]Steps:  85%|████████▌ | 4272/5000 [54:06<2:24:38, 11.92s/it, loss=1.1694, lr=6.34e-07]Steps:  85%|████████▌ | 4272/5000 [54:06<2:24:38, 11.92s/it, loss=0.6509, lr=6.32e-07]Steps:  85%|████████▌ | 4273/5000 [54:18<2:24:19, 11.91s/it, loss=0.6509, lr=6.32e-07]Steps:  85%|████████▌ | 4273/5000 [54:18<2:24:19, 11.91s/it, loss=0.8424, lr=6.30e-07]Steps:  85%|████████▌ | 4274/5000 [54:30<2:24:37, 11.95s/it, loss=0.8424, lr=6.30e-07]Steps:  85%|████████▌ | 4274/5000 [54:30<2:24:37, 11.95s/it, loss=0.9223, lr=6.29e-07]Steps:  86%|████████▌ | 4275/5000 [54:42<2:24:17, 11.94s/it, loss=0.9223, lr=6.29e-07]Steps:  86%|████████▌ | 4275/5000 [54:42<2:24:17, 11.94s/it, loss=1.1539, lr=6.27e-07]Steps:  86%|████████▌ | 4276/5000 [54:54<2:23:48, 11.92s/it, loss=1.1539, lr=6.27e-07]Steps:  86%|████████▌ | 4276/5000 [54:54<2:23:48, 11.92s/it, loss=0.4367, lr=6.25e-07]Steps:  86%|████████▌ | 4277/5000 [55:06<2:23:17, 11.89s/it, loss=0.4367, lr=6.25e-07]Steps:  86%|████████▌ | 4277/5000 [55:06<2:23:17, 11.89s/it, loss=0.4283, lr=6.24e-07]Steps:  86%|████████▌ | 4278/5000 [55:17<2:23:02, 11.89s/it, loss=0.4283, lr=6.24e-07]Steps:  86%|████████▌ | 4278/5000 [55:17<2:23:02, 11.89s/it, loss=1.0680, lr=6.22e-07]Steps:  86%|████████▌ | 4279/5000 [55:29<2:22:50, 11.89s/it, loss=1.0680, lr=6.22e-07]Steps:  86%|████████▌ | 4279/5000 [55:29<2:22:50, 11.89s/it, loss=0.5388, lr=6.20e-07]Steps:  86%|████████▌ | 4280/5000 [55:41<2:22:49, 11.90s/it, loss=0.5388, lr=6.20e-07]Steps:  86%|████████▌ | 4280/5000 [55:41<2:22:49, 11.90s/it, loss=0.5831, lr=6.18e-07]
[Step 4280] Training Debug Info:
  Loss: 1.145887
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0034, std: 0.8828
  Noise mean: -0.0021, std: 1.0000
  Target mean: -0.0055, std: 1.3359
  Model pred mean: -0.0058, std: 0.8008
  Sigmas: [0.24609375]... (timesteps: [246.0])

[Step 4280] Training Debug Info:
  Loss: 1.128093
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0104, std: 0.9219
  Noise mean: -0.0034, std: 1.0000
  Target mean: -0.0139, std: 1.3594
  Model pred mean: -0.0101, std: 0.8516
  Sigmas: [0.2373046875]... (timesteps: [237.0])

[Step 4280] Training Debug Info:
  Loss: 0.424370
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0162, std: 0.9414
  Noise mean: 0.0019, std: 1.0000
  Target mean: -0.0143, std: 1.3672
  Model pred mean: -0.0143, std: 1.2031
  Sigmas: [0.76953125]... (timesteps: [769.0])

[Step 4280] Training Debug Info:
  Loss: 1.103400
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0200, std: 0.8594
  Noise mean: 0.0050, std: 1.0000
  Target mean: 0.0249, std: 1.3203
  Model pred mean: 0.0208, std: 0.8008
  Sigmas: [0.05908203125]... (timesteps: [59.0])
Steps:  86%|████████▌ | 4281/5000 [55:53<2:22:48, 11.92s/it, loss=0.5831, lr=6.18e-07]Steps:  86%|████████▌ | 4281/5000 [55:53<2:22:48, 11.92s/it, loss=1.1034, lr=6.17e-07]Steps:  86%|████████▌ | 4282/5000 [56:05<2:22:19, 11.89s/it, loss=1.1034, lr=6.17e-07]Steps:  86%|████████▌ | 4282/5000 [56:05<2:22:19, 11.89s/it, loss=1.1348, lr=6.15e-07]Steps:  86%|████████▌ | 4283/5000 [56:17<2:22:25, 11.92s/it, loss=1.1348, lr=6.15e-07]Steps:  86%|████████▌ | 4283/5000 [56:17<2:22:25, 11.92s/it, loss=1.0513, lr=6.13e-07]Steps:  86%|████████▌ | 4284/5000 [56:29<2:22:12, 11.92s/it, loss=1.0513, lr=6.13e-07]Steps:  86%|████████▌ | 4284/5000 [56:29<2:22:12, 11.92s/it, loss=0.9576, lr=6.12e-07]Steps:  86%|████████▌ | 4285/5000 [56:41<2:21:48, 11.90s/it, loss=0.9576, lr=6.12e-07]Steps:  86%|████████▌ | 4285/5000 [56:41<2:21:48, 11.90s/it, loss=1.0742, lr=6.10e-07]Steps:  86%|████████▌ | 4286/5000 [56:53<2:21:31, 11.89s/it, loss=1.0742, lr=6.10e-07]Steps:  86%|████████▌ | 4286/5000 [56:53<2:21:31, 11.89s/it, loss=1.2229, lr=6.08e-07]Steps:  86%|████████▌ | 4287/5000 [57:05<2:21:38, 11.92s/it, loss=1.2229, lr=6.08e-07]Steps:  86%|████████▌ | 4287/5000 [57:05<2:21:38, 11.92s/it, loss=0.3901, lr=6.07e-07]Steps:  86%|████████▌ | 4288/5000 [57:17<2:21:09, 11.90s/it, loss=0.3901, lr=6.07e-07]Steps:  86%|████████▌ | 4288/5000 [57:17<2:21:09, 11.90s/it, loss=0.4105, lr=6.05e-07]Steps:  86%|████████▌ | 4289/5000 [57:28<2:20:54, 11.89s/it, loss=0.4105, lr=6.05e-07]Steps:  86%|████████▌ | 4289/5000 [57:28<2:20:54, 11.89s/it, loss=0.9487, lr=6.03e-07]Steps:  86%|████████▌ | 4290/5000 [57:40<2:20:50, 11.90s/it, loss=0.9487, lr=6.03e-07]Steps:  86%|████████▌ | 4290/5000 [57:40<2:20:50, 11.90s/it, loss=0.3903, lr=6.02e-07]
[Step 4290] Training Debug Info:
  Loss: 1.039577
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0045, std: 0.9375
  Noise mean: -0.0015, std: 1.0000
  Target mean: -0.0060, std: 1.3672
  Model pred mean: -0.0037, std: 0.9141
  Sigmas: [0.2890625]... (timesteps: [290.0])

[Step 4290] Training Debug Info:
  Loss: 0.971949
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0569, std: 0.9336
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0554, std: 1.3672
  Model pred mean: -0.0566, std: 0.9492
  Sigmas: [0.2265625]... (timesteps: [227.0])

[Step 4290] Training Debug Info:
  Loss: 1.009972
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0170, std: 0.9141
  Noise mean: -0.0006, std: 0.9961
  Target mean: -0.0176, std: 1.3516
  Model pred mean: -0.0167, std: 0.9062
  Sigmas: [0.33203125]... (timesteps: [332.0])

[Step 4290] Training Debug Info:
  Loss: 1.029137
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0082, std: 0.8906
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0095, std: 1.3438
  Model pred mean: -0.0057, std: 0.8789
  Sigmas: [0.01202392578125]... (timesteps: [12.0])
Steps:  86%|████████▌ | 4291/5000 [57:52<2:20:49, 11.92s/it, loss=0.3903, lr=6.02e-07]Steps:  86%|████████▌ | 4291/5000 [57:52<2:20:49, 11.92s/it, loss=1.0291, lr=6.00e-07]Steps:  86%|████████▌ | 4292/5000 [58:04<2:20:51, 11.94s/it, loss=1.0291, lr=6.00e-07]Steps:  86%|████████▌ | 4292/5000 [58:04<2:20:51, 11.94s/it, loss=0.9828, lr=5.98e-07]Steps:  86%|████████▌ | 4293/5000 [58:16<2:20:24, 11.92s/it, loss=0.9828, lr=5.98e-07]Steps:  86%|████████▌ | 4293/5000 [58:16<2:20:24, 11.92s/it, loss=1.0166, lr=5.97e-07]Steps:  86%|████████▌ | 4294/5000 [58:28<2:20:15, 11.92s/it, loss=1.0166, lr=5.97e-07]Steps:  86%|████████▌ | 4294/5000 [58:28<2:20:15, 11.92s/it, loss=0.4035, lr=5.95e-07]Steps:  86%|████████▌ | 4295/5000 [58:40<2:19:51, 11.90s/it, loss=0.4035, lr=5.95e-07]Steps:  86%|████████▌ | 4295/5000 [58:40<2:19:51, 11.90s/it, loss=1.0745, lr=5.93e-07]Steps:  86%|████████▌ | 4296/5000 [58:52<2:19:36, 11.90s/it, loss=1.0745, lr=5.93e-07]Steps:  86%|████████▌ | 4296/5000 [58:52<2:19:36, 11.90s/it, loss=0.3989, lr=5.92e-07]Steps:  86%|████████▌ | 4297/5000 [59:04<2:19:27, 11.90s/it, loss=0.3989, lr=5.92e-07]Steps:  86%|████████▌ | 4297/5000 [59:04<2:19:27, 11.90s/it, loss=1.0539, lr=5.90e-07]Steps:  86%|████████▌ | 4298/5000 [59:16<2:18:59, 11.88s/it, loss=1.0539, lr=5.90e-07]Steps:  86%|████████▌ | 4298/5000 [59:16<2:18:59, 11.88s/it, loss=0.4654, lr=5.89e-07]Steps:  86%|████████▌ | 4299/5000 [59:27<2:18:48, 11.88s/it, loss=0.4654, lr=5.89e-07]Steps:  86%|████████▌ | 4299/5000 [59:27<2:18:48, 11.88s/it, loss=0.4663, lr=5.87e-07]Steps:  86%|████████▌ | 4300/5000 [59:39<2:18:42, 11.89s/it, loss=0.4663, lr=5.87e-07]Steps:  86%|████████▌ | 4300/5000 [59:39<2:18:42, 11.89s/it, loss=0.9981, lr=5.85e-07]01/27/2026 04:29:41 - INFO - __main__ - 
[Step 4300] ✅ Loss in normal range (0.9981)
01/27/2026 04:29:41 - INFO - __main__ -   Loss avg (last 100): 0.7482
01/27/2026 04:29:41 - INFO - __main__ -   Loss range: [0.3486, 1.2229]

[Step 4300] Training Debug Info:
  Loss: 0.917149
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0096, std: 0.8867
  Noise mean: 0.0000, std: 1.0000
  Target mean: -0.0095, std: 1.3359
  Model pred mean: -0.0092, std: 0.9336
  Sigmas: [0.400390625]... (timesteps: [401.0])

[Step 4300] Training Debug Info:
  Loss: 0.358855
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0003, std: 0.8945
  Noise mean: 0.0020, std: 1.0000
  Target mean: 0.0023, std: 1.3438
  Model pred mean: -0.0014, std: 1.2109
  Sigmas: [0.875]... (timesteps: [875.0])

[Step 4300] Training Debug Info:
  Loss: 0.839965
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0159, std: 0.9375
  Noise mean: 0.0011, std: 1.0000
  Target mean: -0.0147, std: 1.3672
  Model pred mean: -0.0175, std: 1.0156
  Sigmas: [0.404296875]... (timesteps: [404.0])

[Step 4300] Training Debug Info:
  Loss: 1.157084
  Latent shape: torch.Size([1, 32, 54, 156]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0026, std: 0.9141
  Noise mean: 0.0011, std: 1.0000
  Target mean: 0.0036, std: 1.3594
  Model pred mean: 0.0028, std: 0.8242
  Sigmas: [0.1435546875]... (timesteps: [144.0])
