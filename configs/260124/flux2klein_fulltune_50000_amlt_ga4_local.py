"""
Flux2Klein 9B Full Fine-tuning Config for Azure ML (AMLT)
- gradient_accumulation_steps = 4
- Effective batch = 1 × 4 × 4 GPUs = 16
- num_train_epochs = 1
- ~32.5K optimization steps for 520K samples
"""

_base_ = [
    '../base_config.py',
]

# Model Configuration
model_type = 'Flux2Klein'
base_filesys_path = "/home/v-yuxluo/yuxuanluo/"

model_dir = base_filesys_path + "models/Flux2Klein_9B"
model_path = model_dir
model_name_or_path = None

# ====== Model Weights ======
pretrained_model_name_or_path = "black-forest-labs/FLUX.2-klein-base-9B"
# HuggingFace token is now managed centrally
# Priority: Environment variable HF_TOKEN > secrets.py > None
# DO NOT hardcode tokens here - use secrets.py for local dev, or HF_TOKEN env var for AMLT
huggingface_token = None  # Will be resolved from env or secrets.py

transformer_cfg = dict(
    type='Flux2Transformer2DModel',
)

# Training Configuration
use_lora = False  # Full fine-tuning
train_batch_size = 1
gradient_accumulation_steps = 4  # Effective batch = 1 * 4 * 4 GPUs = 16

# ====== Dataset Configuration ======
use_parquet_dataset = True

dataset_cfg = dict(
    type='ArXiVParquetDatasetV2',
    base_dir=base_filesys_path,
    parquet_base_path='ArXiV_parquet/Flux2Klein9BParquet_260118',
    num_workers=4,
    num_train_examples=None,
    debug_mode=False,
    is_main_process=True,
    stat_data=False,
)

sampler_cfg = dict(
    type='DistributedBucketSamplerV2',
    dataset=None,
    batch_size=1,
    num_replicas=None,
    rank=None,
    drop_last=True,
    shuffle=True,
)

# ====== Training Iteration Function ======
train_iteration_func = 'Flux2Klein_fulltune_train_iteration'

# Steps: 32500 optimizer steps
# With GA=4 and 4 GPUs, effective batch = 16
# Total samples seen = 32500 * 16 = 520K samples (~1 epoch of 538K dataset)
max_train_steps = 32500
num_train_epochs = 1

checkpointing_steps = 250
checkpoints_total_limit = None
validation_steps = 250

# Learning Rate
learning_rate = 1e-5
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_weight_decay = 0.01
adam_epsilon = 1e-8

# Warmup - ~1% of total steps
lr_warmup_steps = 500
lr_scheduler = "cosine"

# Mixed Precision
mixed_precision = "bf16"
allow_tf32 = True

# Dataset - Parquet format
from_hf_hub = False
dataset_parquet_path = base_filesys_path + "ArXiV_parquet/Flux2Klein9BParquet_260118/"

# Data Processing
max_sequence_length = 1024 
dataloader_num_workers = 4
pin_memory = True

# Cache
cache_latents_to_disk = False
cache_text_embeddings_to_disk = False

# Validation
validation_guidance_scale = 3.5
num_inference_steps = 28

# Output
model_output_dir = base_filesys_path + "experiments/260124_fluxklein9B_fulltune_GA4"
output_dir = model_output_dir

# Logging
logging_steps = 10
logging_dir = model_output_dir + "/logs"
log_with = "wandb"
tracker_project_name = "0124localOpenSciDraw_Flux2Klein_9B_Fulltune"
tracker_run_name = "260124_GA4_adamw_1e-5"

# Resume
resume_from_checkpoint = "latest"

# Logging verbosity
verbose_logging = False  # Set to True to enable detailed health/loss logs every 100 steps

# Debug
report_to = "wandb"
wandb_project = "OpenSciDraw_Flux2Klein_9B_0124_Fulltune"

seed = 42



validation_prompts = [
    "The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", #2508.18950_0_img_SINGLE.png 
    "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", #2507.04841/2507.04841_0_img_SINGLE.png
    "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", #2507.20331
    "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", #2509.02962_1_img_SINGLE.pn
    "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", #2507.00761_6_img_SINGLE.png 
    "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", #2508.20279_1_img_SINGLE.png    
    "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", #2509.09525_23_MULTIFIGURE_COMPOSED.png 
    "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", #2509.08205_1_img_SINGLE.png 
    "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", #2025/2506.23207/2506.23207_1_img_SINGLE.png 
    "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", #2025/2508.04427/2508.04427_6_img_SINGLE.png
    "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", #2404.14604_0_img_SINGLE.png   
    "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \(\hat{y}^{(i)}\), and below is the input, denoted as \(x^{(i)}\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \(x^{(i)}\) is fed into the DNN, producing \(\hat{y}^{(i)}\). A dashed orange curved arrow connects \(\hat{y}^{(i)}\) to the ground truth \(y^{(i)}\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \(x^{(i)}\) (partial) and \(y^{(j)}\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \(\hat{y}^{(i)}\) to \(x^{(i)}\) labeled 'matching loss', enforcing shape consistency with the input, and another from \(\hat{y}^{(i)}\) to \(y^{(j)}\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \(\hat{y}_1^{(i)}, \hat{y}_2^{(i)}, ..., \hat{y}_k^{(i)}\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \(x^{(i)}\) going into the DNN, producing \(\hat{y}^{(i)}\). A dashed orange curved arrow loops back from \(\hat{y}^{(i)}\) to \(x^{(i)}\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements.", # 2307.14726_0_img_SINGLE.png 
]

resolution_list = [ #w, h
    [576, 960],
    [576, 960],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [1008, 576],
    [768, 720],
    [768, 720],
    
]