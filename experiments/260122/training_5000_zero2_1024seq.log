==========================================
Starting Flux2Klein Full Fine-tuning (5000 steps)
Using DeepSpeed ZeRO-2 with 4 GPUs
==========================================
Thu Jan 22 03:23:53 UTC 2026
W0122 03:24:01.243000 70423 site-packages/torch/distributed/run.py:793] 
W0122 03:24:01.243000 70423 site-packages/torch/distributed/run.py:793] *****************************************
W0122 03:24:01.243000 70423 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0122 03:24:01.243000 70423 site-packages/torch/distributed/run.py:793] *****************************************
Config (path: configs/260122/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': '***REMOVED***', 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 500, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 5, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 500, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260122/flux2klein_fulltune_5000.py'}
Config (path: configs/260122/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': '***REMOVED***', 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 500, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 5, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 500, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260122/flux2klein_fulltune_5000.py'}
Config (path: configs/260122/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': '***REMOVED***', 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 500, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 5, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 500, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260122/flux2klein_fulltune_5000.py'}
Config (path: configs/260122/flux2klein_fulltune_5000.py): {'seed': 42, 'device': 'cuda', 'dtype': 'float32', 'revision': None, 'variant': None, 'bnb_quantization_config_path': None, 'model_type': 'Flux2Klein', 'transformer_cfg': {'type': 'Flux2Transformer2DModel'}, 'pretrained_model_name_or_path': 'black-forest-labs/FLUX.2-klein-base-9B', 'huggingface_token': '***REMOVED***', 'use_lora': False, 'lora_layers': None, 'rank': 64, 'lora_alpha': 4, 'lora_dropout': 0.0, 'layer_weighting': 5.0, 'pos_embedding': 'rope', 'decoder_arch': 'vit', 'use_parquet_dataset': True, 'train_batch_size': 1, 'num_train_epochs': 100, 'max_train_steps': 5000, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'cache_latents': False, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'learning_rate': 1e-05, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'prodigy_beta3': None, 'prodigy_decouple': True, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'checkpointing_steps': 500, 'resume_from_checkpoint': None, 'checkpoints_total_limit': 5, 'mixed_precision': 'bf16', 'allow_tf32': True, 'upcast_before_saving': False, 'offload': False, 'report_to': 'wandb', 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'cache_dir': None, 'scale_lr': False, 'lr_num_cycles': 1, 'lr_power': 1.0, 'weighting_scheme': 'none', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'validation_guidance_scale': 3.5, 'dataset_cfg': {'type': 'ArXiVParquetDatasetV2', 'base_dir': '/home/v-yuxluo/data', 'parquet_base_path': 'ArXiV_parquet/flux_latents_test', 'num_workers': 4, 'num_train_examples': None, 'debug_mode': False, 'is_main_process': True, 'stat_data': False}, 'sampler_cfg': {'type': 'DistributedBucketSamplerV2', 'dataset': None, 'batch_size': 2, 'num_replicas': None, 'rank': None, 'drop_last': True, 'shuffle': True}, 'train_iteration_func': 'Flux2Klein_fulltune_train_iteration', 'model_output_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000', 'logging_dir': '/home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/logs', 'log_steps': 1, 'wandb_project': 'Flux2Klein-FullTune', 'run_name': 'flux2klein_9b_fulltune_5000steps', 'validation_func': 'Flux2Klein_fulltune_validation_func_parquet', 'validation_steps': 500, 'num_inference_steps': 28, 'validation_prompts': ["The figure illustrates a process hooking mechanism using the LD_PRELOAD environment variable to inject a custom data collection library, siren.so, into target ELF binary executables during runtime. The global layout is a top-down flowchart depicting the sequence of interactions from environment setup to data analysis. At the top, a light blue rectangular box labeled 'Environment Variable: LD_PRELOAD=siren.so' initiates the process. This points downward to a green rectangle labeled 'Dynamic Linker: ld.so', which branches into two paths: one to a light blue box 'Injected Library: siren.so' and another to a green box 'Shared Libraries: DT_NEEDED'. Both converge into a large green rectangular container labeled 'ELF Binary Executable', which contains three internal components arranged vertically. The first is a light blue hexagon labeled 'Constructor: Data Collection and UDP Sender', followed by a green rectangle 'Application Code: main()', and then another light blue hexagon 'Destructor: Data Collection and UDP Sender'. These indicate that the injected library's data collection routines are triggered at both process startup (via constructor) and shutdown (via destructor). An arrow from the destructor leads to a light blue rectangle 'Message Receiver: UDP Server', which in turn connects to a light blue cylinder labeled 'Database: SQLite'. From the database, a downward arrow leads to a light blue rectangle 'Post-processing and Consolidation: Python', which then connects leftward to another light blue rectangle 'Statistics and Similarity Analysis: Python'. All elements shaded in light blue represent components of the SIREN architecture, while green elements denote standard system or application components. The arrows indicate the direction of control flow and data transmission, showing how injected data is sent via UDP, received, stored, processed, and finally analyzed. The diagram emphasizes the non-intrusive nature of the hooking mechanism, leveraging dynamic linking to collect runtime data without modifying the target application’s source code.", "The figure presents an overview of four distinct end-to-end Task-Oriented Dialogue (TOD) approaches, arranged vertically as subfigures (a) through (d), each illustrating a different methodology for integrating language models into dialogue systems.\n\n[1] Global Layout and Structure:\nThe figure is divided into four horizontal sections, each representing a different approach. Each section contains a central model component at the top, with input/output modules below or connected via arrows. The layout follows a top-down flow, where user inputs lead to model processing and then to outputs such as actions or responses. Subfigure labels (a), (b), (c), and (d) are placed beneath each section, along with descriptive captions explaining the approach.\n\n[2] Visual Modules and Attributes:\nIn subfigure (a), labeled 'Full-shot approach with fine-tuning LM', a large light green rounded rectangle at the top represents a 'Pre-trained Language Model (e.g., GPT2, T5)', marked with a red flame icon. Below it, five light blue rectangular boxes labeled 'User', 'Belief State', 'DB', 'Action', and 'Resp' are aligned horizontally. Arrows connect these boxes to the model, indicating bidirectional interaction between the model and all components except 'Resp', which receives output from the model.\n\nSubfigure (b), titled 'Zero-shot approach via schema-guided prompting LLM', features a similar light green rounded rectangle labeled 'Large Language Model (e.g., GPT 3.5, GPT-4)', marked with a blue snowflake icon. Below, two yellow rounded rectangles labeled 'DST Prompter' and 'Policy Prompter' receive input from 'User' and 'DB' respectively, and feed into the LLM. The LLM outputs to 'Action' and 'Resp', both light blue boxes.\n\nSubfigure (c), 'Zero-shot approach via autonomous Agent LLM', shows a light green rounded rectangle containing a robot icon and a pink rounded rectangle labeled 'Instruction following LLM'. This module is labeled 'Large Language Model' and marked with a blue snowflake. A bidirectional arrow connects the 'User' box to the LLM, with 'Resp' labeled on the return path. To the right, a set of yellow boxes labeled 'API tool-1' through 'API tool-n' are connected to the LLM via a blue circular arrow, indicating iterative interaction.\n\nSubfigure (d), 'Spec-TOD (ours): Few-shot approach with specialized instruction-tuned LLM', displays a light green rounded rectangle labeled 'Specialized Task-Oriented LLM', marked with a red flame icon. Inside, a robot icon with a gear symbol is adjacent to a pink rounded rectangle labeled 'Specified-Task Instruct.'. A bidirectional arrow connects the 'User' box to this module, with 'Resp' labeled on the return path. To the right, a vertical stack of yellow boxes labeled 'Task-1 Spec. Rep.', 'Task-2 Spec. Rep.', ..., 'Task-m Spec. Rep.' is connected to the 'Specified-Task Instruct.' box via a blue circular arrow, indicating iterative refinement using task-specific representations.\n\n[3] Connections and Arrows:\nIn (a), arrows show bidirectional communication between the pre-trained LM and 'User', 'Belief State', and 'DB', while unidirectional arrows point from the LM to 'Action' and 'Resp'.\n\nIn (b), arrows go from 'User' to 'DST Prompter', from 'DB' to 'Policy Prompter', and from both prompters to the LLM. The LLM sends outputs to 'Action' and 'Resp'.\n\nIn (c), a bidirectional arrow links 'User' and the LLM, with 'Resp' labeled on the response path. A blue circular arrow connects the LLM to the API tools, indicating iterative tool calling.\n\nIn (d), a bidirectional arrow connects 'User' and the LLM, with 'Resp' on the return path. A blue circular arrow links the 'Specified-Task Instruct.' box to the stack of task-specific representations, suggesting iterative refinement using these representations.", "The figure illustrates a network architecture for a single-step diffusion model with an enhanced decoder. The global layout is horizontal, progressing from left to right, with multiple parallel input streams converging into a central processing unit before diverging again toward the output. On the far left, three distinct input conditioning vectors, labeled c₁, c₂, and c₃, are represented as gray rounded rectangles. Each of these inputs is processed by a separate blue parallelogram-shaped module labeled ε, indicating an encoder or feature extraction component. These encoders are marked as 'Frozen' according to the legend at the bottom right, which uses a blue snowflake icon to denote frozen modules. The outputs of these encoders are combined via two circular summation nodes (⊕), where the first summation node receives the output of ε(c₁) and ε(c₂), and the second summation node combines the result with ε(c₃). Additionally, a noise latent vector z_T, shown as a gray rounded rectangle, is fed directly into the first summation node. The combined feature representation from both summation nodes is then passed into a large, centrally located orange bowtie-shaped module labeled 'UNet'. This UNet is marked as 'Trained' in the legend, indicated by an orange flame icon, signifying it is the primary trainable component of the architecture. The UNet outputs a denoised latent representation, denoted as -ẑ₀, shown as a gray rounded rectangle. This output is then fed into a blue parallelogram-shaped decoder module labeled D, also marked as 'Frozen'. Prior to entering the decoder, an additional orange parallelogram-shaped module labeled ε_f, which is trained, provides auxiliary features that are concatenated or fused with the main latent stream before decoding. The final output emerges from the decoder D. The connections between all components are depicted using gray arrows, indicating the flow of data. The overall structure emphasizes a multi-scale feature fusion strategy, where conditioned features from multiple encoders are aggregated and combined with noise to guide the UNet’s denoising process, followed by reconstruction through a frozen decoder enhanced by an additional trained feature extractor ε_f.", "The figure presents a comparative diagram of four different defect detection tasks, labeled (a) ISDD, (b) MISDD, (c) MIISDD, and (d) MISDD-MM, illustrating variations in data modality handling and fusion strategies. The global layout consists of four vertically aligned workflows side-by-side, each depicting a distinct approach to processing RGB and 3D data inputs for defect detection. At the top of the diagram, a legend indicates that pink circles represent RGB Data and light green circles represent 3D Data, which are visually represented as cylindrical containers feeding into processing modules.\n\nIn workflow (a) ISDD, a single pink cylinder (RGB Data) feeds into a rectangular 'Model' box with a pale yellow fill and black border, which then outputs 'Defect'. This represents a unimodal approach using only RGB data.\n\nWorkflow (b) MISDD shows two parallel inputs: one pink cylinder (RGB Data) and one light green cylinder (3D Data), each feeding into separate 'Model' boxes. The outputs from both models converge into a 'Fusion' box (light gray fill, rounded rectangle), which then produces the 'Defect' output. This illustrates a multimodal setup where both modalities are fully available.\n\nWorkflow (c) MIISDD features a smaller pink cylinder (RGB Data) and a full-sized light green cylinder (3D Data), indicating a static, incomplete modality scenario where RGB data is reduced or partially missing. Both inputs feed into separate 'Model' boxes, whose outputs are fused in a 'Fusion' box before producing the 'Defect' result. This highlights a fixed modality incompleteness.\n\nWorkflow (d) MISDD-MM, the proposed method, includes two dashed-line cylinders above the actual input cylinders—one pink and one light green—symbolizing dynamic, potentially missing modalities. The actual pink and green cylinders feed into separate 'Model' boxes, which are connected by bidirectional dashed arrows labeled with an equals sign, suggesting alignment or interaction between the models. The outputs from these models are combined via a 'Text-guided Fusion' module (light gray, elongated rounded rectangle), which then generates the final 'Defect' output. This emphasizes multimodal learning under dynamic missing conditions, guided by textual information.\n\nAll 'Model' boxes are uniformly styled with pale yellow fill and black borders, while 'Fusion' and 'Text-guided Fusion' boxes use light gray fills with rounded corners. All connections are solid black arrows pointing downward, except for the bidirectional dashed arrows between models in (d). The figure’s caption clarifies that MISDD-MM differs from MIISDD by addressing dynamic missing modalities rather than static incompleteness.", "The figure illustrates a model evaluation framework for a diffusion-based prediction system, structured as a horizontal workflow from left to right. The global layout consists of an input stage on the far left, a central processing module, multiple inference outputs, and a comparison stage on the right for evaluating predictions against targets.\n\nAt the center is a rounded rectangular box labeled 'Diffusion Model' in bold black text, filled with light purple color and outlined in dark blue. This module receives two inputs: one from the left, labeled 'x_n', represented as a black square containing a white, irregularly shaped cluster resembling a cloud or porous structure; and another from above, labeled 'noise', indicated by a downward arrow. From the Diffusion Model, multiple downward arrows emerge, labeled collectively as 'Multiple inference', pointing to a sequence of output images arranged horizontally. These outputs are denoted as 'x̂_{n+1}^{(1)}', 'x̂_{n+1}^{(2)}', 'x̂_{n+1}^{(3)}', 'x̂_{n+1}^{(4)}', ..., up to 'x̂_{n+1}^{(m)}', each shown as a black square with a similar white cluster pattern, suggesting multiple stochastic realizations generated by the model.\n\nTo the right of these outputs, a large gray arrow points toward a comparison section enclosed in two dashed boxes stacked vertically. The top box, outlined in blue dashed lines and labeled 'target' in blue text at the top right, contains two side-by-side images: on the left, a black square with a white cluster labeled 'x_{n+1}', and on the right, a pinkish-red square with a red cluster. The bottom box, outlined in green dashed lines and labeled 'prediction' in green text at the bottom right, mirrors this layout with identical images, but labeled 'x̂_{n+1}^{en}' below them. This indicates that the ensemble prediction (denoted by 'en') is compared against the actual target data for evaluation.\n\nAll connections are represented by solid blue arrows, except for the final comparison arrow which is gray and thicker. Text labels are in black unless specified otherwise, with key terms like 'target' and 'prediction' colored to match their respective bounding boxes. The overall design emphasizes the stochastic nature of the diffusion model through multiple inference paths and highlights the evaluation process by visually contrasting predicted and actual outcomes.", "The figure illustrates a linear probing framework applied to a frozen multimodal large language model (LLM) across different decoder layers, specifically focusing on the last-token representation at layer k. The diagram is divided into two main sections: 'Linear Prob Training' (top) and 'Linear Prob Testing' (bottom), each depicting a distinct phase of the evaluation pipeline.\n\nIn the training phase, a training image (depicted as a photo of a German Shepherd in a field) is fed into a pink rounded rectangle labeled 'Vision Encoder', which is stacked above a 'Projector' module; both components are marked with blue snowflake icons indicating they are frozen during training. Simultaneously, an 'Anchor Question' is processed by a light green rounded rectangle labeled 'Tokenizer'. The outputs from the Vision Encoder and Tokenizer are represented as sequences of colored squares—red for visual features and green for textual tokens—which are then concatenated and passed through a series of vertical purple rectangles labeled 'Decoder Layer 1', 'Decoder Layer 2', ..., 'Decoder Layer k', each also marked with a blue snowflake icon to denote freezing. At the final layer, the last token (highlighted with a darker border) is extracted and fed into a yellow rounded rectangle labeled 'Linear', which has a small orange flame icon, symbolizing the trainable linear probe. This probe is connected to a 'CE Loss' (Cross-Entropy Loss) node, indicating the optimization objective during training.\n\nIn the testing phase, a different image (a German Shepherd lying on a wooden surface) is processed through the same frozen Vision Encoder and Projector modules. A 'Prompt Variant' (e.g., a modified or semantically altered version of the original question) is tokenized using the same Tokenizer. The resulting feature sequences again flow through the identical frozen decoder layers. The last token from the final decoder layer is extracted and passed to a second 'Linear' module, this time marked with a blue snowflake icon to indicate it is kept fixed (i.e., not retrained). This fixed probe outputs a prediction, which is evaluated against ground truth to compute 'Accuracy'. A dashed vertical line connects the training and testing Linear probes, emphasizing that the same probe weights are used in both phases.\n\nThe overall layout is horizontal, with data flowing left to right, and the two phases are vertically stacked. The visual modules are color-coded: pink for vision processing, green for text tokenization, purple for decoder layers, and yellow for the linear probe. All modules are rounded rectangles, except for the input images and text labels. The connections are solid arrows for data flow and a dashed arrow for parameter sharing between training and testing probes. The figure visually conveys the process of training a linear classifier on features extracted from a specific decoder layer and then evaluating its performance on new data under varied prompts, enabling layer-wise analysis of the model's learned representations.", "The figure presents a comparative architectural diagram illustrating two different approaches to managing heap growth in a system utilizing CXL (Compute Express Link) memory, labeled as (a) Vanilla DAX and (b) Our system. The global layout is split into two side-by-side panels, each depicting a virtual address space and associated CXL memory structure, with a shared caption at the bottom explaining the context: 'The result of heap growth during execution after restoring the heap area of function X on CXL memory.'\n\nIn panel (a), 'Vanilla DAX', the left side shows the 'Virtual Address Space of X' as a vertical stack of rectangular regions. The top region is blank, followed by a gray-shaded rectangle labeled 'Heap X' with diagonal black stripes. Below it is a red-shaded rectangle labeled 'Heap Growth' with red diagonal stripes. A dashed blue arrow extends from the 'Heap X' region to a 'CXL Memory' block on the right, which contains two gray rectangles labeled 'Image X' and 'Image Y'. A solid red arrow points downward from the 'Heap Growth' region to a label 'Leakage' in red text, indicating that uncontrolled heap expansion causes data to spill over into unintended memory areas.\n\nIn panel (b), 'Our system', the same 'Virtual Address Space of X' is shown, with 'Heap X' (gray, diagonal black stripes) and 'Heap Growth' (red, diagonal red stripes) stacked vertically. However, the 'Heap Growth' region now connects via a dashed red arrow to a new memory component labeled 'Local Memory' below the CXL Memory block. This 'Local Memory' is a red-shaded rectangle labeled 'Private', signifying dedicated private memory for heap expansion. The CXL Memory block above still contains 'Image X' and 'Image Y', but the dashed blue arrow from 'Heap X' to 'Image X' remains, while the 'Heap Growth' is now isolated to the private local memory, preventing leakage.\n\nThe visual modules are primarily rectangular blocks with distinct fill patterns: gray with black diagonal lines for 'Heap X', red with red diagonal lines for 'Heap Growth', and solid red for 'Private' memory. Text labels are black except for 'Leakage', which is red. Arrows are dashed (blue for mapping to CXL, red for growth to local memory) or solid (red for leakage). The connections show a clear contrast: in Vanilla DAX, heap growth leads to leakage into CXL memory, whereas in the proposed system, heap growth is directed to a private local memory, thus avoiding leakage and improving memory safety.", "The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep learning network designed for image processing tasks, likely involving background estimation, target extraction, noise reduction, and image reconstruction. The global layout consists of a top-level pipeline showing K sequential stages (Stage 1, Stage k, Stage K), each containing four modular components: SEBEM (Squeeze-and-Excitation Background Estimation Module), SETEM (Squeeze-and-Excitation Target Extraction Module), SENRM (Squeeze-and-Excitation Noise Reduction Module), and SEIRM (Squeeze-and-Excitation Image Reconstruction Module). These modules are arranged horizontally within each stage, forming a consistent processing flow from left to right. The entire pipeline begins with an 'Original' grayscale input image on the far left and ends with a 'Target' output image on the far right. Each stage outputs intermediate representations labeled B^k, T^k, N^k, D^k, corresponding to background, target, noise, and reconstructed image features respectively.\n\nBelow the main pipeline, a detailed breakdown of a single stage is shown, enclosed in a dashed box. This expanded view reveals the internal structure of each module. SEBEM is depicted in light blue, SETEM in light green, SENRM in pale yellow, and SEIRM in gray. Each module contains convolutional layers (represented by rectangular blocks with varying colors indicating kernel size and channel dimensions), activation functions (light yellow blocks), batch normalization (pink blocks), and a Squeeze-and-Excitation Network (gray block with 'Squeeze-and-Excitation Network' label). The modules are interconnected via element-wise addition operations (⊕ symbols) and feature transmission paths. Specifically, SEBEM receives inputs from previous stage outputs (D^{k-1}, T^{k-1}, N^{k-1}) and produces B^k; SETEM takes B^k and generates T^k; SENRM processes T^k to produce N^k; and SEIRM uses N^k to generate D^k.\n\nConnections between modules and stages are indicated by arrows with distinct colors and labels in a legend below the main pipeline: black arrows denote 'module transmission path', red arrows represent 'ε^k transmission path', purple arrows indicate 'σ^k transmission path', and orange arrows show 'stage transmission path'. These paths illustrate how features are propagated across modules and stages, including residual or skip connections.\n\nIn the bottom-right corner, a schematic of the Squeeze-and-Excitation Network (SENet) is provided. It shows an input tensor X of dimensions C' × H' × W' being transformed through a function F_tr to output U of dimensions C × H × W. This is followed by a squeeze operation producing a 1×1×C vector, which is then processed by F_scale to generate scaling weights. These weights are applied to the original feature map to produce the final output X̄, demonstrating the channel-wise attention mechanism.\n\nThe figure also includes a legend at the bottom-left explaining the visual attributes: pink blocks represent Batch Normalization, light yellow blocks represent Activation Functions, and various shades of red/brown blocks represent convolutional layers with specified kernel sizes (3×3) and channel dimensions (e.g., 1-BC, BC-BC, C-1). The overall design emphasizes modularity, hierarchical processing, and the integration of attention mechanisms via SENets within each functional module.", "The figure illustrates the complete pipeline of a 3D scene reconstruction system, divided into two main stages: Tracking and Mapping, with an initial preprocessing step of Tri-view Matching. The global layout is structured from left to right and top to bottom, beginning with an input Image Sequence represented as a stack of frames along the time axis T, with spatial axes x and y indicated. This sequence feeds into the Tri-view Matching module, depicted below, where three consecutive frames (k-1, k, k+1) are shown with yellow lines connecting corresponding feature points across them, forming a triangular matching pattern. This module outputs robust correspondences used in subsequent steps.\n\nIn the Tracking stage, located at the top-right, the system estimates camera poses (T_k, T_{k-1}) for each frame using Hybrid Geometric Constraints. A 3D Pointmap is shown with red dots representing feature points, blue dots re-projection points, and red stars 3D points, connected via dashed lines indicating geometric relationships between frames. The tracking process involves a decision node labeled 'Keyframe?' which determines whether the current frame should be added to the map. If yes, it proceeds to the Mapping stage. The tracking loss function L_track is defined as a weighted sum of photometric loss (L_photo), 2D geometric loss (L_2D), and 3D geometric loss (L_3D), with explicit formulas provided: L_2D sums squared differences between projected and observed 2D points; L_3D computes the distance between transformed 3D points and their ground-truth positions; and L_track combines these with hyperparameters λ_p, λ_2D, λ_3D.\n\nThe Mapping stage, shown at the bottom-right, begins with the TUGI (Tri-view Uncertainty-guided Gaussian Initialization) module. This takes the tri-view matches and initializes 3D Gaussians, visualized as colored spheres with parameters (μ_xyz, σ²) indicating mean position and variance. These Gaussians are then rasterized into a 3D Gaussian Representation, shown as a dense, textured point cloud model of the scene. The photometric loss L_photo is computed by comparing the rendered image from this Gaussian model with the ground truth image, using a combination of L1 and SSIM metrics: L_photo = (1−γ)L1(I_t, Î_t) + γL_SSIM(I_t, Î_t), where γ is a weighting factor.\n\nVisual elements include rectangular boxes for modules (e.g., 'Image Sequence', 'Tri-view Matching'), dashed-line arrows for data flow, and a legend specifying point types (red circle: feature points, blue circle: re-projection points, red star: 3D points). The keyframe decision is marked with a diamond-shaped node. Equations are enclosed in rounded rectangles with light blue backgrounds. The overall structure emphasizes a real-time, incremental processing flow from raw images to a high-fidelity 3D representation through robust geometric constraints and uncertainty-aware initialization.", "The figure presents seven distinct architectural patterns for fusing multi-modal inputs using attention mechanisms, arranged in two rows. The top row contains diagrams (a) through (c), and the bottom row contains (d) through (g). Each diagram illustrates a different fusion strategy, with blue and orange rectangular blocks representing input feature sequences from two different modalities. Green rectangular blocks denote output representations, such as classification scores or generative outputs; a single green block indicates a scalar or simple output, while multiple green blocks suggest a sequence or multi-modal output. Dashed boxes represent modules with arbitrary internal architectures.\n\nIn diagram (a) 'Early Summation', three blue and three orange input blocks are summed element-wise via '+' operations, producing a single fused representation that is fed into an 'Attention-based Model' which outputs a single green block.\n\nDiagram (b) 'Early Concatenation' shows the same blue and orange input blocks being concatenated via a '||' operator into a single sequence, which is then processed by an 'Attention-based Model' to produce a single green output block.\n\nDiagram (c) 'Hierarchical' features two separate 'Attention Module' blocks, each processing one modality's input (blue or orange). Their outputs feed into a higher-level 'Model' (dashed box), which produces a single green output. This structure implies a hierarchical processing flow.\n\nDiagram (d) 'Single Cross-attention branch' introduces a cross-attention mechanism. The blue input provides keys (K_i) and values (V_i), while the orange input provides queries (Q_j). These are fed into a 'Cross-attention Module' that generates a single green output block.\n\nDiagram (e) 'Multi-cross attention' extends this by having two cross-attention modules. The first takes K_i, V_i from blue and Q_i from orange; the second takes K_j, V_j from orange and Q_j from blue. Both modules feed into a dashed box labeled 'Multiple output streams or other intermediate modules', indicating flexible downstream processing.\n\nDiagram (f) 'Single-stream to generative output' shows blue inputs going through an 'Attention-based Model' to produce a sequence of green blocks, suggesting a generative output like a text sequence.\n\nFinally, diagram (g) 'Modular multi-stream' shows two 'Attention Module' blocks processing blue and orange inputs respectively. Their outputs feed into 'Module A' (dashed), which in turn feeds into 'Module B' (dashed), producing a single green output. This represents a modular, multi-stream pipeline.\n\nAll connections are directed arrows indicating data flow. The figure uses consistent color coding: blue and orange for inputs, green for outputs, and black text for module labels. The layout is clean and modular, emphasizing the logical progression of data through each fusion type.", "The figure presents a comparative analysis between a baseline method and the proposed VCAR (Visual Comprehension Augmented Reasoning) framework for solving a multimodal question involving visual and textual data. The global layout is divided into two main horizontal sections: the top section illustrates the baseline approach, and the bottom section details the VCAR approach. Each section contains a left-side diagram of the model workflow and a right-side box displaying the generated rationale and description, with a dashed line separating the two methods.\n\nIn the baseline section, two robot-like icons represent models: one gray and one orange. Both receive 'Rationales' as input, indicated by red arrows from a yellow box labeled 'Rationales'. A gray arrow points from these models to a large beige box on the right containing the generated rationale. This rationale incorrectly states that grilled steak costs $10 and mushroom pizza costs $8, leading to a total of $18, marked with a red 'X' to indicate error. The multimodal question at the top asks: 'How much money does Damon need to buy a grilled steak and a mushroom pizza?' with a price list image showing pasta with white sauce ($15), mushroom pizza ($11), grilled steak ($13), and pasta with meat sauce ($12).\n\nIn the VCAR section, the same two robot icons appear, but now they receive different inputs. The gray robot receives 'Descriptions' from a blue box, while the orange robot receives both 'Descriptions' and 'Rationales' from stacked blue and yellow boxes. Blue arrows indicate the flow of descriptions, and a red arrow indicates the flow of rationales. Two gray arrows point from the robots to two boxes on the right: a light blue box labeled 'Description' and a beige box labeled 'Rationale'. The description accurately lists the food items and their correct prices: $15, $11, $13, and $12. The rationale correctly identifies the cost of grilled steak as $13 and mushroom pizza as $11, summing to $24, marked with a green checkmark to indicate correctness.\n\nThe figure visually emphasizes that the baseline method, which only uses rationales, fails due to incorrect visual interpretation, whereas VCAR, which incorporates visual description training, achieves accurate results. The caption explains that VCAR includes an additional visual comprehension task alongside mathematical reasoning, preventing errors from inaccurate visual understanding.", "The figure presents a conceptual comparison of four different point cloud completion learning paradigms, arranged in a 2x2 grid layout. The top row contrasts supervised and unpaired methods, while the bottom row compares weakly-supervised and the proposed self-supervised approach. Each panel contains a central deep neural network (DNN) block, depicted as a rounded rectangle with a light blue-to-lavender gradient fill and gray border, labeled 'DNN'. Above each DNN is the predicted output, denoted as \\(\\hat{y}^{(i)}\\), and below is the input, denoted as \\(x^{(i)}\\) or its variants. The panels are labeled (a) through (d) with corresponding descriptive subcaptions.\n\nIn panel (a) 'supervised', a single input \\(x^{(i)}\\) is fed into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow connects \\(\\hat{y}^{(i)}\\) to the ground truth \\(y^{(i)}\\), labeled 'matching loss', indicating supervision via direct comparison between prediction and true complete point cloud.\n\nPanel (b) 'unpaired' shows two inputs: \\(x^{(i)}\\) (partial) and \\(y^{(j)}\\) (complete, possibly from a different object), both feeding into the DNN. Two dashed orange curved arrows emerge: one from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\) labeled 'matching loss', enforcing shape consistency with the input, and another from \\(\\hat{y}^{(i)}\\) to \\(y^{(j)}\\) labeled 'adversarial loss', guiding the prediction to follow the distribution of complete shapes.\n\nPanel (c) 'weakly-supervised' features multiple inputs \\(x_1^{(i)}, x_2^{(i)}, ..., x_k^{(i)}\\) — different partial views of the same object — all processed by the DNN to produce multiple outputs \\(\\hat{y}_1^{(i)}, \\hat{y}_2^{(i)}, ..., \\hat{y}_k^{(i)}\\). A dashed orange curved arrow connects these outputs, labeled 'view-consistency loss', enforcing agreement among completions derived from different views of the same object.\n\nPanel (d) 'Ours' shows a single input \\(x^{(i)}\\) going into the DNN, producing \\(\\hat{y}^{(i)}\\). A dashed orange curved arrow loops back from \\(\\hat{y}^{(i)}\\) to \\(x^{(i)}\\), labeled 'self-supervised loss', indicating that the model is trained using a self-supervised signal derived from the prediction itself, without any external ground truth or additional views. This setup reflects the core contribution: learning from a single partial observation per object instance.\n\nAll connections are represented by solid gray arrows for data flow and dashed orange curved arrows for loss functions. The figure uses consistent visual elements across panels to highlight differences in training signals and data requirements."], 'resolution_list': [[576, 960], [576, 960], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [1008, 576], [768, 720], [768, 720]], 'max_sequence_length': 1024, 'dataloader_num_workers': 4, 'debug_mode': False, 'config_dir': 'configs/260122/flux2klein_fulltune_5000.py'}
NCCL version 2.21.5+cuda12.4
01/22/2026 03:24:09 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/22/2026 03:24:09 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/22/2026 03:24:09 - INFO - __main__ - [INFO] Using model type: Flux2Klein
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory - ============================================================
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory - 🏭 Model Factory Initialized
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Model Type: Flux2Klein
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Pretrained Path: black-forest-labs/FLUX.2-klein-base-9B
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Cache Dir: None
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    VAE Class: AutoencoderKLFlux2
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Transformer Class: Flux2Transformer2DModel
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Text Encoder Class: Qwen3ForCausalLM
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory -    Pipeline Class: Flux2KleinPipeline
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory - ============================================================
01/22/2026 03:24:09 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading tokenizer: Qwen2TokenizerFast
01/22/2026 03:24:09 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/22/2026 03:24:09 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

01/22/2026 03:24:10 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading text encoder: Qwen3ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 48.35it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 45.42it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 43.52it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 52.36it/s]
01/22/2026 03:24:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading VAE: AutoencoderKLFlux2
All model checkpoint weights were used when initializing AutoencoderKLFlux2.

All the weights of AutoencoderKLFlux2 were initialized from the model checkpoint at black-forest-labs/FLUX.2-klein-base-9B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKLFlux2 for predictions without further training.
01/22/2026 03:24:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading transformer: Flux2Transformer2DModel
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 16256.99it/s]
Instantiating Flux2Transformer2DModel model under default dtype torch.bfloat16.
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 7313.52it/s]
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 29433.71it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 14004.35it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 77.54it/s]
All model checkpoint weights were used when initializing Flux2Transformer2DModel.

All the weights of Flux2Transformer2DModel were initialized from the model checkpoint at black-forest-labs/FLUX.2-klein-base-9B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Flux2Transformer2DModel for predictions without further training.
01/22/2026 03:24:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Fine-tuning the full model ...
01/22/2026 03:24:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Enabling gradient checkpointing for transformer
01/22/2026 03:24:11 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading scheduler: FlowMatchEulerDiscreteScheduler
01/22/2026 03:24:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] Loading text encoding pipeline: Flux2KleinPipeline
{'is_distilled'} was not found in config. Values will be initialized to default values.
Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 4571.45it/s]
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:343: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:344: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
01/22/2026 03:24:12 - INFO - OpenSciDraw.utils.model_factory - [INFO] VAE scale factor: 16
01/22/2026 03:24:12 - INFO - __main__ - [INFO] DeepSpeed detected - keeping transformer in bf16 for ZeRO-3
01/22/2026 03:24:12 - INFO - __main__ - [INFO] Configuring model devices and offloading
01/22/2026 03:24:12 - INFO - __main__ - [INFO] Using parquet dataset - VAE and text encoder remain on CPU
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:343: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:344: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:343: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:344: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
01/22/2026 03:24:12 - INFO - __main__ - [INFO] DeepSpeed mode: transformer stays on CPU, ZeRO-3 will handle placement
01/22/2026 03:24:12 - INFO - __main__ - [INFO] Gradient checkpointing enabled
01/22/2026 03:24:12 - INFO - __main__ - [INFO] Number of trainable parameters: 9078.58M
01/22/2026 03:24:12 - INFO - __main__ - [INFO] Loading dataset
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:343: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  if hasattr(vae, attr_name):
/home/v-yuxluo/WORK_local/ArXivQwenImage/OpenSciDraw/utils/model_factory.py:344: FutureWarning: Accessing config attribute `block_out_channels` directly via 'AutoencoderKLFlux2' object attribute is deprecated. Please access 'block_out_channels' over 'AutoencoderKLFlux2's config object instead, e.g. 'unet.config.block_out_channels'.
  attr_value = getattr(vae, attr_name)
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...
🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
🔍 Building metadata from all parquet files in /home/v-yuxluo/data/ArXiV_parquet/flux_latents_test...
⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]⏳ Loading/parsing metadata (parquet: path only) from 84 parquet files...
Scanning Parquet Files:   0%|          | 0/84 [00:00<?, ?it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:02, 38.30it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:02, 33.48it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:02, 32.90it/s]Scanning Parquet Files:   6%|▌         | 5/84 [00:00<00:01, 44.75it/s]Scanning Parquet Files:  15%|█▌        | 13/84 [00:00<00:01, 49.59it/s]Scanning Parquet Files:  15%|█▌        | 13/84 [00:00<00:01, 48.82it/s]Scanning Parquet Files:  15%|█▌        | 13/84 [00:00<00:01, 45.60it/s]Scanning Parquet Files:  15%|█▌        | 13/84 [00:00<00:01, 49.47it/s]Scanning Parquet Files:  25%|██▌       | 21/84 [00:00<00:01, 55.17it/s]Scanning Parquet Files:  25%|██▌       | 21/84 [00:00<00:01, 54.64it/s]Scanning Parquet Files:  25%|██▌       | 21/84 [00:00<00:01, 51.73it/s]Scanning Parquet Files:  25%|██▌       | 21/84 [00:00<00:01, 57.25it/s]Scanning Parquet Files:  35%|███▍      | 29/84 [00:00<00:00, 58.23it/s]Scanning Parquet Files:  35%|███▍      | 29/84 [00:00<00:00, 56.04it/s]Scanning Parquet Files:  35%|███▍      | 29/84 [00:00<00:00, 56.97it/s]Scanning Parquet Files:  35%|███▍      | 29/84 [00:00<00:00, 60.47it/s]Scanning Parquet Files:  42%|████▏     | 35/84 [00:00<00:01, 48.62it/s]Scanning Parquet Files:  42%|████▏     | 35/84 [00:00<00:01, 48.22it/s]Scanning Parquet Files:  42%|████▏     | 35/84 [00:00<00:01, 47.72it/s]Scanning Parquet Files:  43%|████▎     | 36/84 [00:00<00:00, 53.25it/s]Scanning Parquet Files:  48%|████▊     | 40/84 [00:00<00:01, 42.32it/s]Scanning Parquet Files:  48%|████▊     | 40/84 [00:00<00:01, 41.56it/s]Scanning Parquet Files:  50%|█████     | 42/84 [00:00<00:01, 36.25it/s]Scanning Parquet Files:  49%|████▉     | 41/84 [00:01<00:01, 32.93it/s]Scanning Parquet Files:  56%|█████▌    | 47/84 [00:01<00:01, 35.48it/s]Scanning Parquet Files:  54%|█████▎    | 45/84 [00:01<00:01, 29.76it/s]Scanning Parquet Files:  54%|█████▎    | 45/84 [00:01<00:01, 28.93it/s]Scanning Parquet Files:  54%|█████▎    | 45/84 [00:01<00:01, 30.51it/s]Scanning Parquet Files:  58%|█████▊    | 49/84 [00:01<00:01, 31.25it/s]Scanning Parquet Files:  63%|██████▎   | 53/84 [00:01<00:00, 36.44it/s]Scanning Parquet Files:  63%|██████▎   | 53/84 [00:01<00:00, 34.53it/s]Scanning Parquet Files:  63%|██████▎   | 53/84 [00:01<00:00, 35.91it/s]Scanning Parquet Files:  68%|██████▊   | 57/84 [00:01<00:00, 37.59it/s]Scanning Parquet Files:  73%|███████▎  | 61/84 [00:01<00:00, 40.74it/s]Scanning Parquet Files:  73%|███████▎  | 61/84 [00:01<00:00, 38.87it/s]Scanning Parquet Files:  73%|███████▎  | 61/84 [00:01<00:00, 39.44it/s]Scanning Parquet Files:  74%|███████▍  | 62/84 [00:01<00:00, 40.02it/s]Scanning Parquet Files:  82%|████████▏ | 69/84 [00:01<00:00, 43.89it/s]Scanning Parquet Files:  82%|████████▏ | 69/84 [00:01<00:00, 41.84it/s]Scanning Parquet Files:  82%|████████▏ | 69/84 [00:01<00:00, 42.45it/s]Scanning Parquet Files:  82%|████████▏ | 69/84 [00:01<00:00, 42.18it/s]Scanning Parquet Files:  92%|█████████▏| 77/84 [00:01<00:00, 47.06it/s]Scanning Parquet Files:  92%|█████████▏| 77/84 [00:01<00:00, 43.86it/s]Scanning Parquet Files:  92%|█████████▏| 77/84 [00:01<00:00, 44.97it/s]Scanning Parquet Files:  92%|█████████▏| 77/84 [00:01<00:00, 45.38it/s]Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 46.80it/s]
✅ Loaded 190237 samples.
Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 43.84it/s]
Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 43.69it/s]
Scanning Parquet Files: 100%|██████████| 84/84 [00:01<00:00, 43.71it/s]
✅ Loaded 190237 samples.
✅ Loaded 190237 samples.
✅ Loaded 190237 samples.
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
01/22/2026 03:24:14 - INFO - __main__ - [INFO] Set DeepSpeed train_micro_batch_size_per_gpu to 1
Filtered dataset: 190132 samples remaining.
Filtered dataset: 190132 samples remaining.
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.
  warnings.warn(
Before initializing optimizer states
MA 25.37 GB         Max_MA 29.59 GB         CA 29.6 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 44.67 GB, percent = 5.2%
After initializing optimizer states
MA 25.37 GB         Max_MA 33.82 GB         CA 38.06 GB         Max_CA 38 GB 
CPU Virtual Memory:  used = 44.96 GB, percent = 5.2%
After initializing ZeRO optimizer
MA 25.37 GB         Max_MA 25.37 GB         CA 38.06 GB         Max_CA 38 GB 
CPU Virtual Memory:  used = 45.59 GB, percent = 5.3%
01/22/2026 03:24:38 - INFO - __main__ - ***** Running training *****
01/22/2026 03:24:38 - INFO - __main__ -   Num examples = 190132
01/22/2026 03:24:38 - INFO - __main__ -   Num Epochs = 2
01/22/2026 03:24:38 - INFO - __main__ -   Instantaneous batch size per device = 1
01/22/2026 03:24:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
01/22/2026 03:24:38 - INFO - __main__ -   Gradient Accumulation steps = 4
01/22/2026 03:24:38 - INFO - __main__ -   Total optimization steps = 5000
Steps:   0%|          | 0/5000 [00:00<?, ?it/s]01/22/2026 03:24:38 - INFO - __main__ - [INFO] Using training iteration function: Flux2Klein_fulltune_train_iteration
01/22/2026 03:24:39 - INFO - __main__ - [INFO] Using validation function: Flux2Klein_fulltune_validation_func_parquet
01/22/2026 03:24:39 - INFO - __main__ - [INFO] Validation every 500 steps
wandb: Loaded settings from
wandb:   /home/v-yuxluo/.config/wandb/settings
wandb: [wandb.login()] Loaded credentials for https://microsoft-research.wandb.io from /home/v-yuxluo/.netrc.
wandb: Currently logged in as: v-yuxluo to https://microsoft-research.wandb.io. Use `wandb login --relogin` to force relogin
wandb: setting up run k5e53u6r
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/v-yuxluo/WORK_local/ArXivQwenImage/wandb/run-20260122_032439-k5e53u6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flux2klein_9b_fulltune_5000steps
wandb: ⭐️ View project at https://microsoft-research.wandb.io/v-yuxluo/Flux2Klein-FullTune
wandb: 🚀 View run at https://microsoft-research.wandb.io/v-yuxluo/Flux2Klein-FullTune/runs/k5e53u6r
01/22/2026 03:24:40 - INFO - __main__ - 
======================================================================
01/22/2026 03:24:40 - INFO - __main__ - Starting Training Loop
01/22/2026 03:24:40 - INFO - __main__ - ======================================================================

[Step 0] Training Debug Info:
  Loss: 0.599027
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0016, std: 0.8984
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0015, std: 1.3438
  Model pred mean: 0.0049, std: 1.2891
  Sigmas: [0.6171875]... (timesteps: [618.0])

[Step 0] Training Debug Info:
  Loss: 1.166942
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0295, std: 0.9609
  Noise mean: -0.0000, std: 1.0000
  Target mean: -0.0295, std: 1.3828
  Model pred mean: -0.0036, std: 0.8789
  Sigmas: [0.041015625]... (timesteps: [41.0])

[Step 0] Training Debug Info:
  Loss: 0.641546
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0056, std: 0.9102
  Noise mean: 0.0040, std: 0.9961
  Target mean: -0.0015, std: 1.3516
  Model pred mean: 0.0015, std: 1.2812
  Sigmas: [0.609375]... (timesteps: [610.0])

[Step 0] Training Debug Info:
  Loss: 1.469110
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0054, std: 0.8672
  Noise mean: -0.0032, std: 1.0000
  Target mean: 0.0021, std: 1.3203
  Model pred mean: 0.0066, std: 1.3906
  Sigmas: [0.400390625]... (timesteps: [400.0])
Steps:   0%|          | 1/5000 [00:16<22:16:14, 16.04s/it]Steps:   0%|          | 1/5000 [00:16<22:16:14, 16.04s/it, loss=1.4691, lr=2.00e-08]01/22/2026 03:24:54 - INFO - __main__ - 
🔍 Running validation at step 1...
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 1 (parquet mode)...
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 1...
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Moving VAE and text_encoder to GPU...
01/22/2026 03:24:57 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 1/12: The figure illustrates a process hooking mechanism using the LD_PRELOAD environm...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.20it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.70it/s][A
 11%|█         | 3/28 [00:01<00:15,  1.59it/s][A
 14%|█▍        | 4/28 [00:02<00:15,  1.54it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.51it/s][A
 21%|██▏       | 6/28 [00:03<00:14,  1.49it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.48it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.48it/s][A
 32%|███▏      | 9/28 [00:05<00:12,  1.47it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.47it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.47it/s][A
 43%|████▎     | 12/28 [00:07<00:10,  1.46it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.46it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.46it/s][A
 54%|█████▎    | 15/28 [00:10<00:08,  1.46it/s][A
 57%|█████▋    | 16/28 [00:10<00:08,  1.46it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.46it/s][A
 64%|██████▍   | 18/28 [00:12<00:06,  1.46it/s][A
 68%|██████▊   | 19/28 [00:12<00:06,  1.46it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.46it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.46it/s][A
 79%|███████▊  | 22/28 [00:14<00:04,  1.46it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.46it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.46it/s][A
 89%|████████▉ | 25/28 [00:16<00:02,  1.46it/s][A
 93%|█████████▎| 26/28 [00:17<00:01,  1.45it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.45it/s][A
100%|██████████| 28/28 [00:18<00:00,  1.46it/s][A100%|██████████| 28/28 [00:18<00:00,  1.48it/s]
01/22/2026 03:25:17 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt00_0_The_figure_illustrates_a_process_hooking_mechanism.png
01/22/2026 03:25:17 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 2/12: The figure presents an overview of four distinct end-to-end Task-Oriented Dialog...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.20it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.68it/s][A
 11%|█         | 3/28 [00:01<00:15,  1.57it/s][A
 14%|█▍        | 4/28 [00:02<00:15,  1.52it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.50it/s][A
 21%|██▏       | 6/28 [00:03<00:14,  1.48it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.47it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.46it/s][A
 32%|███▏      | 9/28 [00:05<00:13,  1.46it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.45it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.45it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.45it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.45it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.45it/s][A
 54%|█████▎    | 15/28 [00:10<00:08,  1.45it/s][A
 57%|█████▋    | 16/28 [00:10<00:08,  1.44it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.44it/s][A
 64%|██████▍   | 18/28 [00:12<00:06,  1.44it/s][A
 68%|██████▊   | 19/28 [00:12<00:06,  1.44it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.44it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.44it/s][A
 79%|███████▊  | 22/28 [00:14<00:04,  1.44it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.44it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.44it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.44it/s][A
 93%|█████████▎| 26/28 [00:17<00:01,  1.44it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.44it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.44it/s][A100%|██████████| 28/28 [00:19<00:00,  1.46it/s]
01/22/2026 03:25:37 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt01_0_The_figure_presents_an_overview_of_four_distinct_e.png
01/22/2026 03:25:37 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 3/12: The figure illustrates a network architecture for a single-step diffusion model ...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.12it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.63it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.51it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.47it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.44it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.43it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.42it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.42it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.41it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.41it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.41it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.41it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.41it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.40it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.40it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.40it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.40it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.40it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.40it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.40it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.40it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.40it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.40it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.40it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.40it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.40it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.40it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.40it/s][A100%|██████████| 28/28 [00:19<00:00,  1.42it/s]
01/22/2026 03:25:58 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt02_0_The_figure_illustrates_a_network_architecture_for.png
01/22/2026 03:25:58 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 4/12: The figure presents a comparative diagram of four different defect detection tas...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.11it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.62it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.51it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.46it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.44it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.42it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.41it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.41it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.40it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.40it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.40it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.40it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.39it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.39it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.39it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.39it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.39it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.39it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.39it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.39it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.39it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.39it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.39it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.39it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.39it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.39it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.39it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.39it/s][A100%|██████████| 28/28 [00:19<00:00,  1.41it/s]
01/22/2026 03:26:18 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt03_0_The_figure_presents_a_comparative_diagram_of_four.png
01/22/2026 03:26:18 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 5/12: The figure illustrates a model evaluation framework for a diffusion-based predic...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.50it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.46it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.43it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.42it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.41it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.40it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.40it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.39it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.39it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.39it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.39it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.39it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.39it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.39it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.39it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.39it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.39it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.39it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.38it/s][A100%|██████████| 28/28 [00:19<00:00,  1.40it/s]
01/22/2026 03:26:39 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt04_0_The_figure_illustrates_a_model_evaluation_framewor.png
01/22/2026 03:26:39 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 6/12: The figure illustrates a linear probing framework applied to a frozen multimodal...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.50it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.43it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.40it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.39it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 03:27:00 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt05_0_The_figure_illustrates_a_linear_probing_framework.png
01/22/2026 03:27:00 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 7/12: The figure presents a comparative architectural diagram illustrating two differe...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.08it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 03:27:20 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt06_0_The_figure_presents_a_comparative_architectural_di.png
01/22/2026 03:27:20 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 8/12: The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.60it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 03:27:41 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt07_0_The_figure_illustrates_the_overall_architecture_of.png
01/22/2026 03:27:41 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 9/12: The figure illustrates the complete pipeline of a 3D scene reconstruction system...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.60it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:07<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 03:28:02 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt08_0_The_figure_illustrates_the_complete_pipeline_of_a.png
01/22/2026 03:28:02 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 10/12: The figure presents seven distinct architectural patterns for fusing multi-modal...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.09it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.50it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.43it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.40it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.39it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.39it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.38it/s][A100%|██████████| 28/28 [00:19<00:00,  1.40it/s]
01/22/2026 03:28:23 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt09_0_The_figure_presents_seven_distinct_architectural_p.png
01/22/2026 03:28:23 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 11/12: The figure presents a comparative analysis between a baseline method and the pro...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.15it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.65it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.54it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.49it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.46it/s][A
 21%|██▏       | 6/28 [00:03<00:15,  1.45it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.44it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.43it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.43it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.43it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.43it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.42it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.42it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.42it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.42it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.42it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.42it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.42it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.42it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.42it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.42it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.42it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.42it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.42it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.42it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.42it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.42it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.42it/s][A100%|██████████| 28/28 [00:19<00:00,  1.44it/s]
01/22/2026 03:28:43 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt10_0_The_figure_presents_a_comparative_analysis_between.png
01/22/2026 03:28:43 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 12/12: The figure presents a conceptual comparison of four different point cloud comple...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.15it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.65it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.54it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.49it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.47it/s][A
 21%|██▏       | 6/28 [00:03<00:15,  1.45it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.44it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.44it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.43it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.43it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.43it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.43it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.43it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.43it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.43it/s][A
 57%|█████▋    | 16/28 [00:10<00:08,  1.42it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.43it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.43it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.42it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.43it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.42it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.43it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.43it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.42it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.43it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.43it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.42it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.43it/s][A100%|██████████| 28/28 [00:19<00:00,  1.44it/s]
01/22/2026 03:29:03 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001/step000001_prompt11_0_The_figure_presents_a_conceptual_comparison_of_fou.png
01/22/2026 03:29:03 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Moving VAE and text_encoder back to CPU...
01/22/2026 03:29:05 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
  ✅ Validation complete! Saved 12 images to:
01/22/2026 03:29:05 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -      /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000001
01/22/2026 03:29:05 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================

Steps:   0%|          | 2/5000 [04:42<226:28:47, 163.13s/it, loss=1.4691, lr=2.00e-08]Steps:   0%|          | 2/5000 [04:42<226:28:47, 163.13s/it, loss=0.3842, lr=4.00e-08]Steps:   0%|          | 3/5000 [04:54<130:53:18, 94.30s/it, loss=0.3842, lr=4.00e-08] Steps:   0%|          | 3/5000 [04:54<130:53:18, 94.30s/it, loss=1.0194, lr=6.00e-08]Steps:   0%|          | 4/5000 [05:06<85:43:21, 61.77s/it, loss=1.0194, lr=6.00e-08] Steps:   0%|          | 4/5000 [05:06<85:43:21, 61.77s/it, loss=0.9794, lr=8.00e-08]Steps:   0%|          | 5/5000 [05:18<60:47:29, 43.81s/it, loss=0.9794, lr=8.00e-08]Steps:   0%|          | 5/5000 [05:18<60:47:29, 43.81s/it, loss=0.6914, lr=1.00e-07]Steps:   0%|          | 6/5000 [05:30<45:43:31, 32.96s/it, loss=0.6914, lr=1.00e-07]Steps:   0%|          | 6/5000 [05:30<45:43:31, 32.96s/it, loss=0.4617, lr=1.20e-07]Steps:   0%|          | 7/5000 [05:42<36:09:31, 26.07s/it, loss=0.4617, lr=1.20e-07]Steps:   0%|          | 7/5000 [05:42<36:09:31, 26.07s/it, loss=0.7304, lr=1.40e-07]Steps:   0%|          | 8/5000 [05:54<29:52:56, 21.55s/it, loss=0.7304, lr=1.40e-07]Steps:   0%|          | 8/5000 [05:54<29:52:56, 21.55s/it, loss=1.0902, lr=1.60e-07]Steps:   0%|          | 9/5000 [06:05<25:42:24, 18.54s/it, loss=1.0902, lr=1.60e-07]Steps:   0%|          | 9/5000 [06:05<25:42:24, 18.54s/it, loss=1.5404, lr=1.80e-07]Steps:   0%|          | 10/5000 [06:18<23:01:19, 16.61s/it, loss=1.5404, lr=1.80e-07]Steps:   0%|          | 10/5000 [06:18<23:01:19, 16.61s/it, loss=0.6125, lr=2.00e-07]
[Step 10] Training Debug Info:
  Loss: 1.936277
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0161, std: 0.9336
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0157, std: 1.3672
  Model pred mean: -0.0078, std: 1.4766
  Sigmas: [0.2890625]... (timesteps: [290.0])

[Step 10] Training Debug Info:
  Loss: 1.297343
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0061, std: 0.9297
  Noise mean: -0.0008, std: 1.0000
  Target mean: -0.0068, std: 1.3672
  Model pred mean: 0.0205, std: 0.8516
  Sigmas: [0.053955078125]... (timesteps: [54.0])

[Step 10] Training Debug Info:
  Loss: 2.330617
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0109, std: 0.8828
  Noise mean: 0.0035, std: 1.0000
  Target mean: 0.0145, std: 1.3359
  Model pred mean: 0.0410, std: 1.3125
  Sigmas: [0.2109375]... (timesteps: [211.0])

[Step 10] Training Debug Info:
  Loss: 0.386492
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0141, std: 0.9297
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0140, std: 1.3672
  Model pred mean: -0.0106, std: 1.2891
  Sigmas: [0.71875]... (timesteps: [719.0])
Steps:   0%|          | 11/5000 [06:30<21:00:26, 15.16s/it, loss=0.6125, lr=2.00e-07]Steps:   0%|          | 11/5000 [06:30<21:00:26, 15.16s/it, loss=0.3865, lr=2.20e-07]Steps:   0%|          | 12/5000 [06:42<19:38:59, 14.18s/it, loss=0.3865, lr=2.20e-07]Steps:   0%|          | 12/5000 [06:42<19:38:59, 14.18s/it, loss=0.3583, lr=2.40e-07]Steps:   0%|          | 13/5000 [06:54<18:42:09, 13.50s/it, loss=0.3583, lr=2.40e-07]Steps:   0%|          | 13/5000 [06:54<18:42:09, 13.50s/it, loss=1.3414, lr=2.60e-07]Steps:   0%|          | 14/5000 [07:05<18:02:49, 13.03s/it, loss=1.3414, lr=2.60e-07]Steps:   0%|          | 14/5000 [07:05<18:02:49, 13.03s/it, loss=0.8281, lr=2.80e-07]Steps:   0%|          | 15/5000 [07:17<17:36:46, 12.72s/it, loss=0.8281, lr=2.80e-07]Steps:   0%|          | 15/5000 [07:17<17:36:46, 12.72s/it, loss=0.4409, lr=3.00e-07]Steps:   0%|          | 16/5000 [07:29<17:17:51, 12.49s/it, loss=0.4409, lr=3.00e-07]Steps:   0%|          | 16/5000 [07:29<17:17:51, 12.49s/it, loss=0.6131, lr=3.20e-07]Steps:   0%|          | 17/5000 [07:42<17:10:08, 12.40s/it, loss=0.6131, lr=3.20e-07]Steps:   0%|          | 17/5000 [07:42<17:10:08, 12.40s/it, loss=0.3990, lr=3.40e-07]Steps:   0%|          | 18/5000 [07:54<16:57:13, 12.25s/it, loss=0.3990, lr=3.40e-07]Steps:   0%|          | 18/5000 [07:54<16:57:13, 12.25s/it, loss=0.5144, lr=3.60e-07]Steps:   0%|          | 19/5000 [08:05<16:48:02, 12.14s/it, loss=0.5144, lr=3.60e-07]Steps:   0%|          | 19/5000 [08:05<16:48:02, 12.14s/it, loss=1.7654, lr=3.80e-07]Steps:   0%|          | 20/5000 [08:17<16:41:41, 12.07s/it, loss=1.7654, lr=3.80e-07]Steps:   0%|          | 20/5000 [08:17<16:41:41, 12.07s/it, loss=0.3744, lr=4.00e-07]
[Step 20] Training Debug Info:
  Loss: 2.881507
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: -0.0125, std: 0.8477
  Noise mean: 0.0039, std: 1.0000
  Target mean: 0.0164, std: 1.3125
  Model pred mean: 0.0204, std: 1.5391
  Sigmas: [0.2353515625]... (timesteps: [235.0])

[Step 20] Training Debug Info:
  Loss: 0.467385
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0133, std: 0.9062
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0123, std: 1.3516
  Model pred mean: -0.0162, std: 1.2656
  Sigmas: [0.703125]... (timesteps: [703.0])

[Step 20] Training Debug Info:
  Loss: 2.029189
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0032, std: 0.8984
  Noise mean: 0.0021, std: 1.0000
  Target mean: 0.0053, std: 1.3438
  Model pred mean: 0.0349, std: 1.1797
  Sigmas: [0.197265625]... (timesteps: [197.0])

[Step 20] Training Debug Info:
  Loss: 0.654174
  Latent shape: torch.Size([1, 32, 132, 66]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0347, std: 0.9102
  Noise mean: -0.0033, std: 1.0000
  Target mean: -0.0378, std: 1.3594
  Model pred mean: -0.0305, std: 1.2656
  Sigmas: [0.6171875]... (timesteps: [619.0])
Steps:   0%|          | 21/5000 [08:29<16:34:54, 11.99s/it, loss=0.3744, lr=4.00e-07]Steps:   0%|          | 21/5000 [08:29<16:34:54, 11.99s/it, loss=0.6542, lr=4.20e-07]Steps:   0%|          | 22/5000 [08:41<16:32:16, 11.96s/it, loss=0.6542, lr=4.20e-07]Steps:   0%|          | 22/5000 [08:41<16:32:16, 11.96s/it, loss=1.7384, lr=4.40e-07]Steps:   0%|          | 23/5000 [08:53<16:41:07, 12.07s/it, loss=1.7384, lr=4.40e-07]Steps:   0%|          | 23/5000 [08:53<16:41:07, 12.07s/it, loss=1.8890, lr=4.60e-07]Steps:   0%|          | 24/5000 [09:05<16:36:55, 12.02s/it, loss=1.8890, lr=4.60e-07]Steps:   0%|          | 24/5000 [09:05<16:36:55, 12.02s/it, loss=0.5236, lr=4.80e-07]Steps:   0%|          | 25/5000 [09:17<16:34:24, 11.99s/it, loss=0.5236, lr=4.80e-07]Steps:   0%|          | 25/5000 [09:17<16:34:24, 11.99s/it, loss=1.2238, lr=5.00e-07]Steps:   1%|          | 26/5000 [09:29<16:31:24, 11.96s/it, loss=1.2238, lr=5.00e-07]Steps:   1%|          | 26/5000 [09:29<16:31:24, 11.96s/it, loss=0.3737, lr=5.20e-07]Steps:   1%|          | 27/5000 [09:41<16:29:45, 11.94s/it, loss=0.3737, lr=5.20e-07]Steps:   1%|          | 27/5000 [09:41<16:29:45, 11.94s/it, loss=0.3795, lr=5.40e-07]Steps:   1%|          | 28/5000 [09:53<16:29:11, 11.94s/it, loss=0.3795, lr=5.40e-07]Steps:   1%|          | 28/5000 [09:53<16:29:11, 11.94s/it, loss=0.5863, lr=5.60e-07]Steps:   1%|          | 29/5000 [10:05<16:24:29, 11.88s/it, loss=0.5863, lr=5.60e-07]Steps:   1%|          | 29/5000 [10:05<16:24:29, 11.88s/it, loss=0.6199, lr=5.80e-07]Steps:   1%|          | 30/5000 [10:17<16:31:42, 11.97s/it, loss=0.6199, lr=5.80e-07]Steps:   1%|          | 30/5000 [10:17<16:31:42, 11.97s/it, loss=0.5186, lr=6.00e-07]
[Step 30] Training Debug Info:
  Loss: 0.759749
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0045, std: 0.9023
  Noise mean: 0.0014, std: 1.0000
  Target mean: -0.0031, std: 1.3438
  Model pred mean: -0.0007, std: 1.2969
  Sigmas: [0.53515625]... (timesteps: [535.0])

[Step 30] Training Debug Info:
  Loss: 0.383190
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0266, std: 0.9023
  Noise mean: 0.0008, std: 0.9961
  Target mean: 0.0275, std: 1.3438
  Model pred mean: 0.0157, std: 1.2266
  Sigmas: [0.83984375]... (timesteps: [839.0])

[Step 30] Training Debug Info:
  Loss: 0.382366
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0231, std: 0.9297
  Noise mean: -0.0019, std: 1.0000
  Target mean: -0.0249, std: 1.3672
  Model pred mean: -0.0242, std: 1.2500
  Sigmas: [0.84375]... (timesteps: [844.0])

[Step 30] Training Debug Info:
  Loss: 0.408918
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0043, std: 0.8945
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0040, std: 1.3438
  Model pred mean: -0.0060, std: 1.2422
  Sigmas: [0.79296875]... (timesteps: [792.0])
Steps:   1%|          | 31/5000 [10:29<16:30:41, 11.96s/it, loss=0.5186, lr=6.00e-07]Steps:   1%|          | 31/5000 [10:29<16:30:41, 11.96s/it, loss=0.4089, lr=6.20e-07]Steps:   1%|          | 32/5000 [10:41<16:28:22, 11.94s/it, loss=0.4089, lr=6.20e-07]Steps:   1%|          | 32/5000 [10:41<16:28:22, 11.94s/it, loss=0.6223, lr=6.40e-07]Steps:   1%|          | 33/5000 [10:52<16:25:21, 11.90s/it, loss=0.6223, lr=6.40e-07]Steps:   1%|          | 33/5000 [10:52<16:25:21, 11.90s/it, loss=1.6142, lr=6.60e-07]Steps:   1%|          | 34/5000 [11:04<16:27:21, 11.93s/it, loss=1.6142, lr=6.60e-07]Steps:   1%|          | 34/5000 [11:04<16:27:21, 11.93s/it, loss=0.5774, lr=6.80e-07]Steps:   1%|          | 35/5000 [11:16<16:26:32, 11.92s/it, loss=0.5774, lr=6.80e-07]Steps:   1%|          | 35/5000 [11:16<16:26:32, 11.92s/it, loss=0.4670, lr=7.00e-07]Steps:   1%|          | 36/5000 [11:28<16:27:19, 11.93s/it, loss=0.4670, lr=7.00e-07]Steps:   1%|          | 36/5000 [11:28<16:27:19, 11.93s/it, loss=1.0457, lr=7.20e-07]Steps:   1%|          | 37/5000 [11:40<16:32:45, 12.00s/it, loss=1.0457, lr=7.20e-07]Steps:   1%|          | 37/5000 [11:40<16:32:45, 12.00s/it, loss=0.4700, lr=7.40e-07]Steps:   1%|          | 38/5000 [11:52<16:30:55, 11.98s/it, loss=0.4700, lr=7.40e-07]Steps:   1%|          | 38/5000 [11:52<16:30:55, 11.98s/it, loss=0.3642, lr=7.60e-07]Steps:   1%|          | 39/5000 [12:04<16:28:55, 11.96s/it, loss=0.3642, lr=7.60e-07]Steps:   1%|          | 39/5000 [12:04<16:28:55, 11.96s/it, loss=1.7193, lr=7.80e-07]Steps:   1%|          | 40/5000 [12:16<16:28:16, 11.95s/it, loss=1.7193, lr=7.80e-07]Steps:   1%|          | 40/5000 [12:16<16:28:16, 11.95s/it, loss=1.2983, lr=8.00e-07]
[Step 40] Training Debug Info:
  Loss: 1.073382
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0229, std: 0.9570
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0214, std: 1.3828
  Model pred mean: -0.0186, std: 1.3281
  Sigmas: [0.435546875]... (timesteps: [436.0])

[Step 40] Training Debug Info:
  Loss: 0.612083
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0142, std: 0.9531
  Noise mean: 0.0006, std: 1.0000
  Target mean: 0.0148, std: 1.3828
  Model pred mean: 0.0085, std: 1.1406
  Sigmas: [0.94140625]... (timesteps: [941.0])

[Step 40] Training Debug Info:
  Loss: 1.939589
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0227, std: 0.8594
  Noise mean: -0.0005, std: 1.0000
  Target mean: 0.0222, std: 1.3203
  Model pred mean: 0.0417, std: 1.2891
  Sigmas: [0.291015625]... (timesteps: [291.0])

[Step 40] Training Debug Info:
  Loss: 0.625237
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0354, std: 0.9297
  Noise mean: 0.0038, std: 1.0000
  Target mean: -0.0317, std: 1.3594
  Model pred mean: -0.0337, std: 1.2656
  Sigmas: [0.57421875]... (timesteps: [576.0])
Steps:   1%|          | 41/5000 [12:28<16:24:44, 11.91s/it, loss=1.2983, lr=8.00e-07]Steps:   1%|          | 41/5000 [12:28<16:24:44, 11.91s/it, loss=0.6252, lr=8.20e-07]Steps:   1%|          | 42/5000 [12:40<16:23:42, 11.90s/it, loss=0.6252, lr=8.20e-07]Steps:   1%|          | 42/5000 [12:40<16:23:42, 11.90s/it, loss=0.4022, lr=8.40e-07]Steps:   1%|          | 43/5000 [12:52<16:26:26, 11.94s/it, loss=0.4022, lr=8.40e-07]Steps:   1%|          | 43/5000 [12:52<16:26:26, 11.94s/it, loss=0.5520, lr=8.60e-07]Steps:   1%|          | 44/5000 [13:04<16:30:42, 11.99s/it, loss=0.5520, lr=8.60e-07]Steps:   1%|          | 44/5000 [13:04<16:30:42, 11.99s/it, loss=1.4373, lr=8.80e-07]Steps:   1%|          | 45/5000 [13:16<16:28:45, 11.97s/it, loss=1.4373, lr=8.80e-07]Steps:   1%|          | 45/5000 [13:16<16:28:45, 11.97s/it, loss=1.4059, lr=9.00e-07]Steps:   1%|          | 46/5000 [13:28<16:25:18, 11.93s/it, loss=1.4059, lr=9.00e-07]Steps:   1%|          | 46/5000 [13:28<16:25:18, 11.93s/it, loss=1.3273, lr=9.20e-07]Steps:   1%|          | 47/5000 [13:40<16:24:23, 11.92s/it, loss=1.3273, lr=9.20e-07]Steps:   1%|          | 47/5000 [13:40<16:24:23, 11.92s/it, loss=0.9287, lr=9.40e-07]Steps:   1%|          | 48/5000 [13:52<16:23:20, 11.91s/it, loss=0.9287, lr=9.40e-07]Steps:   1%|          | 48/5000 [13:52<16:23:20, 11.91s/it, loss=1.1581, lr=9.60e-07]Steps:   1%|          | 49/5000 [14:04<16:21:36, 11.90s/it, loss=1.1581, lr=9.60e-07]Steps:   1%|          | 49/5000 [14:04<16:21:36, 11.90s/it, loss=0.6491, lr=9.80e-07]Steps:   1%|          | 50/5000 [14:16<16:27:36, 11.97s/it, loss=0.6491, lr=9.80e-07]Steps:   1%|          | 50/5000 [14:16<16:27:36, 11.97s/it, loss=0.5051, lr=1.00e-06]
[Step 50] Training Debug Info:
  Loss: 1.159831
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0046, std: 0.9180
  Noise mean: -0.0031, std: 1.0000
  Target mean: 0.0016, std: 1.3594
  Model pred mean: 0.0125, std: 0.8477
  Sigmas: [0.0791015625]... (timesteps: [79.0])

[Step 50] Training Debug Info:
  Loss: 1.159369
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0145, std: 0.8711
  Noise mean: 0.0020, std: 1.0000
  Target mean: -0.0125, std: 1.3281
  Model pred mean: 0.0069, std: 0.8828
  Sigmas: [0.306640625]... (timesteps: [306.0])

[Step 50] Training Debug Info:
  Loss: 0.656016
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0361, std: 0.9336
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0361, std: 1.3672
  Model pred mean: -0.0238, std: 1.1719
  Sigmas: [0.5234375]... (timesteps: [525.0])

[Step 50] Training Debug Info:
  Loss: 0.407384
  Latent shape: torch.Size([1, 32, 132, 66]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0152, std: 0.9102
  Noise mean: 0.0021, std: 1.0000
  Target mean: 0.0173, std: 1.3516
  Model pred mean: 0.0108, std: 1.2109
  Sigmas: [0.80078125]... (timesteps: [802.0])
Steps:   1%|          | 51/5000 [14:27<16:24:16, 11.93s/it, loss=0.5051, lr=1.00e-06]Steps:   1%|          | 51/5000 [14:27<16:24:16, 11.93s/it, loss=0.4074, lr=1.02e-06]Steps:   1%|          | 52/5000 [14:39<16:25:08, 11.95s/it, loss=0.4074, lr=1.02e-06]Steps:   1%|          | 52/5000 [14:39<16:25:08, 11.95s/it, loss=1.0813, lr=1.04e-06]Steps:   1%|          | 53/5000 [14:51<16:22:49, 11.92s/it, loss=1.0813, lr=1.04e-06]Steps:   1%|          | 53/5000 [14:51<16:22:49, 11.92s/it, loss=0.4011, lr=1.06e-06]Steps:   1%|          | 54/5000 [15:03<16:20:35, 11.90s/it, loss=0.4011, lr=1.06e-06]Steps:   1%|          | 54/5000 [15:03<16:20:35, 11.90s/it, loss=0.5807, lr=1.08e-06]Steps:   1%|          | 55/5000 [15:15<16:19:16, 11.88s/it, loss=0.5807, lr=1.08e-06]Steps:   1%|          | 55/5000 [15:15<16:19:16, 11.88s/it, loss=1.2212, lr=1.10e-06]Steps:   1%|          | 56/5000 [15:27<16:20:01, 11.89s/it, loss=1.2212, lr=1.10e-06]Steps:   1%|          | 56/5000 [15:27<16:20:01, 11.89s/it, loss=1.2777, lr=1.12e-06]Steps:   1%|          | 57/5000 [15:39<16:23:42, 11.94s/it, loss=1.2777, lr=1.12e-06]Steps:   1%|          | 57/5000 [15:39<16:23:42, 11.94s/it, loss=0.6022, lr=1.14e-06]Steps:   1%|          | 58/5000 [15:51<16:21:18, 11.91s/it, loss=0.6022, lr=1.14e-06]Steps:   1%|          | 58/5000 [15:51<16:21:18, 11.91s/it, loss=1.1287, lr=1.16e-06]Steps:   1%|          | 59/5000 [16:03<16:20:30, 11.91s/it, loss=1.1287, lr=1.16e-06]Steps:   1%|          | 59/5000 [16:03<16:20:30, 11.91s/it, loss=0.3854, lr=1.18e-06]Steps:   1%|          | 60/5000 [16:15<16:21:57, 11.93s/it, loss=0.3854, lr=1.18e-06]Steps:   1%|          | 60/5000 [16:15<16:21:57, 11.93s/it, loss=0.4405, lr=1.20e-06]
[Step 60] Training Debug Info:
  Loss: 0.628610
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0134, std: 0.9180
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0132, std: 1.3594
  Model pred mean: -0.0118, std: 1.0703
  Sigmas: [0.6015625]... (timesteps: [601.0])

[Step 60] Training Debug Info:
  Loss: 0.380292
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0070, std: 0.9336
  Noise mean: -0.0040, std: 1.0000
  Target mean: -0.0110, std: 1.3672
  Model pred mean: -0.0065, std: 1.2109
  Sigmas: [0.7890625]... (timesteps: [788.0])

[Step 60] Training Debug Info:
  Loss: 0.588454
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0105, std: 0.9062
  Noise mean: -0.0021, std: 1.0000
  Target mean: -0.0126, std: 1.3516
  Model pred mean: -0.0002, std: 1.0703
  Sigmas: [0.59375]... (timesteps: [592.0])

[Step 60] Training Debug Info:
  Loss: 0.558793
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0255, std: 0.9883
  Noise mean: 0.0054, std: 1.0000
  Target mean: -0.0200, std: 1.4062
  Model pred mean: -0.0286, std: 1.1719
  Sigmas: [0.8515625]... (timesteps: [852.0])
Steps:   1%|          | 61/5000 [16:27<16:23:18, 11.95s/it, loss=0.4405, lr=1.20e-06]Steps:   1%|          | 61/5000 [16:27<16:23:18, 11.95s/it, loss=0.5588, lr=1.22e-06]Steps:   1%|          | 62/5000 [16:39<16:21:20, 11.92s/it, loss=0.5588, lr=1.22e-06]Steps:   1%|          | 62/5000 [16:39<16:21:20, 11.92s/it, loss=1.2091, lr=1.24e-06]Steps:   1%|▏         | 63/5000 [16:51<16:21:45, 11.93s/it, loss=1.2091, lr=1.24e-06]Steps:   1%|▏         | 63/5000 [16:51<16:21:45, 11.93s/it, loss=0.4260, lr=1.26e-06]Steps:   1%|▏         | 64/5000 [17:03<16:25:42, 11.98s/it, loss=0.4260, lr=1.26e-06]Steps:   1%|▏         | 64/5000 [17:03<16:25:42, 11.98s/it, loss=0.4517, lr=1.28e-06]Steps:   1%|▏         | 65/5000 [17:15<16:25:19, 11.98s/it, loss=0.4517, lr=1.28e-06]Steps:   1%|▏         | 65/5000 [17:15<16:25:19, 11.98s/it, loss=0.4094, lr=1.30e-06]Steps:   1%|▏         | 66/5000 [17:26<16:22:28, 11.95s/it, loss=0.4094, lr=1.30e-06]Steps:   1%|▏         | 66/5000 [17:26<16:22:28, 11.95s/it, loss=1.0731, lr=1.32e-06]Steps:   1%|▏         | 67/5000 [17:38<16:21:51, 11.94s/it, loss=1.0731, lr=1.32e-06]Steps:   1%|▏         | 67/5000 [17:38<16:21:51, 11.94s/it, loss=0.4160, lr=1.34e-06]Steps:   1%|▏         | 68/5000 [17:50<16:21:15, 11.94s/it, loss=0.4160, lr=1.34e-06]Steps:   1%|▏         | 68/5000 [17:50<16:21:15, 11.94s/it, loss=1.1796, lr=1.36e-06]Steps:   1%|▏         | 69/5000 [18:02<16:19:25, 11.92s/it, loss=1.1796, lr=1.36e-06]Steps:   1%|▏         | 69/5000 [18:02<16:19:25, 11.92s/it, loss=0.8002, lr=1.38e-06]Steps:   1%|▏         | 70/5000 [18:14<16:22:51, 11.96s/it, loss=0.8002, lr=1.38e-06]Steps:   1%|▏         | 70/5000 [18:14<16:22:51, 11.96s/it, loss=0.4133, lr=1.40e-06]
[Step 70] Training Debug Info:
  Loss: 0.655928
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0089, std: 0.9141
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0088, std: 1.3516
  Model pred mean: 0.0063, std: 1.1641
  Sigmas: [0.54296875]... (timesteps: [544.0])

[Step 70] Training Debug Info:
  Loss: 0.393802
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0261, std: 0.9375
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0269, std: 1.3750
  Model pred mean: -0.0295, std: 1.2266
  Sigmas: [0.74609375]... (timesteps: [747.0])

[Step 70] Training Debug Info:
  Loss: 0.464334
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0010, std: 0.8828
  Noise mean: 0.0019, std: 1.0000
  Target mean: 0.0030, std: 1.3359
  Model pred mean: 0.0002, std: 1.1641
  Sigmas: [0.703125]... (timesteps: [705.0])

[Step 70] Training Debug Info:
  Loss: 0.519136
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0242, std: 0.9219
  Noise mean: -0.0012, std: 1.0000
  Target mean: -0.0254, std: 1.3594
  Model pred mean: -0.0286, std: 1.1797
  Sigmas: [0.66015625]... (timesteps: [659.0])
Steps:   1%|▏         | 71/5000 [18:26<16:26:17, 12.01s/it, loss=0.4133, lr=1.40e-06]Steps:   1%|▏         | 71/5000 [18:26<16:26:17, 12.01s/it, loss=0.5191, lr=1.42e-06]Steps:   1%|▏         | 72/5000 [18:38<16:23:56, 11.98s/it, loss=0.5191, lr=1.42e-06]Steps:   1%|▏         | 72/5000 [18:38<16:23:56, 11.98s/it, loss=0.6248, lr=1.44e-06]Steps:   1%|▏         | 73/5000 [18:50<16:22:14, 11.96s/it, loss=0.6248, lr=1.44e-06]Steps:   1%|▏         | 73/5000 [18:50<16:22:14, 11.96s/it, loss=0.6165, lr=1.46e-06]Steps:   1%|▏         | 74/5000 [19:02<16:22:05, 11.96s/it, loss=0.6165, lr=1.46e-06]Steps:   1%|▏         | 74/5000 [19:02<16:22:05, 11.96s/it, loss=0.7354, lr=1.48e-06]Steps:   2%|▏         | 75/5000 [19:14<16:20:58, 11.95s/it, loss=0.7354, lr=1.48e-06]Steps:   2%|▏         | 75/5000 [19:14<16:20:58, 11.95s/it, loss=0.6101, lr=1.50e-06]Steps:   2%|▏         | 76/5000 [19:26<16:18:27, 11.92s/it, loss=0.6101, lr=1.50e-06]Steps:   2%|▏         | 76/5000 [19:26<16:18:27, 11.92s/it, loss=1.1319, lr=1.52e-06]Steps:   2%|▏         | 77/5000 [19:38<16:22:08, 11.97s/it, loss=1.1319, lr=1.52e-06]Steps:   2%|▏         | 77/5000 [19:38<16:22:08, 11.97s/it, loss=0.8073, lr=1.54e-06]Steps:   2%|▏         | 78/5000 [19:50<16:17:48, 11.92s/it, loss=0.8073, lr=1.54e-06]Steps:   2%|▏         | 78/5000 [19:50<16:17:48, 11.92s/it, loss=1.1267, lr=1.56e-06]Steps:   2%|▏         | 79/5000 [20:02<16:20:06, 11.95s/it, loss=1.1267, lr=1.56e-06]Steps:   2%|▏         | 79/5000 [20:02<16:20:06, 11.95s/it, loss=1.0319, lr=1.58e-06]Steps:   2%|▏         | 80/5000 [20:14<16:19:10, 11.94s/it, loss=1.0319, lr=1.58e-06]Steps:   2%|▏         | 80/5000 [20:14<16:19:10, 11.94s/it, loss=0.5773, lr=1.60e-06]
[Step 80] Training Debug Info:
  Loss: 1.181698
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0031, std: 0.8984
  Noise mean: 0.0024, std: 1.0000
  Target mean: -0.0006, std: 1.3438
  Model pred mean: -0.0082, std: 0.8047
  Sigmas: [0.1298828125]... (timesteps: [130.0])

[Step 80] Training Debug Info:
  Loss: 1.102908
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0310, std: 0.9062
  Noise mean: -0.0022, std: 1.0000
  Target mean: -0.0332, std: 1.3516
  Model pred mean: -0.0300, std: 0.8516
  Sigmas: [0.244140625]... (timesteps: [244.0])

[Step 80] Training Debug Info:
  Loss: 0.407966
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0009, std: 0.8906
  Noise mean: 0.0008, std: 0.9961
  Target mean: 0.0017, std: 1.3359
  Model pred mean: -0.0010, std: 1.1641
  Sigmas: [0.81640625]... (timesteps: [817.0])

[Step 80] Training Debug Info:
  Loss: 0.510111
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0227, std: 0.8867
  Noise mean: 0.0014, std: 1.0000
  Target mean: 0.0242, std: 1.3359
  Model pred mean: 0.0154, std: 1.0938
  Sigmas: [0.90234375]... (timesteps: [901.0])
Steps:   2%|▏         | 81/5000 [20:26<16:18:27, 11.93s/it, loss=0.5773, lr=1.60e-06]Steps:   2%|▏         | 81/5000 [20:26<16:18:27, 11.93s/it, loss=0.5101, lr=1.62e-06]Steps:   2%|▏         | 82/5000 [20:38<16:16:25, 11.91s/it, loss=0.5101, lr=1.62e-06]Steps:   2%|▏         | 82/5000 [20:38<16:16:25, 11.91s/it, loss=0.5577, lr=1.64e-06]Steps:   2%|▏         | 83/5000 [20:49<16:15:16, 11.90s/it, loss=0.5577, lr=1.64e-06]Steps:   2%|▏         | 83/5000 [20:49<16:15:16, 11.90s/it, loss=0.4859, lr=1.66e-06]Steps:   2%|▏         | 84/5000 [21:02<16:19:55, 11.96s/it, loss=0.4859, lr=1.66e-06]Steps:   2%|▏         | 84/5000 [21:02<16:19:55, 11.96s/it, loss=1.0154, lr=1.68e-06]Steps:   2%|▏         | 85/5000 [21:13<16:18:48, 11.95s/it, loss=1.0154, lr=1.68e-06]Steps:   2%|▏         | 85/5000 [21:13<16:18:48, 11.95s/it, loss=0.4153, lr=1.70e-06]Steps:   2%|▏         | 86/5000 [21:25<16:18:10, 11.94s/it, loss=0.4153, lr=1.70e-06]Steps:   2%|▏         | 86/5000 [21:25<16:18:10, 11.94s/it, loss=0.5016, lr=1.72e-06]Steps:   2%|▏         | 87/5000 [21:37<16:15:48, 11.92s/it, loss=0.5016, lr=1.72e-06]Steps:   2%|▏         | 87/5000 [21:37<16:15:48, 11.92s/it, loss=0.9783, lr=1.74e-06]Steps:   2%|▏         | 88/5000 [21:49<16:18:49, 11.96s/it, loss=0.9783, lr=1.74e-06]Steps:   2%|▏         | 88/5000 [21:49<16:18:49, 11.96s/it, loss=1.0630, lr=1.76e-06]Steps:   2%|▏         | 89/5000 [22:01<16:17:21, 11.94s/it, loss=1.0630, lr=1.76e-06]Steps:   2%|▏         | 89/5000 [22:01<16:17:21, 11.94s/it, loss=0.5976, lr=1.78e-06]Steps:   2%|▏         | 90/5000 [22:13<16:15:38, 11.92s/it, loss=0.5976, lr=1.78e-06]Steps:   2%|▏         | 90/5000 [22:13<16:15:38, 11.92s/it, loss=0.3996, lr=1.80e-06]
[Step 90] Training Debug Info:
  Loss: 0.597896
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0201, std: 0.8867
  Noise mean: 0.0023, std: 1.0000
  Target mean: -0.0179, std: 1.3359
  Model pred mean: -0.0046, std: 1.0703
  Sigmas: [0.94921875]... (timesteps: [951.0])

[Step 90] Training Debug Info:
  Loss: 0.739711
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0059, std: 0.9258
  Noise mean: -0.0036, std: 1.0000
  Target mean: -0.0095, std: 1.3672
  Model pred mean: -0.0084, std: 1.0625
  Sigmas: [0.478515625]... (timesteps: [478.0])

[Step 90] Training Debug Info:
  Loss: 1.008305
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0366, std: 0.9414
  Noise mean: -0.0004, std: 0.9961
  Target mean: -0.0369, std: 1.3750
  Model pred mean: -0.0383, std: 0.9414
  Sigmas: [0.279296875]... (timesteps: [279.0])

[Step 90] Training Debug Info:
  Loss: 0.873823
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0366, std: 0.8945
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0364, std: 1.3438
  Model pred mean: -0.0371, std: 0.9531
  Sigmas: [0.390625]... (timesteps: [390.0])
Steps:   2%|▏         | 91/5000 [22:25<16:23:10, 12.02s/it, loss=0.3996, lr=1.80e-06]Steps:   2%|▏         | 91/5000 [22:25<16:23:10, 12.02s/it, loss=0.8738, lr=1.82e-06]Steps:   2%|▏         | 92/5000 [22:37<16:20:06, 11.98s/it, loss=0.8738, lr=1.82e-06]Steps:   2%|▏         | 92/5000 [22:37<16:20:06, 11.98s/it, loss=0.6526, lr=1.84e-06]Steps:   2%|▏         | 93/5000 [22:49<16:19:25, 11.98s/it, loss=0.6526, lr=1.84e-06]Steps:   2%|▏         | 93/5000 [22:49<16:19:25, 11.98s/it, loss=0.6006, lr=1.86e-06]Steps:   2%|▏         | 94/5000 [23:01<16:18:56, 11.97s/it, loss=0.6006, lr=1.86e-06]Steps:   2%|▏         | 94/5000 [23:01<16:18:56, 11.97s/it, loss=1.2296, lr=1.88e-06]Steps:   2%|▏         | 95/5000 [23:13<16:15:51, 11.94s/it, loss=1.2296, lr=1.88e-06]Steps:   2%|▏         | 95/5000 [23:13<16:15:51, 11.94s/it, loss=0.3310, lr=1.90e-06]Steps:   2%|▏         | 96/5000 [23:25<16:14:49, 11.93s/it, loss=0.3310, lr=1.90e-06]Steps:   2%|▏         | 96/5000 [23:25<16:14:49, 11.93s/it, loss=1.0911, lr=1.92e-06]Steps:   2%|▏         | 97/5000 [23:37<16:15:14, 11.93s/it, loss=1.0911, lr=1.92e-06]Steps:   2%|▏         | 97/5000 [23:37<16:15:14, 11.93s/it, loss=0.5777, lr=1.94e-06]Steps:   2%|▏         | 98/5000 [23:49<16:18:37, 11.98s/it, loss=0.5777, lr=1.94e-06]Steps:   2%|▏         | 98/5000 [23:49<16:18:37, 11.98s/it, loss=1.1812, lr=1.96e-06]Steps:   2%|▏         | 99/5000 [24:01<16:15:17, 11.94s/it, loss=1.1812, lr=1.96e-06]Steps:   2%|▏         | 99/5000 [24:01<16:15:17, 11.94s/it, loss=0.6986, lr=1.98e-06]Steps:   2%|▏         | 100/5000 [24:13<16:14:15, 11.93s/it, loss=0.6986, lr=1.98e-06]Steps:   2%|▏         | 100/5000 [24:13<16:14:15, 11.93s/it, loss=1.2216, lr=2.00e-06]01/22/2026 03:48:51 - INFO - __main__ - 
[Step 100] ✅ Loss in normal range (1.2216)
01/22/2026 03:48:51 - INFO - __main__ -   Loss avg (last 100): 0.7931
01/22/2026 03:48:51 - INFO - __main__ -   Loss range: [0.3310, 1.8890]

[Step 100] Training Debug Info:
  Loss: 0.717592
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0334, std: 0.9297
  Noise mean: 0.0053, std: 1.0000
  Target mean: -0.0281, std: 1.3672
  Model pred mean: -0.0356, std: 1.0781
  Sigmas: [0.4609375]... (timesteps: [461.0])

[Step 100] Training Debug Info:
  Loss: 0.781877
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0059, std: 0.9180
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0043, std: 1.3594
  Model pred mean: -0.0085, std: 1.0547
  Sigmas: [0.478515625]... (timesteps: [478.0])

[Step 100] Training Debug Info:
  Loss: 0.476084
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0021, std: 0.9688
  Noise mean: -0.0035, std: 1.0000
  Target mean: -0.0014, std: 1.3906
  Model pred mean: 0.0022, std: 1.2188
  Sigmas: [0.625]... (timesteps: [624.0])

[Step 100] Training Debug Info:
  Loss: 0.567920
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0108, std: 0.9180
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0106, std: 1.3594
  Model pred mean: -0.0045, std: 1.1328
  Sigmas: [0.953125]... (timesteps: [953.0])
Steps:   2%|▏         | 101/5000 [24:25<16:15:41, 11.95s/it, loss=1.2216, lr=2.00e-06]Steps:   2%|▏         | 101/5000 [24:25<16:15:41, 11.95s/it, loss=0.5679, lr=2.02e-06]Steps:   2%|▏         | 102/5000 [24:37<16:13:06, 11.92s/it, loss=0.5679, lr=2.02e-06]Steps:   2%|▏         | 102/5000 [24:37<16:13:06, 11.92s/it, loss=0.9706, lr=2.04e-06]Steps:   2%|▏         | 103/5000 [24:48<16:12:58, 11.92s/it, loss=0.9706, lr=2.04e-06]Steps:   2%|▏         | 103/5000 [24:48<16:12:58, 11.92s/it, loss=0.8196, lr=2.06e-06]Steps:   2%|▏         | 104/5000 [25:01<16:16:09, 11.96s/it, loss=0.8196, lr=2.06e-06]Steps:   2%|▏         | 104/5000 [25:01<16:16:09, 11.96s/it, loss=1.0623, lr=2.08e-06]Steps:   2%|▏         | 105/5000 [25:12<16:13:39, 11.93s/it, loss=1.0623, lr=2.08e-06]Steps:   2%|▏         | 105/5000 [25:12<16:13:39, 11.93s/it, loss=0.4806, lr=2.10e-06]Steps:   2%|▏         | 106/5000 [25:24<16:15:27, 11.96s/it, loss=0.4806, lr=2.10e-06]Steps:   2%|▏         | 106/5000 [25:24<16:15:27, 11.96s/it, loss=0.9356, lr=2.12e-06]Steps:   2%|▏         | 107/5000 [25:36<16:12:23, 11.92s/it, loss=0.9356, lr=2.12e-06]Steps:   2%|▏         | 107/5000 [25:36<16:12:23, 11.92s/it, loss=0.9232, lr=2.14e-06]Steps:   2%|▏         | 108/5000 [25:48<16:11:56, 11.92s/it, loss=0.9232, lr=2.14e-06]Steps:   2%|▏         | 108/5000 [25:48<16:11:56, 11.92s/it, loss=1.1054, lr=2.16e-06]Steps:   2%|▏         | 109/5000 [26:00<16:10:28, 11.91s/it, loss=1.1054, lr=2.16e-06]Steps:   2%|▏         | 109/5000 [26:00<16:10:28, 11.91s/it, loss=0.7251, lr=2.18e-06]Steps:   2%|▏         | 110/5000 [26:12<16:08:08, 11.88s/it, loss=0.7251, lr=2.18e-06]Steps:   2%|▏         | 110/5000 [26:12<16:08:08, 11.88s/it, loss=1.1876, lr=2.20e-06]
[Step 110] Training Debug Info:
  Loss: 0.569485
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0496, std: 0.9531
  Noise mean: 0.0028, std: 1.0000
  Target mean: -0.0466, std: 1.3828
  Model pred mean: -0.0513, std: 1.1406
  Sigmas: [0.51171875]... (timesteps: [513.0])

[Step 110] Training Debug Info:
  Loss: 1.171261
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0017, std: 0.8750
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0015, std: 1.3281
  Model pred mean: 0.0028, std: 0.7695
  Sigmas: [0.10986328125]... (timesteps: [110.0])

[Step 110] Training Debug Info:
  Loss: 1.044782
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0007, std: 0.9336
  Noise mean: 0.0000, std: 0.9961
  Target mean: 0.0007, std: 1.3672
  Model pred mean: 0.0003, std: 0.8867
  Sigmas: [0.02001953125]... (timesteps: [20.0])

[Step 110] Training Debug Info:
  Loss: 0.424559
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0069, std: 0.9375
  Noise mean: 0.0002, std: 1.0000
  Target mean: 0.0071, std: 1.3750
  Model pred mean: 0.0052, std: 1.2109
  Sigmas: [0.7421875]... (timesteps: [744.0])
Steps:   2%|▏         | 111/5000 [26:24<16:14:20, 11.96s/it, loss=1.1876, lr=2.20e-06]Steps:   2%|▏         | 111/5000 [26:24<16:14:20, 11.96s/it, loss=0.4246, lr=2.22e-06]Steps:   2%|▏         | 112/5000 [26:36<16:11:07, 11.92s/it, loss=0.4246, lr=2.22e-06]Steps:   2%|▏         | 112/5000 [26:36<16:11:07, 11.92s/it, loss=0.3608, lr=2.24e-06]Steps:   2%|▏         | 113/5000 [26:48<16:12:03, 11.93s/it, loss=0.3608, lr=2.24e-06]Steps:   2%|▏         | 113/5000 [26:48<16:12:03, 11.93s/it, loss=0.7297, lr=2.26e-06]Steps:   2%|▏         | 114/5000 [27:00<16:11:13, 11.93s/it, loss=0.7297, lr=2.26e-06]Steps:   2%|▏         | 114/5000 [27:00<16:11:13, 11.93s/it, loss=1.0832, lr=2.28e-06]Steps:   2%|▏         | 115/5000 [27:12<16:12:57, 11.95s/it, loss=1.0832, lr=2.28e-06]Steps:   2%|▏         | 115/5000 [27:12<16:12:57, 11.95s/it, loss=0.8856, lr=2.30e-06]Steps:   2%|▏         | 116/5000 [27:24<16:11:21, 11.93s/it, loss=0.8856, lr=2.30e-06]Steps:   2%|▏         | 116/5000 [27:24<16:11:21, 11.93s/it, loss=0.4421, lr=2.32e-06]Steps:   2%|▏         | 117/5000 [27:35<16:10:20, 11.92s/it, loss=0.4421, lr=2.32e-06]Steps:   2%|▏         | 117/5000 [27:35<16:10:20, 11.92s/it, loss=1.1028, lr=2.34e-06]Steps:   2%|▏         | 118/5000 [27:48<16:13:49, 11.97s/it, loss=1.1028, lr=2.34e-06]Steps:   2%|▏         | 118/5000 [27:48<16:13:49, 11.97s/it, loss=0.6526, lr=2.36e-06]Steps:   2%|▏         | 119/5000 [27:59<16:12:50, 11.96s/it, loss=0.6526, lr=2.36e-06]Steps:   2%|▏         | 119/5000 [27:59<16:12:50, 11.96s/it, loss=0.3810, lr=2.38e-06]Steps:   2%|▏         | 120/5000 [28:11<16:11:18, 11.94s/it, loss=0.3810, lr=2.38e-06]Steps:   2%|▏         | 120/5000 [28:11<16:11:18, 11.94s/it, loss=0.3888, lr=2.40e-06]
[Step 120] Training Debug Info:
  Loss: 0.390080
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0140, std: 0.9648
  Noise mean: -0.0035, std: 1.0000
  Target mean: -0.0176, std: 1.3906
  Model pred mean: -0.0044, std: 1.2266
  Sigmas: [0.796875]... (timesteps: [795.0])

[Step 120] Training Debug Info:
  Loss: 0.909142
  Latent shape: torch.Size([1, 32, 54, 156]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0030, std: 0.8594
  Noise mean: -0.0011, std: 1.0000
  Target mean: -0.0040, std: 1.3203
  Model pred mean: -0.0006, std: 0.9219
  Sigmas: [0.43359375]... (timesteps: [433.0])

[Step 120] Training Debug Info:
  Loss: 0.402360
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0276, std: 0.9297
  Noise mean: -0.0027, std: 1.0000
  Target mean: 0.0249, std: 1.3672
  Model pred mean: 0.0248, std: 1.2031
  Sigmas: [0.796875]... (timesteps: [795.0])

[Step 120] Training Debug Info:
  Loss: 0.366637
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0072, std: 0.8906
  Noise mean: 0.0019, std: 1.0000
  Target mean: -0.0053, std: 1.3359
  Model pred mean: -0.0060, std: 1.1797
  Sigmas: [0.82421875]... (timesteps: [826.0])
Steps:   2%|▏         | 121/5000 [28:23<16:11:17, 11.94s/it, loss=0.3888, lr=2.40e-06]Steps:   2%|▏         | 121/5000 [28:23<16:11:17, 11.94s/it, loss=0.3666, lr=2.42e-06]Steps:   2%|▏         | 122/5000 [28:35<16:08:00, 11.91s/it, loss=0.3666, lr=2.42e-06]Steps:   2%|▏         | 122/5000 [28:35<16:08:00, 11.91s/it, loss=1.0362, lr=2.44e-06]Steps:   2%|▏         | 123/5000 [28:47<16:07:27, 11.90s/it, loss=1.0362, lr=2.44e-06]Steps:   2%|▏         | 123/5000 [28:47<16:07:27, 11.90s/it, loss=0.7873, lr=2.46e-06]Steps:   2%|▏         | 124/5000 [28:59<16:09:42, 11.93s/it, loss=0.7873, lr=2.46e-06]Steps:   2%|▏         | 124/5000 [28:59<16:09:42, 11.93s/it, loss=1.0711, lr=2.48e-06]Steps:   2%|▎         | 125/5000 [29:11<16:12:36, 11.97s/it, loss=1.0711, lr=2.48e-06]Steps:   2%|▎         | 125/5000 [29:11<16:12:36, 11.97s/it, loss=0.4219, lr=2.50e-06]Steps:   3%|▎         | 126/5000 [29:23<16:10:13, 11.94s/it, loss=0.4219, lr=2.50e-06]Steps:   3%|▎         | 126/5000 [29:23<16:10:13, 11.94s/it, loss=1.1509, lr=2.52e-06]Steps:   3%|▎         | 127/5000 [29:35<16:08:58, 11.93s/it, loss=1.1509, lr=2.52e-06]Steps:   3%|▎         | 127/5000 [29:35<16:08:58, 11.93s/it, loss=0.6346, lr=2.54e-06]Steps:   3%|▎         | 128/5000 [29:47<16:07:29, 11.91s/it, loss=0.6346, lr=2.54e-06]Steps:   3%|▎         | 128/5000 [29:47<16:07:29, 11.91s/it, loss=0.3917, lr=2.56e-06]Steps:   3%|▎         | 129/5000 [29:59<16:07:14, 11.91s/it, loss=0.3917, lr=2.56e-06]Steps:   3%|▎         | 129/5000 [29:59<16:07:14, 11.91s/it, loss=0.6328, lr=2.58e-06]Steps:   3%|▎         | 130/5000 [30:11<16:05:53, 11.90s/it, loss=0.6328, lr=2.58e-06]Steps:   3%|▎         | 130/5000 [30:11<16:05:53, 11.90s/it, loss=1.1153, lr=2.60e-06]
[Step 130] Training Debug Info:
  Loss: 0.584766
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0317, std: 0.9609
  Noise mean: 0.0000, std: 1.0000
  Target mean: -0.0317, std: 1.3828
  Model pred mean: -0.0312, std: 1.1562
  Sigmas: [0.546875]... (timesteps: [547.0])

[Step 130] Training Debug Info:
  Loss: 1.116641
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0291, std: 0.9336
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0283, std: 1.3672
  Model pred mean: -0.0330, std: 0.8672
  Sigmas: [0.1865234375]... (timesteps: [187.0])

[Step 130] Training Debug Info:
  Loss: 1.172308
  Latent shape: torch.Size([1, 32, 138, 66]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0023, std: 0.9023
  Noise mean: 0.0025, std: 1.0000
  Target mean: 0.0001, std: 1.3438
  Model pred mean: 0.0002, std: 0.8086
  Sigmas: [0.138671875]... (timesteps: [139.0])

[Step 130] Training Debug Info:
  Loss: 0.460359
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0119, std: 0.9062
  Noise mean: 0.0005, std: 1.0000
  Target mean: 0.0124, std: 1.3516
  Model pred mean: -0.0072, std: 1.1484
  Sigmas: [0.93359375]... (timesteps: [935.0])
Steps:   3%|▎         | 131/5000 [30:23<16:12:51, 11.99s/it, loss=1.1153, lr=2.60e-06]Steps:   3%|▎         | 131/5000 [30:23<16:12:51, 11.99s/it, loss=0.4604, lr=2.62e-06]Steps:   3%|▎         | 132/5000 [30:35<16:09:08, 11.95s/it, loss=0.4604, lr=2.62e-06]Steps:   3%|▎         | 132/5000 [30:35<16:09:08, 11.95s/it, loss=0.8710, lr=2.64e-06]Steps:   3%|▎         | 133/5000 [30:47<16:08:29, 11.94s/it, loss=0.8710, lr=2.64e-06]Steps:   3%|▎         | 133/5000 [30:47<16:08:29, 11.94s/it, loss=1.0552, lr=2.66e-06]Steps:   3%|▎         | 134/5000 [30:58<16:07:48, 11.93s/it, loss=1.0552, lr=2.66e-06]Steps:   3%|▎         | 134/5000 [30:58<16:07:48, 11.93s/it, loss=0.8058, lr=2.68e-06]Steps:   3%|▎         | 135/5000 [31:10<16:07:01, 11.93s/it, loss=0.8058, lr=2.68e-06]Steps:   3%|▎         | 135/5000 [31:10<16:07:01, 11.93s/it, loss=0.6658, lr=2.70e-06]Steps:   3%|▎         | 136/5000 [31:22<16:04:20, 11.90s/it, loss=0.6658, lr=2.70e-06]Steps:   3%|▎         | 136/5000 [31:22<16:04:20, 11.90s/it, loss=0.6378, lr=2.72e-06]Steps:   3%|▎         | 137/5000 [31:34<16:03:44, 11.89s/it, loss=0.6378, lr=2.72e-06]Steps:   3%|▎         | 137/5000 [31:34<16:03:44, 11.89s/it, loss=0.5318, lr=2.74e-06]Steps:   3%|▎         | 138/5000 [31:46<16:07:40, 11.94s/it, loss=0.5318, lr=2.74e-06]Steps:   3%|▎         | 138/5000 [31:46<16:07:40, 11.94s/it, loss=1.1546, lr=2.76e-06]Steps:   3%|▎         | 139/5000 [31:58<16:05:00, 11.91s/it, loss=1.1546, lr=2.76e-06]Steps:   3%|▎         | 139/5000 [31:58<16:05:00, 11.91s/it, loss=0.9278, lr=2.78e-06]Steps:   3%|▎         | 140/5000 [32:10<16:04:11, 11.90s/it, loss=0.9278, lr=2.78e-06]Steps:   3%|▎         | 140/5000 [32:10<16:04:11, 11.90s/it, loss=1.0630, lr=2.80e-06]
[Step 140] Training Debug Info:
  Loss: 0.800874
  Latent shape: torch.Size([1, 32, 126, 72]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0016, std: 0.9180
  Noise mean: -0.0036, std: 1.0000
  Target mean: -0.0052, std: 1.3594
  Model pred mean: 0.0005, std: 1.0234
  Sigmas: [0.474609375]... (timesteps: [474.0])

[Step 140] Training Debug Info:
  Loss: 0.434639
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: -0.0097, std: 0.9062
  Noise mean: -0.0022, std: 1.0000
  Target mean: 0.0075, std: 1.3516
  Model pred mean: 0.0200, std: 1.1641
  Sigmas: [0.9296875]... (timesteps: [931.0])

[Step 140] Training Debug Info:
  Loss: 0.413609
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0027, std: 0.9062
  Noise mean: 0.0039, std: 1.0000
  Target mean: 0.0066, std: 1.3516
  Model pred mean: 0.0058, std: 1.1719
  Sigmas: [0.8515625]... (timesteps: [851.0])

[Step 140] Training Debug Info:
  Loss: 0.356610
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0222, std: 0.9219
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0215, std: 1.3594
  Model pred mean: -0.0164, std: 1.2188
  Sigmas: [0.8125]... (timesteps: [811.0])
Steps:   3%|▎         | 141/5000 [32:22<16:02:35, 11.89s/it, loss=1.0630, lr=2.80e-06]Steps:   3%|▎         | 141/5000 [32:22<16:02:35, 11.89s/it, loss=0.3566, lr=2.82e-06]Steps:   3%|▎         | 142/5000 [32:34<16:06:19, 11.93s/it, loss=0.3566, lr=2.82e-06]Steps:   3%|▎         | 142/5000 [32:34<16:06:19, 11.93s/it, loss=0.6617, lr=2.84e-06]Steps:   3%|▎         | 143/5000 [32:46<16:05:32, 11.93s/it, loss=0.6617, lr=2.84e-06]Steps:   3%|▎         | 143/5000 [32:46<16:05:32, 11.93s/it, loss=1.0800, lr=2.86e-06]Steps:   3%|▎         | 144/5000 [32:58<16:04:33, 11.92s/it, loss=1.0800, lr=2.86e-06]Steps:   3%|▎         | 144/5000 [32:58<16:04:33, 11.92s/it, loss=1.1307, lr=2.88e-06]Steps:   3%|▎         | 145/5000 [33:10<16:08:47, 11.97s/it, loss=1.1307, lr=2.88e-06]Steps:   3%|▎         | 145/5000 [33:10<16:08:47, 11.97s/it, loss=0.5500, lr=2.90e-06]Steps:   3%|▎         | 146/5000 [33:21<16:04:10, 11.92s/it, loss=0.5500, lr=2.90e-06]Steps:   3%|▎         | 146/5000 [33:21<16:04:10, 11.92s/it, loss=0.9341, lr=2.92e-06]Steps:   3%|▎         | 147/5000 [33:33<16:01:43, 11.89s/it, loss=0.9341, lr=2.92e-06]Steps:   3%|▎         | 147/5000 [33:33<16:01:43, 11.89s/it, loss=0.8170, lr=2.94e-06]Steps:   3%|▎         | 148/5000 [33:45<16:00:46, 11.88s/it, loss=0.8170, lr=2.94e-06]Steps:   3%|▎         | 148/5000 [33:45<16:00:46, 11.88s/it, loss=0.7206, lr=2.96e-06]Steps:   3%|▎         | 149/5000 [33:57<16:01:08, 11.89s/it, loss=0.7206, lr=2.96e-06]Steps:   3%|▎         | 149/5000 [33:57<16:01:08, 11.89s/it, loss=0.7427, lr=2.98e-06]Steps:   3%|▎         | 150/5000 [34:09<16:00:51, 11.89s/it, loss=0.7427, lr=2.98e-06]Steps:   3%|▎         | 150/5000 [34:09<16:00:51, 11.89s/it, loss=1.1539, lr=3.00e-06]
[Step 150] Training Debug Info:
  Loss: 0.574136
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0134, std: 0.8906
  Noise mean: -0.0009, std: 1.0000
  Target mean: 0.0125, std: 1.3359
  Model pred mean: -0.0052, std: 1.0938
  Sigmas: [0.97265625]... (timesteps: [974.0])

[Step 150] Training Debug Info:
  Loss: 0.460522
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0527, std: 0.9219
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0520, std: 1.3594
  Model pred mean: -0.0574, std: 1.1719
  Sigmas: [0.6953125]... (timesteps: [694.0])

[Step 150] Training Debug Info:
  Loss: 0.581881
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0161, std: 0.8945
  Noise mean: -0.0022, std: 1.0000
  Target mean: -0.0184, std: 1.3438
  Model pred mean: -0.0178, std: 1.1094
  Sigmas: [0.59765625]... (timesteps: [598.0])

[Step 150] Training Debug Info:
  Loss: 0.472940
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0103, std: 0.8750
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0112, std: 1.3281
  Model pred mean: 0.0077, std: 1.1328
  Sigmas: [0.92578125]... (timesteps: [925.0])
Steps:   3%|▎         | 151/5000 [34:21<16:01:03, 11.89s/it, loss=1.1539, lr=3.00e-06]Steps:   3%|▎         | 151/5000 [34:21<16:01:03, 11.89s/it, loss=0.4729, lr=3.02e-06]Steps:   3%|▎         | 152/5000 [34:33<16:07:16, 11.97s/it, loss=0.4729, lr=3.02e-06]Steps:   3%|▎         | 152/5000 [34:33<16:07:16, 11.97s/it, loss=1.0402, lr=3.04e-06]Steps:   3%|▎         | 153/5000 [34:45<16:04:33, 11.94s/it, loss=1.0402, lr=3.04e-06]Steps:   3%|▎         | 153/5000 [34:45<16:04:33, 11.94s/it, loss=1.1068, lr=3.06e-06]Steps:   3%|▎         | 154/5000 [34:57<16:01:04, 11.90s/it, loss=1.1068, lr=3.06e-06]Steps:   3%|▎         | 154/5000 [34:57<16:01:04, 11.90s/it, loss=0.4605, lr=3.08e-06]Steps:   3%|▎         | 155/5000 [35:08<15:59:15, 11.88s/it, loss=0.4605, lr=3.08e-06]Steps:   3%|▎         | 155/5000 [35:08<15:59:15, 11.88s/it, loss=0.6983, lr=3.10e-06]Steps:   3%|▎         | 156/5000 [35:20<15:58:27, 11.87s/it, loss=0.6983, lr=3.10e-06]Steps:   3%|▎         | 156/5000 [35:20<15:58:27, 11.87s/it, loss=0.4183, lr=3.12e-06]Steps:   3%|▎         | 157/5000 [35:32<15:58:37, 11.88s/it, loss=0.4183, lr=3.12e-06]Steps:   3%|▎         | 157/5000 [35:32<15:58:37, 11.88s/it, loss=0.8052, lr=3.14e-06]Steps:   3%|▎         | 158/5000 [35:44<16:02:59, 11.93s/it, loss=0.8052, lr=3.14e-06]Steps:   3%|▎         | 158/5000 [35:44<16:02:59, 11.93s/it, loss=1.1458, lr=3.16e-06]Steps:   3%|▎         | 159/5000 [35:56<16:00:51, 11.91s/it, loss=1.1458, lr=3.16e-06]Steps:   3%|▎         | 159/5000 [35:56<16:00:51, 11.91s/it, loss=0.4523, lr=3.18e-06]Steps:   3%|▎         | 160/5000 [36:08<16:03:04, 11.94s/it, loss=0.4523, lr=3.18e-06]Steps:   3%|▎         | 160/5000 [36:08<16:03:04, 11.94s/it, loss=0.5668, lr=3.20e-06]
[Step 160] Training Debug Info:
  Loss: 1.110595
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0160, std: 0.8945
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0159, std: 1.3438
  Model pred mean: 0.0166, std: 0.8242
  Sigmas: [0.06396484375]... (timesteps: [64.0])

[Step 160] Training Debug Info:
  Loss: 1.089890
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0586, std: 0.9375
  Noise mean: 0.0026, std: 1.0000
  Target mean: -0.0559, std: 1.3672
  Model pred mean: -0.0527, std: 0.8828
  Sigmas: [0.1865234375]... (timesteps: [187.0])

[Step 160] Training Debug Info:
  Loss: 0.587464
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0020, std: 0.8867
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0019, std: 1.3359
  Model pred mean: -0.0048, std: 1.1250
  Sigmas: [0.98828125]... (timesteps: [990.0])

[Step 160] Training Debug Info:
  Loss: 0.417147
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0269, std: 0.9023
  Noise mean: -0.0004, std: 1.0000
  Target mean: -0.0272, std: 1.3438
  Model pred mean: -0.0250, std: 1.1797
  Sigmas: [0.73828125]... (timesteps: [739.0])
Steps:   3%|▎         | 161/5000 [36:20<16:00:39, 11.91s/it, loss=0.5668, lr=3.20e-06]Steps:   3%|▎         | 161/5000 [36:20<16:00:39, 11.91s/it, loss=0.4171, lr=3.22e-06]Steps:   3%|▎         | 162/5000 [36:32<16:02:28, 11.94s/it, loss=0.4171, lr=3.22e-06]Steps:   3%|▎         | 162/5000 [36:32<16:02:28, 11.94s/it, loss=0.4691, lr=3.24e-06]Steps:   3%|▎         | 163/5000 [36:44<15:59:52, 11.91s/it, loss=0.4691, lr=3.24e-06]Steps:   3%|▎         | 163/5000 [36:44<15:59:52, 11.91s/it, loss=0.7941, lr=3.26e-06]Steps:   3%|▎         | 164/5000 [36:56<15:59:50, 11.91s/it, loss=0.7941, lr=3.26e-06]Steps:   3%|▎         | 164/5000 [36:56<15:59:50, 11.91s/it, loss=0.4217, lr=3.28e-06]Steps:   3%|▎         | 165/5000 [37:08<16:02:46, 11.95s/it, loss=0.4217, lr=3.28e-06]Steps:   3%|▎         | 165/5000 [37:08<16:02:46, 11.95s/it, loss=0.8987, lr=3.30e-06]Steps:   3%|▎         | 166/5000 [37:20<16:02:30, 11.95s/it, loss=0.8987, lr=3.30e-06]Steps:   3%|▎         | 166/5000 [37:20<16:02:30, 11.95s/it, loss=0.8088, lr=3.32e-06]Steps:   3%|▎         | 167/5000 [37:32<16:03:23, 11.96s/it, loss=0.8088, lr=3.32e-06]Steps:   3%|▎         | 167/5000 [37:32<16:03:23, 11.96s/it, loss=0.4430, lr=3.34e-06]Steps:   3%|▎         | 168/5000 [37:44<16:02:36, 11.95s/it, loss=0.4430, lr=3.34e-06]Steps:   3%|▎         | 168/5000 [37:44<16:02:36, 11.95s/it, loss=1.1044, lr=3.36e-06]Steps:   3%|▎         | 169/5000 [37:56<16:03:24, 11.97s/it, loss=1.1044, lr=3.36e-06]Steps:   3%|▎         | 169/5000 [37:56<16:03:24, 11.97s/it, loss=0.5792, lr=3.38e-06]Steps:   3%|▎         | 170/5000 [38:08<16:01:50, 11.95s/it, loss=0.5792, lr=3.38e-06]Steps:   3%|▎         | 170/5000 [38:08<16:01:50, 11.95s/it, loss=0.7898, lr=3.40e-06]
[Step 170] Training Debug Info:
  Loss: 0.430819
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0339, std: 0.9023
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0349, std: 1.3516
  Model pred mean: -0.0315, std: 1.1875
  Sigmas: [0.79296875]... (timesteps: [792.0])

[Step 170] Training Debug Info:
  Loss: 0.983905
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0055, std: 0.9141
  Noise mean: -0.0030, std: 1.0000
  Target mean: -0.0085, std: 1.3516
  Model pred mean: -0.0099, std: 0.9141
  Sigmas: [0.34765625]... (timesteps: [347.0])

[Step 170] Training Debug Info:
  Loss: 0.518949
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0325, std: 0.9062
  Noise mean: -0.0014, std: 1.0000
  Target mean: 0.0310, std: 1.3516
  Model pred mean: 0.0337, std: 1.1562
  Sigmas: [0.66015625]... (timesteps: [661.0])

[Step 170] Training Debug Info:
  Loss: 1.081678
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0133, std: 0.9219
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0125, std: 1.3594
  Model pred mean: -0.0132, std: 0.8750
  Sigmas: [0.0439453125]... (timesteps: [44.0])
Steps:   3%|▎         | 171/5000 [38:19<16:00:26, 11.93s/it, loss=0.7898, lr=3.40e-06]Steps:   3%|▎         | 171/5000 [38:19<16:00:26, 11.93s/it, loss=1.0817, lr=3.42e-06]Steps:   3%|▎         | 172/5000 [38:32<16:05:46, 12.00s/it, loss=1.0817, lr=3.42e-06]Steps:   3%|▎         | 172/5000 [38:32<16:05:46, 12.00s/it, loss=0.4181, lr=3.44e-06]Steps:   3%|▎         | 173/5000 [38:43<16:02:56, 11.97s/it, loss=0.4181, lr=3.44e-06]Steps:   3%|▎         | 173/5000 [38:43<16:02:56, 11.97s/it, loss=1.0284, lr=3.46e-06]Steps:   3%|▎         | 174/5000 [38:55<16:01:48, 11.96s/it, loss=1.0284, lr=3.46e-06]Steps:   3%|▎         | 174/5000 [38:55<16:01:48, 11.96s/it, loss=1.1130, lr=3.48e-06]Steps:   4%|▎         | 175/5000 [39:07<15:56:57, 11.90s/it, loss=1.1130, lr=3.48e-06]Steps:   4%|▎         | 175/5000 [39:07<15:56:57, 11.90s/it, loss=0.4539, lr=3.50e-06]Steps:   4%|▎         | 176/5000 [39:19<15:56:00, 11.89s/it, loss=0.4539, lr=3.50e-06]Steps:   4%|▎         | 176/5000 [39:19<15:56:00, 11.89s/it, loss=0.3979, lr=3.52e-06]Steps:   4%|▎         | 177/5000 [39:31<15:56:10, 11.90s/it, loss=0.3979, lr=3.52e-06]Steps:   4%|▎         | 177/5000 [39:31<15:56:10, 11.90s/it, loss=0.9299, lr=3.54e-06]Steps:   4%|▎         | 178/5000 [39:43<15:56:32, 11.90s/it, loss=0.9299, lr=3.54e-06]Steps:   4%|▎         | 178/5000 [39:43<15:56:32, 11.90s/it, loss=0.6054, lr=3.56e-06]Steps:   4%|▎         | 179/5000 [39:55<16:00:08, 11.95s/it, loss=0.6054, lr=3.56e-06]Steps:   4%|▎         | 179/5000 [39:55<16:00:08, 11.95s/it, loss=0.4658, lr=3.58e-06]Steps:   4%|▎         | 180/5000 [40:07<15:58:47, 11.94s/it, loss=0.4658, lr=3.58e-06]Steps:   4%|▎         | 180/5000 [40:07<15:58:47, 11.94s/it, loss=1.1769, lr=3.60e-06]
[Step 180] Training Debug Info:
  Loss: 1.116655
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0128, std: 0.8789
  Noise mean: 0.0003, std: 1.0000
  Target mean: 0.0131, std: 1.3281
  Model pred mean: 0.0153, std: 0.8203
  Sigmas: [0.318359375]... (timesteps: [319.0])

[Step 180] Training Debug Info:
  Loss: 1.163122
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0146, std: 0.8906
  Noise mean: -0.0012, std: 0.9961
  Target mean: 0.0134, std: 1.3359
  Model pred mean: 0.0183, std: 0.7852
  Sigmas: [0.10400390625]... (timesteps: [104.0])

[Step 180] Training Debug Info:
  Loss: 0.401876
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0364, std: 0.9414
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0371, std: 1.3750
  Model pred mean: -0.0371, std: 1.2031
  Sigmas: [0.6875]... (timesteps: [688.0])

[Step 180] Training Debug Info:
  Loss: 1.106864
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0089, std: 0.8398
  Noise mean: -0.0013, std: 1.0000
  Target mean: 0.0076, std: 1.3047
  Model pred mean: 0.0099, std: 0.7891
  Sigmas: [0.33203125]... (timesteps: [332.0])
Steps:   4%|▎         | 181/5000 [40:19<15:57:39, 11.92s/it, loss=1.1769, lr=3.60e-06]Steps:   4%|▎         | 181/5000 [40:19<15:57:39, 11.92s/it, loss=1.1069, lr=3.62e-06]Steps:   4%|▎         | 182/5000 [40:31<15:58:02, 11.93s/it, loss=1.1069, lr=3.62e-06]Steps:   4%|▎         | 182/5000 [40:31<15:58:02, 11.93s/it, loss=0.4016, lr=3.64e-06]Steps:   4%|▎         | 183/5000 [40:43<15:56:24, 11.91s/it, loss=0.4016, lr=3.64e-06]Steps:   4%|▎         | 183/5000 [40:43<15:56:24, 11.91s/it, loss=1.2029, lr=3.66e-06]Steps:   4%|▎         | 184/5000 [40:54<15:55:20, 11.90s/it, loss=1.2029, lr=3.66e-06]Steps:   4%|▎         | 184/5000 [40:54<15:55:20, 11.90s/it, loss=0.3792, lr=3.68e-06]Steps:   4%|▎         | 185/5000 [41:06<15:57:05, 11.93s/it, loss=0.3792, lr=3.68e-06]Steps:   4%|▎         | 185/5000 [41:06<15:57:05, 11.93s/it, loss=0.9361, lr=3.70e-06]Steps:   4%|▎         | 186/5000 [41:18<15:55:52, 11.91s/it, loss=0.9361, lr=3.70e-06]Steps:   4%|▎         | 186/5000 [41:18<15:55:52, 11.91s/it, loss=0.4536, lr=3.72e-06]Steps:   4%|▎         | 187/5000 [41:30<15:58:40, 11.95s/it, loss=0.4536, lr=3.72e-06]Steps:   4%|▎         | 187/5000 [41:30<15:58:40, 11.95s/it, loss=1.1412, lr=3.74e-06]Steps:   4%|▍         | 188/5000 [41:42<15:56:21, 11.92s/it, loss=1.1412, lr=3.74e-06]Steps:   4%|▍         | 188/5000 [41:42<15:56:21, 11.92s/it, loss=0.6512, lr=3.76e-06]Steps:   4%|▍         | 189/5000 [41:54<15:55:29, 11.92s/it, loss=0.6512, lr=3.76e-06]Steps:   4%|▍         | 189/5000 [41:54<15:55:29, 11.92s/it, loss=1.1308, lr=3.78e-06]Steps:   4%|▍         | 190/5000 [42:06<15:54:58, 11.91s/it, loss=1.1308, lr=3.78e-06]Steps:   4%|▍         | 190/5000 [42:06<15:54:58, 11.91s/it, loss=0.5397, lr=3.80e-06]
[Step 190] Training Debug Info:
  Loss: 0.517753
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0145, std: 0.9648
  Noise mean: 0.0020, std: 1.0000
  Target mean: -0.0125, std: 1.3906
  Model pred mean: -0.0130, std: 1.2031
  Sigmas: [0.63671875]... (timesteps: [636.0])

[Step 190] Training Debug Info:
  Loss: 0.394076
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0204, std: 0.8516
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0219, std: 1.3125
  Model pred mean: -0.0166, std: 1.1484
  Sigmas: [0.82421875]... (timesteps: [826.0])

[Step 190] Training Debug Info:
  Loss: 0.581997
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0098, std: 0.8945
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0086, std: 1.3438
  Model pred mean: -0.0223, std: 1.0938
  Sigmas: [0.9921875]... (timesteps: [991.0])

[Step 190] Training Debug Info:
  Loss: 1.188735
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0010, std: 0.9023
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0002, std: 1.3438
  Model pred mean: -0.0013, std: 0.7930
  Sigmas: [0.21875]... (timesteps: [219.0])
Steps:   4%|▍         | 191/5000 [42:18<15:54:24, 11.91s/it, loss=0.5397, lr=3.80e-06]Steps:   4%|▍         | 191/5000 [42:18<15:54:24, 11.91s/it, loss=1.1887, lr=3.82e-06]Steps:   4%|▍         | 192/5000 [42:30<15:59:55, 11.98s/it, loss=1.1887, lr=3.82e-06]Steps:   4%|▍         | 192/5000 [42:30<15:59:55, 11.98s/it, loss=1.0265, lr=3.84e-06]Steps:   4%|▍         | 193/5000 [42:42<15:55:34, 11.93s/it, loss=1.0265, lr=3.84e-06]Steps:   4%|▍         | 193/5000 [42:42<15:55:34, 11.93s/it, loss=0.4440, lr=3.86e-06]Steps:   4%|▍         | 194/5000 [42:54<15:54:18, 11.91s/it, loss=0.4440, lr=3.86e-06]Steps:   4%|▍         | 194/5000 [42:54<15:54:18, 11.91s/it, loss=0.8312, lr=3.88e-06]Steps:   4%|▍         | 195/5000 [43:06<15:52:33, 11.89s/it, loss=0.8312, lr=3.88e-06]Steps:   4%|▍         | 195/5000 [43:06<15:52:33, 11.89s/it, loss=0.4796, lr=3.90e-06]Steps:   4%|▍         | 196/5000 [43:18<15:55:14, 11.93s/it, loss=0.4796, lr=3.90e-06]Steps:   4%|▍         | 196/5000 [43:18<15:55:14, 11.93s/it, loss=0.5765, lr=3.92e-06]Steps:   4%|▍         | 197/5000 [43:30<15:56:13, 11.95s/it, loss=0.5765, lr=3.92e-06]Steps:   4%|▍         | 197/5000 [43:30<15:56:13, 11.95s/it, loss=1.0456, lr=3.94e-06]Steps:   4%|▍         | 198/5000 [43:42<15:55:23, 11.94s/it, loss=1.0456, lr=3.94e-06]Steps:   4%|▍         | 198/5000 [43:42<15:55:23, 11.94s/it, loss=0.4924, lr=3.96e-06]Steps:   4%|▍         | 199/5000 [43:54<15:57:37, 11.97s/it, loss=0.4924, lr=3.96e-06]Steps:   4%|▍         | 199/5000 [43:54<15:57:37, 11.97s/it, loss=0.5863, lr=3.98e-06]Steps:   4%|▍         | 200/5000 [44:05<15:55:30, 11.94s/it, loss=0.5863, lr=3.98e-06]Steps:   4%|▍         | 200/5000 [44:05<15:55:30, 11.94s/it, loss=1.0404, lr=4.00e-06]01/22/2026 04:08:44 - INFO - __main__ - 
[Step 200] ✅ Loss in normal range (1.0404)
01/22/2026 04:08:44 - INFO - __main__ -   Loss avg (last 100): 0.7638
01/22/2026 04:08:44 - INFO - __main__ -   Loss range: [0.3566, 1.2029]

[Step 200] Training Debug Info:
  Loss: 0.588164
  Latent shape: torch.Size([1, 32, 126, 72]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0231, std: 0.9219
  Noise mean: -0.0022, std: 1.0000
  Target mean: -0.0253, std: 1.3594
  Model pred mean: -0.0238, std: 1.1328
  Sigmas: [0.9453125]... (timesteps: [947.0])

[Step 200] Training Debug Info:
  Loss: 1.202167
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0058, std: 0.8945
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0056, std: 1.3438
  Model pred mean: 0.0088, std: 0.7734
  Sigmas: [0.212890625]... (timesteps: [213.0])

[Step 200] Training Debug Info:
  Loss: 0.517359
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0023, std: 0.9062
  Noise mean: -0.0012, std: 1.0000
  Target mean: -0.0035, std: 1.3516
  Model pred mean: -0.0024, std: 1.1484
  Sigmas: [0.65625]... (timesteps: [656.0])

[Step 200] Training Debug Info:
  Loss: 0.541766
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0537, std: 0.9219
  Noise mean: -0.0000, std: 1.0000
  Target mean: -0.0540, std: 1.3594
  Model pred mean: -0.0500, std: 1.1328
  Sigmas: [0.8828125]... (timesteps: [881.0])
Steps:   4%|▍         | 201/5000 [44:17<15:55:04, 11.94s/it, loss=1.0404, lr=4.00e-06]Steps:   4%|▍         | 201/5000 [44:17<15:55:04, 11.94s/it, loss=0.5418, lr=4.02e-06]Steps:   4%|▍         | 202/5000 [44:29<15:52:49, 11.92s/it, loss=0.5418, lr=4.02e-06]Steps:   4%|▍         | 202/5000 [44:29<15:52:49, 11.92s/it, loss=0.4413, lr=4.04e-06]Steps:   4%|▍         | 203/5000 [44:41<15:50:57, 11.89s/it, loss=0.4413, lr=4.04e-06]Steps:   4%|▍         | 203/5000 [44:41<15:50:57, 11.89s/it, loss=1.0463, lr=4.06e-06]Steps:   4%|▍         | 204/5000 [44:53<15:51:52, 11.91s/it, loss=1.0463, lr=4.06e-06]Steps:   4%|▍         | 204/5000 [44:53<15:51:52, 11.91s/it, loss=1.1652, lr=4.08e-06]Steps:   4%|▍         | 205/5000 [45:05<15:53:38, 11.93s/it, loss=1.1652, lr=4.08e-06]Steps:   4%|▍         | 205/5000 [45:05<15:53:38, 11.93s/it, loss=1.1005, lr=4.10e-06]Steps:   4%|▍         | 206/5000 [45:17<15:56:42, 11.97s/it, loss=1.1005, lr=4.10e-06]Steps:   4%|▍         | 206/5000 [45:17<15:56:42, 11.97s/it, loss=0.6471, lr=4.12e-06]Steps:   4%|▍         | 207/5000 [45:29<15:54:28, 11.95s/it, loss=0.6471, lr=4.12e-06]Steps:   4%|▍         | 207/5000 [45:29<15:54:28, 11.95s/it, loss=0.4540, lr=4.14e-06]Steps:   4%|▍         | 208/5000 [45:41<15:52:30, 11.93s/it, loss=0.4540, lr=4.14e-06]Steps:   4%|▍         | 208/5000 [45:41<15:52:30, 11.93s/it, loss=0.4131, lr=4.16e-06]Steps:   4%|▍         | 209/5000 [45:53<15:51:24, 11.91s/it, loss=0.4131, lr=4.16e-06]Steps:   4%|▍         | 209/5000 [45:53<15:51:24, 11.91s/it, loss=0.6710, lr=4.18e-06]Steps:   4%|▍         | 210/5000 [46:05<15:49:48, 11.90s/it, loss=0.6710, lr=4.18e-06]Steps:   4%|▍         | 210/5000 [46:05<15:49:48, 11.90s/it, loss=0.6891, lr=4.20e-06]
[Step 210] Training Debug Info:
  Loss: 0.343470
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0020, std: 0.8984
  Noise mean: 0.0020, std: 1.0000
  Target mean: 0.0039, std: 1.3438
  Model pred mean: 0.0017, std: 1.2031
  Sigmas: [0.859375]... (timesteps: [858.0])

[Step 210] Training Debug Info:
  Loss: 0.412773
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0002, std: 0.8984
  Noise mean: 0.0012, std: 1.0000
  Target mean: 0.0011, std: 1.3438
  Model pred mean: -0.0051, std: 1.1719
  Sigmas: [0.953125]... (timesteps: [954.0])

[Step 210] Training Debug Info:
  Loss: 0.455636
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0181, std: 0.9375
  Noise mean: -0.0023, std: 1.0000
  Target mean: -0.0205, std: 1.3750
  Model pred mean: -0.0189, std: 1.1875
  Sigmas: [0.65234375]... (timesteps: [652.0])

[Step 210] Training Debug Info:
  Loss: 0.506922
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0031, std: 0.9141
  Noise mean: -0.0001, std: 1.0000
  Target mean: -0.0033, std: 1.3516
  Model pred mean: -0.0064, std: 1.1562
  Sigmas: [0.68359375]... (timesteps: [683.0])
Steps:   4%|▍         | 211/5000 [46:16<15:47:37, 11.87s/it, loss=0.6891, lr=4.20e-06]Steps:   4%|▍         | 211/5000 [46:16<15:47:37, 11.87s/it, loss=0.5069, lr=4.22e-06]Steps:   4%|▍         | 212/5000 [46:28<15:52:57, 11.94s/it, loss=0.5069, lr=4.22e-06]Steps:   4%|▍         | 212/5000 [46:28<15:52:57, 11.94s/it, loss=1.0798, lr=4.24e-06]Steps:   4%|▍         | 213/5000 [46:40<15:51:50, 11.93s/it, loss=1.0798, lr=4.24e-06]Steps:   4%|▍         | 213/5000 [46:40<15:51:50, 11.93s/it, loss=0.9392, lr=4.26e-06]Steps:   4%|▍         | 214/5000 [46:52<15:52:02, 11.94s/it, loss=0.9392, lr=4.26e-06]Steps:   4%|▍         | 214/5000 [46:52<15:52:02, 11.94s/it, loss=1.0367, lr=4.28e-06]Steps:   4%|▍         | 215/5000 [47:04<15:47:33, 11.88s/it, loss=1.0367, lr=4.28e-06]Steps:   4%|▍         | 215/5000 [47:04<15:47:33, 11.88s/it, loss=0.5261, lr=4.30e-06]Steps:   4%|▍         | 216/5000 [47:16<15:47:36, 11.88s/it, loss=0.5261, lr=4.30e-06]Steps:   4%|▍         | 216/5000 [47:16<15:47:36, 11.88s/it, loss=1.1061, lr=4.32e-06]Steps:   4%|▍         | 217/5000 [47:28<15:46:53, 11.88s/it, loss=1.1061, lr=4.32e-06]Steps:   4%|▍         | 217/5000 [47:28<15:46:53, 11.88s/it, loss=0.6195, lr=4.34e-06]Steps:   4%|▍         | 218/5000 [47:40<15:46:18, 11.87s/it, loss=0.6195, lr=4.34e-06]Steps:   4%|▍         | 218/5000 [47:40<15:46:18, 11.87s/it, loss=0.7668, lr=4.36e-06]Steps:   4%|▍         | 219/5000 [47:52<15:51:19, 11.94s/it, loss=0.7668, lr=4.36e-06]Steps:   4%|▍         | 219/5000 [47:52<15:51:19, 11.94s/it, loss=0.6408, lr=4.38e-06]Steps:   4%|▍         | 220/5000 [48:04<15:48:23, 11.90s/it, loss=0.6408, lr=4.38e-06]Steps:   4%|▍         | 220/5000 [48:04<15:48:23, 11.90s/it, loss=1.1043, lr=4.40e-06]
[Step 220] Training Debug Info:
  Loss: 0.870370
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0186, std: 0.9375
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0184, std: 1.3672
  Model pred mean: -0.0164, std: 1.0078
  Sigmas: [0.421875]... (timesteps: [421.0])

[Step 220] Training Debug Info:
  Loss: 1.144154
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0073, std: 0.9062
  Noise mean: -0.0007, std: 1.0000
  Target mean: -0.0081, std: 1.3516
  Model pred mean: -0.0068, std: 0.8242
  Sigmas: [0.185546875]... (timesteps: [186.0])

[Step 220] Training Debug Info:
  Loss: 1.143144
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0066, std: 0.8828
  Noise mean: 0.0039, std: 1.0000
  Target mean: 0.0106, std: 1.3359
  Model pred mean: 0.0079, std: 0.8047
  Sigmas: [0.1865234375]... (timesteps: [187.0])

[Step 220] Training Debug Info:
  Loss: 1.061707
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0530, std: 0.9648
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0515, std: 1.3906
  Model pred mean: -0.0474, std: 0.9375
  Sigmas: [0.037109375]... (timesteps: [37.0])
Steps:   4%|▍         | 221/5000 [48:16<15:48:30, 11.91s/it, loss=1.1043, lr=4.40e-06]Steps:   4%|▍         | 221/5000 [48:16<15:48:30, 11.91s/it, loss=1.0617, lr=4.42e-06]Steps:   4%|▍         | 222/5000 [48:27<15:47:43, 11.90s/it, loss=1.0617, lr=4.42e-06]Steps:   4%|▍         | 222/5000 [48:27<15:47:43, 11.90s/it, loss=0.9017, lr=4.44e-06]Steps:   4%|▍         | 223/5000 [48:39<15:50:22, 11.94s/it, loss=0.9017, lr=4.44e-06]Steps:   4%|▍         | 223/5000 [48:39<15:50:22, 11.94s/it, loss=0.7036, lr=4.46e-06]Steps:   4%|▍         | 224/5000 [48:51<15:49:03, 11.92s/it, loss=0.7036, lr=4.46e-06]Steps:   4%|▍         | 224/5000 [48:51<15:49:03, 11.92s/it, loss=0.4682, lr=4.48e-06]Steps:   4%|▍         | 225/5000 [49:03<15:47:30, 11.91s/it, loss=0.4682, lr=4.48e-06]Steps:   4%|▍         | 225/5000 [49:03<15:47:30, 11.91s/it, loss=1.0940, lr=4.50e-06]Steps:   5%|▍         | 226/5000 [49:15<15:51:54, 11.96s/it, loss=1.0940, lr=4.50e-06]Steps:   5%|▍         | 226/5000 [49:15<15:51:54, 11.96s/it, loss=1.0547, lr=4.52e-06]Steps:   5%|▍         | 227/5000 [49:27<15:50:03, 11.94s/it, loss=1.0547, lr=4.52e-06]Steps:   5%|▍         | 227/5000 [49:27<15:50:03, 11.94s/it, loss=0.3846, lr=4.54e-06]Steps:   5%|▍         | 228/5000 [49:39<15:48:19, 11.92s/it, loss=0.3846, lr=4.54e-06]Steps:   5%|▍         | 228/5000 [49:39<15:48:19, 11.92s/it, loss=1.0855, lr=4.56e-06]Steps:   5%|▍         | 229/5000 [49:51<15:47:30, 11.92s/it, loss=1.0855, lr=4.56e-06]Steps:   5%|▍         | 229/5000 [49:51<15:47:30, 11.92s/it, loss=0.8606, lr=4.58e-06]Steps:   5%|▍         | 230/5000 [50:03<15:45:25, 11.89s/it, loss=0.8606, lr=4.58e-06]Steps:   5%|▍         | 230/5000 [50:03<15:45:25, 11.89s/it, loss=1.0174, lr=4.60e-06]
[Step 230] Training Debug Info:
  Loss: 0.353787
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0195, std: 0.9453
  Noise mean: -0.0007, std: 1.0000
  Target mean: 0.0188, std: 1.3750
  Model pred mean: 0.0216, std: 1.2422
  Sigmas: [0.8046875]... (timesteps: [803.0])

[Step 230] Training Debug Info:
  Loss: 0.637301
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0396, std: 0.9531
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0415, std: 1.3750
  Model pred mean: -0.0371, std: 1.1328
  Sigmas: [0.482421875]... (timesteps: [482.0])

[Step 230] Training Debug Info:
  Loss: 1.040674
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0065, std: 0.9531
  Noise mean: -0.0010, std: 1.0000
  Target mean: -0.0074, std: 1.3828
  Model pred mean: -0.0035, std: 0.9297
  Sigmas: [0.01300048828125]... (timesteps: [13.0])

[Step 230] Training Debug Info:
  Loss: 0.529187
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: -0.0004, std: 0.8750
  Noise mean: 0.0002, std: 1.0000
  Target mean: 0.0006, std: 1.3281
  Model pred mean: -0.0005, std: 1.1094
  Sigmas: [0.65234375]... (timesteps: [654.0])
Steps:   5%|▍         | 231/5000 [50:15<15:43:36, 11.87s/it, loss=1.0174, lr=4.60e-06]Steps:   5%|▍         | 231/5000 [50:15<15:43:36, 11.87s/it, loss=0.5292, lr=4.62e-06]Steps:   5%|▍         | 232/5000 [50:27<15:44:22, 11.88s/it, loss=0.5292, lr=4.62e-06]Steps:   5%|▍         | 232/5000 [50:27<15:44:22, 11.88s/it, loss=0.5861, lr=4.64e-06]Steps:   5%|▍         | 233/5000 [50:39<15:48:33, 11.94s/it, loss=0.5861, lr=4.64e-06]Steps:   5%|▍         | 233/5000 [50:39<15:48:33, 11.94s/it, loss=0.3728, lr=4.66e-06]Steps:   5%|▍         | 234/5000 [50:51<15:48:02, 11.94s/it, loss=0.3728, lr=4.66e-06]Steps:   5%|▍         | 234/5000 [50:51<15:48:02, 11.94s/it, loss=1.0188, lr=4.68e-06]Steps:   5%|▍         | 235/5000 [51:02<15:44:18, 11.89s/it, loss=1.0188, lr=4.68e-06]Steps:   5%|▍         | 235/5000 [51:02<15:44:18, 11.89s/it, loss=0.5458, lr=4.70e-06]Steps:   5%|▍         | 236/5000 [51:14<15:44:30, 11.90s/it, loss=0.5458, lr=4.70e-06]Steps:   5%|▍         | 236/5000 [51:14<15:44:30, 11.90s/it, loss=0.9002, lr=4.72e-06]Steps:   5%|▍         | 237/5000 [51:26<15:44:35, 11.90s/it, loss=0.9002, lr=4.72e-06]Steps:   5%|▍         | 237/5000 [51:26<15:44:35, 11.90s/it, loss=1.0845, lr=4.74e-06]Steps:   5%|▍         | 238/5000 [51:38<15:44:48, 11.90s/it, loss=1.0845, lr=4.74e-06]Steps:   5%|▍         | 238/5000 [51:38<15:44:48, 11.90s/it, loss=1.2005, lr=4.76e-06]Steps:   5%|▍         | 239/5000 [51:50<15:47:33, 11.94s/it, loss=1.2005, lr=4.76e-06]Steps:   5%|▍         | 239/5000 [51:50<15:47:33, 11.94s/it, loss=0.4785, lr=4.78e-06]Steps:   5%|▍         | 240/5000 [52:02<15:44:30, 11.91s/it, loss=0.4785, lr=4.78e-06]Steps:   5%|▍         | 240/5000 [52:02<15:44:30, 11.91s/it, loss=0.6619, lr=4.80e-06]
[Step 240] Training Debug Info:
  Loss: 0.448478
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0082, std: 0.9219
  Noise mean: -0.0024, std: 1.0000
  Target mean: 0.0059, std: 1.3594
  Model pred mean: 0.0079, std: 1.1797
  Sigmas: [0.890625]... (timesteps: [889.0])

[Step 240] Training Debug Info:
  Loss: 0.532220
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0148, std: 0.9453
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0132, std: 1.3750
  Model pred mean: -0.0183, std: 1.1719
  Sigmas: [0.609375]... (timesteps: [610.0])

[Step 240] Training Debug Info:
  Loss: 0.477138
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0141, std: 0.9688
  Noise mean: 0.0030, std: 1.0000
  Target mean: -0.0111, std: 1.3906
  Model pred mean: -0.0159, std: 1.2109
  Sigmas: [0.63671875]... (timesteps: [638.0])

[Step 240] Training Debug Info:
  Loss: 1.115469
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0050, std: 0.9102
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0039, std: 1.3516
  Model pred mean: -0.0024, std: 0.8398
  Sigmas: [0.06787109375]... (timesteps: [68.0])
Steps:   5%|▍         | 241/5000 [52:14<15:44:53, 11.91s/it, loss=0.6619, lr=4.80e-06]Steps:   5%|▍         | 241/5000 [52:14<15:44:53, 11.91s/it, loss=1.1155, lr=4.82e-06]Steps:   5%|▍         | 242/5000 [52:26<15:44:54, 11.92s/it, loss=1.1155, lr=4.82e-06]Steps:   5%|▍         | 242/5000 [52:26<15:44:54, 11.92s/it, loss=0.4228, lr=4.84e-06]Steps:   5%|▍         | 243/5000 [52:38<15:46:12, 11.93s/it, loss=0.4228, lr=4.84e-06]Steps:   5%|▍         | 243/5000 [52:38<15:46:12, 11.93s/it, loss=0.4608, lr=4.86e-06]Steps:   5%|▍         | 244/5000 [52:50<15:44:45, 11.92s/it, loss=0.4608, lr=4.86e-06]Steps:   5%|▍         | 244/5000 [52:50<15:44:45, 11.92s/it, loss=1.2056, lr=4.88e-06]Steps:   5%|▍         | 245/5000 [53:01<15:42:30, 11.89s/it, loss=1.2056, lr=4.88e-06]Steps:   5%|▍         | 245/5000 [53:01<15:42:30, 11.89s/it, loss=0.4939, lr=4.90e-06]Steps:   5%|▍         | 246/5000 [53:14<15:46:26, 11.94s/it, loss=0.4939, lr=4.90e-06]Steps:   5%|▍         | 246/5000 [53:14<15:46:26, 11.94s/it, loss=0.5993, lr=4.92e-06]Steps:   5%|▍         | 247/5000 [53:25<15:45:31, 11.94s/it, loss=0.5993, lr=4.92e-06]Steps:   5%|▍         | 247/5000 [53:25<15:45:31, 11.94s/it, loss=0.6165, lr=4.94e-06]Steps:   5%|▍         | 248/5000 [53:37<15:45:47, 11.94s/it, loss=0.6165, lr=4.94e-06]Steps:   5%|▍         | 248/5000 [53:37<15:45:47, 11.94s/it, loss=0.4284, lr=4.96e-06]Steps:   5%|▍         | 249/5000 [53:49<15:44:15, 11.92s/it, loss=0.4284, lr=4.96e-06]Steps:   5%|▍         | 249/5000 [53:49<15:44:15, 11.92s/it, loss=0.7487, lr=4.98e-06]Steps:   5%|▌         | 250/5000 [54:01<15:45:55, 11.95s/it, loss=0.7487, lr=4.98e-06]Steps:   5%|▌         | 250/5000 [54:01<15:45:55, 11.95s/it, loss=0.4364, lr=5.00e-06]
[Step 250] Training Debug Info:
  Loss: 0.514557
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0090, std: 0.9609
  Noise mean: 0.0034, std: 1.0000
  Target mean: -0.0056, std: 1.3906
  Model pred mean: -0.0024, std: 1.1875
  Sigmas: [0.76171875]... (timesteps: [760.0])

[Step 250] Training Debug Info:
  Loss: 0.971694
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0454, std: 0.9023
  Noise mean: 0.0015, std: 1.0000
  Target mean: 0.0469, std: 1.3438
  Model pred mean: 0.0488, std: 0.9375
  Sigmas: [0.380859375]... (timesteps: [381.0])

[Step 250] Training Debug Info:
  Loss: 0.420412
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0054, std: 0.8906
  Noise mean: 0.0001, std: 1.0000
  Target mean: 0.0055, std: 1.3359
  Model pred mean: 0.0076, std: 1.1797
  Sigmas: [0.76171875]... (timesteps: [761.0])

[Step 250] Training Debug Info:
  Loss: 0.413941
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0062, std: 0.9258
  Noise mean: -0.0016, std: 1.0000
  Target mean: -0.0077, std: 1.3594
  Model pred mean: -0.0069, std: 1.2031
  Sigmas: [0.73046875]... (timesteps: [732.0])
Steps:   5%|▌         | 251/5000 [54:13<15:43:58, 11.93s/it, loss=0.4364, lr=5.00e-06]Steps:   5%|▌         | 251/5000 [54:13<15:43:58, 11.93s/it, loss=0.4139, lr=5.02e-06]Steps:   5%|▌         | 252/5000 [54:25<15:43:58, 11.93s/it, loss=0.4139, lr=5.02e-06]Steps:   5%|▌         | 252/5000 [54:25<15:43:58, 11.93s/it, loss=0.5348, lr=5.04e-06]Steps:   5%|▌         | 253/5000 [54:37<15:50:46, 12.02s/it, loss=0.5348, lr=5.04e-06]Steps:   5%|▌         | 253/5000 [54:37<15:50:46, 12.02s/it, loss=0.6457, lr=5.06e-06]Steps:   5%|▌         | 254/5000 [54:49<15:46:28, 11.97s/it, loss=0.6457, lr=5.06e-06]Steps:   5%|▌         | 254/5000 [54:49<15:46:28, 11.97s/it, loss=1.0353, lr=5.08e-06]Steps:   5%|▌         | 255/5000 [55:01<15:46:20, 11.97s/it, loss=1.0353, lr=5.08e-06]Steps:   5%|▌         | 255/5000 [55:01<15:46:20, 11.97s/it, loss=0.4755, lr=5.10e-06]Steps:   5%|▌         | 256/5000 [55:13<15:44:27, 11.95s/it, loss=0.4755, lr=5.10e-06]Steps:   5%|▌         | 256/5000 [55:13<15:44:27, 11.95s/it, loss=0.6363, lr=5.12e-06]Steps:   5%|▌         | 257/5000 [55:25<15:42:02, 11.92s/it, loss=0.6363, lr=5.12e-06]Steps:   5%|▌         | 257/5000 [55:25<15:42:02, 11.92s/it, loss=0.4236, lr=5.14e-06]Steps:   5%|▌         | 258/5000 [55:37<15:41:55, 11.92s/it, loss=0.4236, lr=5.14e-06]Steps:   5%|▌         | 258/5000 [55:37<15:41:55, 11.92s/it, loss=0.6168, lr=5.16e-06]Steps:   5%|▌         | 259/5000 [55:49<15:44:33, 11.95s/it, loss=0.6168, lr=5.16e-06]Steps:   5%|▌         | 259/5000 [55:49<15:44:33, 11.95s/it, loss=0.4004, lr=5.18e-06]Steps:   5%|▌         | 260/5000 [56:01<15:47:36, 12.00s/it, loss=0.4004, lr=5.18e-06]Steps:   5%|▌         | 260/5000 [56:01<15:47:36, 12.00s/it, loss=1.1284, lr=5.20e-06]
[Step 260] Training Debug Info:
  Loss: 0.746245
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0062, std: 0.9062
  Noise mean: 0.0016, std: 1.0000
  Target mean: 0.0078, std: 1.3516
  Model pred mean: 0.0078, std: 1.0547
  Sigmas: [0.4921875]... (timesteps: [492.0])

[Step 260] Training Debug Info:
  Loss: 0.823385
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0177, std: 0.9336
  Noise mean: 0.0016, std: 1.0000
  Target mean: -0.0161, std: 1.3672
  Model pred mean: -0.0173, std: 1.0156
  Sigmas: [0.39453125]... (timesteps: [395.0])

[Step 260] Training Debug Info:
  Loss: 0.478946
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0059, std: 0.8672
  Noise mean: 0.0008, std: 1.0078
  Target mean: -0.0051, std: 1.3281
  Model pred mean: -0.0074, std: 1.1406
  Sigmas: [0.703125]... (timesteps: [702.0])

[Step 260] Training Debug Info:
  Loss: 0.423887
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0124, std: 0.8906
  Noise mean: -0.0014, std: 1.0000
  Target mean: 0.0110, std: 1.3438
  Model pred mean: 0.0135, std: 1.1719
  Sigmas: [0.734375]... (timesteps: [734.0])
Steps:   5%|▌         | 261/5000 [56:13<15:42:58, 11.94s/it, loss=1.1284, lr=5.20e-06]Steps:   5%|▌         | 261/5000 [56:13<15:42:58, 11.94s/it, loss=0.4239, lr=5.22e-06]Steps:   5%|▌         | 262/5000 [56:25<15:39:41, 11.90s/it, loss=0.4239, lr=5.22e-06]Steps:   5%|▌         | 262/5000 [56:25<15:39:41, 11.90s/it, loss=0.4467, lr=5.24e-06]Steps:   5%|▌         | 263/5000 [56:36<15:38:13, 11.88s/it, loss=0.4467, lr=5.24e-06]Steps:   5%|▌         | 263/5000 [56:36<15:38:13, 11.88s/it, loss=0.4337, lr=5.26e-06]Steps:   5%|▌         | 264/5000 [56:48<15:39:48, 11.91s/it, loss=0.4337, lr=5.26e-06]Steps:   5%|▌         | 264/5000 [56:48<15:39:48, 11.91s/it, loss=0.4862, lr=5.28e-06]Steps:   5%|▌         | 265/5000 [57:00<15:38:30, 11.89s/it, loss=0.4862, lr=5.28e-06]Steps:   5%|▌         | 265/5000 [57:00<15:38:30, 11.89s/it, loss=1.1147, lr=5.30e-06]Steps:   5%|▌         | 266/5000 [57:12<15:42:54, 11.95s/it, loss=1.1147, lr=5.30e-06]Steps:   5%|▌         | 266/5000 [57:12<15:42:54, 11.95s/it, loss=1.1784, lr=5.32e-06]Steps:   5%|▌         | 267/5000 [57:24<15:39:16, 11.91s/it, loss=1.1784, lr=5.32e-06]Steps:   5%|▌         | 267/5000 [57:24<15:39:16, 11.91s/it, loss=1.1320, lr=5.34e-06]Steps:   5%|▌         | 268/5000 [57:36<15:40:52, 11.93s/it, loss=1.1320, lr=5.34e-06]Steps:   5%|▌         | 268/5000 [57:36<15:40:52, 11.93s/it, loss=1.0791, lr=5.36e-06]Steps:   5%|▌         | 269/5000 [57:48<15:40:17, 11.93s/it, loss=1.0791, lr=5.36e-06]Steps:   5%|▌         | 269/5000 [57:48<15:40:17, 11.93s/it, loss=0.7430, lr=5.38e-06]Steps:   5%|▌         | 270/5000 [58:00<15:39:07, 11.91s/it, loss=0.7430, lr=5.38e-06]Steps:   5%|▌         | 270/5000 [58:00<15:39:07, 11.91s/it, loss=0.5274, lr=5.40e-06]
[Step 270] Training Debug Info:
  Loss: 0.434686
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0259, std: 0.9648
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0248, std: 1.3906
  Model pred mean: -0.0237, std: 1.2188
  Sigmas: [0.8203125]... (timesteps: [820.0])

[Step 270] Training Debug Info:
  Loss: 0.439967
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0017, std: 0.8906
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0017, std: 1.3359
  Model pred mean: -0.0079, std: 1.1797
  Sigmas: [0.94921875]... (timesteps: [950.0])

[Step 270] Training Debug Info:
  Loss: 0.707892
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0381, std: 1.0000
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0378, std: 1.4141
  Model pred mean: -0.0405, std: 1.1328
  Sigmas: [0.46875]... (timesteps: [468.0])

[Step 270] Training Debug Info:
  Loss: 1.088463
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0400, std: 0.9141
  Noise mean: 0.0032, std: 1.0000
  Target mean: -0.0369, std: 1.3594
  Model pred mean: -0.0452, std: 0.8633
  Sigmas: [0.17578125]... (timesteps: [176.0])
Steps:   5%|▌         | 271/5000 [58:12<15:37:32, 11.90s/it, loss=0.5274, lr=5.40e-06]Steps:   5%|▌         | 271/5000 [58:12<15:37:32, 11.90s/it, loss=1.0885, lr=5.42e-06]Steps:   5%|▌         | 272/5000 [58:24<15:37:15, 11.89s/it, loss=1.0885, lr=5.42e-06]Steps:   5%|▌         | 272/5000 [58:24<15:37:15, 11.89s/it, loss=0.5631, lr=5.44e-06]Steps:   5%|▌         | 273/5000 [58:36<15:42:01, 11.96s/it, loss=0.5631, lr=5.44e-06]Steps:   5%|▌         | 273/5000 [58:36<15:42:01, 11.96s/it, loss=1.0477, lr=5.46e-06]Steps:   5%|▌         | 274/5000 [58:48<15:40:14, 11.94s/it, loss=1.0477, lr=5.46e-06]Steps:   5%|▌         | 274/5000 [58:48<15:40:14, 11.94s/it, loss=1.1324, lr=5.48e-06]Steps:   6%|▌         | 275/5000 [59:00<15:39:36, 11.93s/it, loss=1.1324, lr=5.48e-06]Steps:   6%|▌         | 275/5000 [59:00<15:39:36, 11.93s/it, loss=1.0596, lr=5.50e-06]Steps:   6%|▌         | 276/5000 [59:11<15:37:18, 11.90s/it, loss=1.0596, lr=5.50e-06]Steps:   6%|▌         | 276/5000 [59:11<15:37:18, 11.90s/it, loss=1.1521, lr=5.52e-06]Steps:   6%|▌         | 277/5000 [59:23<15:40:15, 11.94s/it, loss=1.1521, lr=5.52e-06]Steps:   6%|▌         | 277/5000 [59:23<15:40:15, 11.94s/it, loss=1.0120, lr=5.54e-06]Steps:   6%|▌         | 278/5000 [59:35<15:37:39, 11.91s/it, loss=1.0120, lr=5.54e-06]Steps:   6%|▌         | 278/5000 [59:35<15:37:39, 11.91s/it, loss=1.1406, lr=5.56e-06]Steps:   6%|▌         | 279/5000 [59:47<15:37:12, 11.91s/it, loss=1.1406, lr=5.56e-06]Steps:   6%|▌         | 279/5000 [59:47<15:37:12, 11.91s/it, loss=1.1195, lr=5.58e-06]Steps:   6%|▌         | 280/5000 [59:59<15:42:07, 11.98s/it, loss=1.1195, lr=5.58e-06]Steps:   6%|▌         | 280/5000 [59:59<15:42:07, 11.98s/it, loss=0.5038, lr=5.60e-06]
[Step 280] Training Debug Info:
  Loss: 0.768262
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0051, std: 0.9023
  Noise mean: 0.0013, std: 1.0000
  Target mean: 0.0064, std: 1.3516
  Model pred mean: 0.0052, std: 1.0234
  Sigmas: [0.482421875]... (timesteps: [482.0])

[Step 280] Training Debug Info:
  Loss: 0.398340
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0289, std: 0.8633
  Noise mean: -0.0013, std: 1.0000
  Target mean: 0.0276, std: 1.3203
  Model pred mean: 0.0250, std: 1.1641
  Sigmas: [0.79296875]... (timesteps: [794.0])

[Step 280] Training Debug Info:
  Loss: 1.134063
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0137, std: 0.9297
  Noise mean: 0.0025, std: 1.0000
  Target mean: -0.0112, std: 1.3672
  Model pred mean: -0.0086, std: 0.8516
  Sigmas: [0.0888671875]... (timesteps: [89.0])

[Step 280] Training Debug Info:
  Loss: 0.586118
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0087, std: 0.8945
  Noise mean: -0.0061, std: 1.0000
  Target mean: -0.0148, std: 1.3438
  Model pred mean: 0.0005, std: 1.0859
  Sigmas: [0.98046875]... (timesteps: [981.0])
Steps:   6%|▌         | 281/5000 [1:00:11<15:40:13, 11.95s/it, loss=0.5038, lr=5.60e-06]Steps:   6%|▌         | 281/5000 [1:00:11<15:40:13, 11.95s/it, loss=0.5861, lr=5.62e-06]Steps:   6%|▌         | 282/5000 [1:00:23<15:41:23, 11.97s/it, loss=0.5861, lr=5.62e-06]Steps:   6%|▌         | 282/5000 [1:00:23<15:41:23, 11.97s/it, loss=0.9266, lr=5.64e-06]Steps:   6%|▌         | 283/5000 [1:00:35<15:40:20, 11.96s/it, loss=0.9266, lr=5.64e-06]Steps:   6%|▌         | 283/5000 [1:00:35<15:40:20, 11.96s/it, loss=0.8327, lr=5.66e-06]Steps:   6%|▌         | 284/5000 [1:00:47<15:42:14, 11.99s/it, loss=0.8327, lr=5.66e-06]Steps:   6%|▌         | 284/5000 [1:00:47<15:42:14, 11.99s/it, loss=1.0780, lr=5.68e-06]Steps:   6%|▌         | 285/5000 [1:00:59<15:41:26, 11.98s/it, loss=1.0780, lr=5.68e-06]Steps:   6%|▌         | 285/5000 [1:00:59<15:41:26, 11.98s/it, loss=0.5164, lr=5.70e-06]Steps:   6%|▌         | 286/5000 [1:01:11<15:39:53, 11.96s/it, loss=0.5164, lr=5.70e-06]Steps:   6%|▌         | 286/5000 [1:01:11<15:39:53, 11.96s/it, loss=0.4433, lr=5.72e-06]Steps:   6%|▌         | 287/5000 [1:01:23<15:45:52, 12.04s/it, loss=0.4433, lr=5.72e-06]Steps:   6%|▌         | 287/5000 [1:01:23<15:45:52, 12.04s/it, loss=0.8725, lr=5.74e-06]Steps:   6%|▌         | 288/5000 [1:01:35<15:42:38, 12.00s/it, loss=0.8725, lr=5.74e-06]Steps:   6%|▌         | 288/5000 [1:01:35<15:42:38, 12.00s/it, loss=0.8978, lr=5.76e-06]Steps:   6%|▌         | 289/5000 [1:01:47<15:41:10, 11.99s/it, loss=0.8978, lr=5.76e-06]Steps:   6%|▌         | 289/5000 [1:01:47<15:41:10, 11.99s/it, loss=1.1706, lr=5.78e-06]Steps:   6%|▌         | 290/5000 [1:01:59<15:39:07, 11.96s/it, loss=1.1706, lr=5.78e-06]Steps:   6%|▌         | 290/5000 [1:01:59<15:39:07, 11.96s/it, loss=1.1863, lr=5.80e-06]
[Step 290] Training Debug Info:
  Loss: 1.131251
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0359, std: 0.9453
  Noise mean: -0.0015, std: 1.0000
  Target mean: 0.0344, std: 1.3750
  Model pred mean: 0.0393, std: 0.8516
  Sigmas: [0.11181640625]... (timesteps: [112.0])

[Step 290] Training Debug Info:
  Loss: 1.113187
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0234, std: 0.9180
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0243, std: 1.3594
  Model pred mean: 0.0233, std: 0.8594
  Sigmas: [0.07421875]... (timesteps: [74.0])

[Step 290] Training Debug Info:
  Loss: 0.385044
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0135, std: 0.8633
  Noise mean: -0.0040, std: 1.0000
  Target mean: 0.0095, std: 1.3203
  Model pred mean: 0.0043, std: 1.1562
  Sigmas: [0.890625]... (timesteps: [890.0])

[Step 290] Training Debug Info:
  Loss: 1.104484
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0122, std: 0.8984
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0139, std: 1.3438
  Model pred mean: 0.0118, std: 0.8398
  Sigmas: [0.06298828125]... (timesteps: [63.0])
Steps:   6%|▌         | 291/5000 [1:02:11<15:37:01, 11.94s/it, loss=1.1863, lr=5.80e-06]Steps:   6%|▌         | 291/5000 [1:02:11<15:37:01, 11.94s/it, loss=1.1045, lr=5.82e-06]Steps:   6%|▌         | 292/5000 [1:02:23<15:35:54, 11.93s/it, loss=1.1045, lr=5.82e-06]Steps:   6%|▌         | 292/5000 [1:02:23<15:35:54, 11.93s/it, loss=0.5337, lr=5.84e-06]Steps:   6%|▌         | 293/5000 [1:02:35<15:41:22, 12.00s/it, loss=0.5337, lr=5.84e-06]Steps:   6%|▌         | 293/5000 [1:02:35<15:41:22, 12.00s/it, loss=0.7604, lr=5.86e-06]Steps:   6%|▌         | 294/5000 [1:02:47<15:40:29, 11.99s/it, loss=0.7604, lr=5.86e-06]Steps:   6%|▌         | 294/5000 [1:02:47<15:40:29, 11.99s/it, loss=1.0910, lr=5.88e-06]Steps:   6%|▌         | 295/5000 [1:02:59<15:38:10, 11.96s/it, loss=1.0910, lr=5.88e-06]Steps:   6%|▌         | 295/5000 [1:02:59<15:38:10, 11.96s/it, loss=0.7638, lr=5.90e-06]Steps:   6%|▌         | 296/5000 [1:03:11<15:38:39, 11.97s/it, loss=0.7638, lr=5.90e-06]Steps:   6%|▌         | 296/5000 [1:03:11<15:38:39, 11.97s/it, loss=0.4803, lr=5.92e-06]Steps:   6%|▌         | 297/5000 [1:03:23<15:35:25, 11.93s/it, loss=0.4803, lr=5.92e-06]Steps:   6%|▌         | 297/5000 [1:03:23<15:35:25, 11.93s/it, loss=1.2100, lr=5.94e-06]Steps:   6%|▌         | 298/5000 [1:03:35<15:34:22, 11.92s/it, loss=1.2100, lr=5.94e-06]Steps:   6%|▌         | 298/5000 [1:03:35<15:34:22, 11.92s/it, loss=0.4932, lr=5.96e-06]Steps:   6%|▌         | 299/5000 [1:03:47<15:33:27, 11.91s/it, loss=0.4932, lr=5.96e-06]Steps:   6%|▌         | 299/5000 [1:03:47<15:33:27, 11.91s/it, loss=0.4645, lr=5.98e-06]Steps:   6%|▌         | 300/5000 [1:03:59<15:38:48, 11.98s/it, loss=0.4645, lr=5.98e-06]Steps:   6%|▌         | 300/5000 [1:03:59<15:38:48, 11.98s/it, loss=0.5290, lr=6.00e-06]01/22/2026 04:28:37 - INFO - __main__ - 
[Step 300] ✅ Loss in normal range (0.5290)
01/22/2026 04:28:37 - INFO - __main__ -   Loss avg (last 100): 0.7767
01/22/2026 04:28:37 - INFO - __main__ -   Loss range: [0.3728, 1.2100]

[Step 300] Training Debug Info:
  Loss: 0.484413
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0116, std: 0.9219
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0125, std: 1.3594
  Model pred mean: -0.0141, std: 1.1797
  Sigmas: [0.6796875]... (timesteps: [681.0])

[Step 300] Training Debug Info:
  Loss: 1.026353
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0393, std: 0.8594
  Noise mean: 0.0004, std: 1.0000
  Target mean: 0.0398, std: 1.3203
  Model pred mean: 0.0325, std: 0.8594
  Sigmas: [0.010009765625]... (timesteps: [10.0])

[Step 300] Training Debug Info:
  Loss: 0.490242
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0080, std: 0.8828
  Noise mean: 0.0013, std: 1.0000
  Target mean: -0.0067, std: 1.3359
  Model pred mean: -0.0129, std: 1.1484
  Sigmas: [0.6875]... (timesteps: [686.0])

[Step 300] Training Debug Info:
  Loss: 0.391249
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0006, std: 0.8984
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0015, std: 1.3438
  Model pred mean: 0.0015, std: 1.1953
  Sigmas: [0.859375]... (timesteps: [859.0])
Steps:   6%|▌         | 301/5000 [1:04:11<15:38:19, 11.98s/it, loss=0.5290, lr=6.00e-06]Steps:   6%|▌         | 301/5000 [1:04:11<15:38:19, 11.98s/it, loss=0.3912, lr=6.02e-06]Steps:   6%|▌         | 302/5000 [1:04:23<15:36:38, 11.96s/it, loss=0.3912, lr=6.02e-06]Steps:   6%|▌         | 302/5000 [1:04:23<15:36:38, 11.96s/it, loss=1.2023, lr=6.04e-06]Steps:   6%|▌         | 303/5000 [1:04:34<15:34:33, 11.94s/it, loss=1.2023, lr=6.04e-06]Steps:   6%|▌         | 303/5000 [1:04:34<15:34:33, 11.94s/it, loss=1.1264, lr=6.06e-06]Steps:   6%|▌         | 304/5000 [1:04:46<15:33:49, 11.93s/it, loss=1.1264, lr=6.06e-06]Steps:   6%|▌         | 304/5000 [1:04:46<15:33:49, 11.93s/it, loss=0.6827, lr=6.08e-06]Steps:   6%|▌         | 305/5000 [1:04:58<15:31:53, 11.91s/it, loss=0.6827, lr=6.08e-06]Steps:   6%|▌         | 305/5000 [1:04:58<15:31:53, 11.91s/it, loss=0.8872, lr=6.10e-06]Steps:   6%|▌         | 306/5000 [1:05:10<15:31:01, 11.90s/it, loss=0.8872, lr=6.10e-06]Steps:   6%|▌         | 306/5000 [1:05:10<15:31:01, 11.90s/it, loss=0.5020, lr=6.12e-06]Steps:   6%|▌         | 307/5000 [1:05:22<15:34:45, 11.95s/it, loss=0.5020, lr=6.12e-06]Steps:   6%|▌         | 307/5000 [1:05:22<15:34:45, 11.95s/it, loss=0.4064, lr=6.14e-06]Steps:   6%|▌         | 308/5000 [1:05:34<15:31:48, 11.92s/it, loss=0.4064, lr=6.14e-06]Steps:   6%|▌         | 308/5000 [1:05:34<15:31:48, 11.92s/it, loss=1.0284, lr=6.16e-06]Steps:   6%|▌         | 309/5000 [1:05:46<15:32:40, 11.93s/it, loss=1.0284, lr=6.16e-06]Steps:   6%|▌         | 309/5000 [1:05:46<15:32:40, 11.93s/it, loss=0.8298, lr=6.18e-06]Steps:   6%|▌         | 310/5000 [1:05:58<15:31:55, 11.92s/it, loss=0.8298, lr=6.18e-06]Steps:   6%|▌         | 310/5000 [1:05:58<15:31:55, 11.92s/it, loss=0.7409, lr=6.20e-06]
[Step 310] Training Debug Info:
  Loss: 0.893445
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0013, std: 0.9180
  Noise mean: -0.0003, std: 1.0000
  Target mean: 0.0010, std: 1.3594
  Model pred mean: -0.0045, std: 0.9844
  Sigmas: [0.40625]... (timesteps: [407.0])

[Step 310] Training Debug Info:
  Loss: 0.578322
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0217, std: 0.9609
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0209, std: 1.3906
  Model pred mean: -0.0233, std: 1.1484
  Sigmas: [0.6328125]... (timesteps: [633.0])

[Step 310] Training Debug Info:
  Loss: 1.150143
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0155, std: 0.8945
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0140, std: 1.3438
  Model pred mean: -0.0153, std: 0.8203
  Sigmas: [0.16015625]... (timesteps: [160.0])

[Step 310] Training Debug Info:
  Loss: 0.867477
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0041, std: 0.8945
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0058, std: 1.3438
  Model pred mean: 0.0039, std: 0.9648
  Sigmas: [0.4453125]... (timesteps: [446.0])
Steps:   6%|▌         | 311/5000 [1:06:10<15:30:06, 11.90s/it, loss=0.7409, lr=6.20e-06]Steps:   6%|▌         | 311/5000 [1:06:10<15:30:06, 11.90s/it, loss=0.8675, lr=6.22e-06]Steps:   6%|▌         | 312/5000 [1:06:22<15:28:28, 11.88s/it, loss=0.8675, lr=6.22e-06]Steps:   6%|▌         | 312/5000 [1:06:22<15:28:28, 11.88s/it, loss=0.7364, lr=6.24e-06]Steps:   6%|▋         | 313/5000 [1:06:33<15:27:29, 11.87s/it, loss=0.7364, lr=6.24e-06]Steps:   6%|▋         | 313/5000 [1:06:33<15:27:29, 11.87s/it, loss=0.6805, lr=6.26e-06]Steps:   6%|▋         | 314/5000 [1:06:46<15:32:57, 11.95s/it, loss=0.6805, lr=6.26e-06]Steps:   6%|▋         | 314/5000 [1:06:46<15:32:57, 11.95s/it, loss=1.1596, lr=6.28e-06]Steps:   6%|▋         | 315/5000 [1:06:57<15:31:47, 11.93s/it, loss=1.1596, lr=6.28e-06]Steps:   6%|▋         | 315/5000 [1:06:57<15:31:47, 11.93s/it, loss=1.1266, lr=6.30e-06]Steps:   6%|▋         | 316/5000 [1:07:10<15:34:55, 11.98s/it, loss=1.1266, lr=6.30e-06]Steps:   6%|▋         | 316/5000 [1:07:10<15:34:55, 11.98s/it, loss=1.1341, lr=6.32e-06]Steps:   6%|▋         | 317/5000 [1:07:21<15:34:36, 11.97s/it, loss=1.1341, lr=6.32e-06]Steps:   6%|▋         | 317/5000 [1:07:21<15:34:36, 11.97s/it, loss=0.6012, lr=6.34e-06]Steps:   6%|▋         | 318/5000 [1:07:33<15:32:02, 11.94s/it, loss=0.6012, lr=6.34e-06]Steps:   6%|▋         | 318/5000 [1:07:33<15:32:02, 11.94s/it, loss=0.9108, lr=6.36e-06]Steps:   6%|▋         | 319/5000 [1:07:45<15:31:10, 11.94s/it, loss=0.9108, lr=6.36e-06]Steps:   6%|▋         | 319/5000 [1:07:45<15:31:10, 11.94s/it, loss=1.1113, lr=6.38e-06]Steps:   6%|▋         | 320/5000 [1:07:58<15:38:12, 12.03s/it, loss=1.1113, lr=6.38e-06]Steps:   6%|▋         | 320/5000 [1:07:58<15:38:12, 12.03s/it, loss=0.5099, lr=6.40e-06]
[Step 320] Training Debug Info:
  Loss: 0.677964
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0034, std: 0.9219
  Noise mean: 0.0005, std: 1.0000
  Target mean: -0.0029, std: 1.3594
  Model pred mean: -0.0001, std: 1.0938
  Sigmas: [0.546875]... (timesteps: [548.0])

[Step 320] Training Debug Info:
  Loss: 0.403973
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0206, std: 0.8750
  Noise mean: 0.0006, std: 1.0000
  Target mean: 0.0212, std: 1.3281
  Model pred mean: 0.0231, std: 1.1719
  Sigmas: [0.77734375]... (timesteps: [779.0])

[Step 320] Training Debug Info:
  Loss: 0.525782
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0200, std: 0.9727
  Noise mean: 0.0000, std: 1.0000
  Target mean: -0.0200, std: 1.3984
  Model pred mean: -0.0103, std: 1.1953
  Sigmas: [0.87109375]... (timesteps: [870.0])

[Step 320] Training Debug Info:
  Loss: 1.142286
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0194, std: 0.9141
  Noise mean: 0.0005, std: 1.0000
  Target mean: -0.0188, std: 1.3516
  Model pred mean: -0.0161, std: 0.8281
  Sigmas: [0.1640625]... (timesteps: [164.0])
Steps:   6%|▋         | 321/5000 [1:08:09<15:34:48, 11.99s/it, loss=0.5099, lr=6.40e-06]Steps:   6%|▋         | 321/5000 [1:08:09<15:34:48, 11.99s/it, loss=1.1423, lr=6.42e-06]Steps:   6%|▋         | 322/5000 [1:08:21<15:33:44, 11.98s/it, loss=1.1423, lr=6.42e-06]Steps:   6%|▋         | 322/5000 [1:08:21<15:33:44, 11.98s/it, loss=1.0591, lr=6.44e-06]Steps:   6%|▋         | 323/5000 [1:08:33<15:35:39, 12.00s/it, loss=1.0591, lr=6.44e-06]Steps:   6%|▋         | 323/5000 [1:08:33<15:35:39, 12.00s/it, loss=0.5251, lr=6.46e-06]Steps:   6%|▋         | 324/5000 [1:08:45<15:32:20, 11.96s/it, loss=0.5251, lr=6.46e-06]Steps:   6%|▋         | 324/5000 [1:08:45<15:32:20, 11.96s/it, loss=0.3372, lr=6.48e-06]Steps:   6%|▋         | 325/5000 [1:08:57<15:30:50, 11.95s/it, loss=0.3372, lr=6.48e-06]Steps:   6%|▋         | 325/5000 [1:08:57<15:30:50, 11.95s/it, loss=0.7812, lr=6.50e-06]Steps:   7%|▋         | 326/5000 [1:09:09<15:29:31, 11.93s/it, loss=0.7812, lr=6.50e-06]Steps:   7%|▋         | 326/5000 [1:09:09<15:29:31, 11.93s/it, loss=1.1144, lr=6.52e-06]Steps:   7%|▋         | 327/5000 [1:09:21<15:33:42, 11.99s/it, loss=1.1144, lr=6.52e-06]Steps:   7%|▋         | 327/5000 [1:09:21<15:33:42, 11.99s/it, loss=0.6756, lr=6.54e-06]Steps:   7%|▋         | 328/5000 [1:09:33<15:32:41, 11.98s/it, loss=0.6756, lr=6.54e-06]Steps:   7%|▋         | 328/5000 [1:09:33<15:32:41, 11.98s/it, loss=0.5440, lr=6.56e-06]Steps:   7%|▋         | 329/5000 [1:09:45<15:28:19, 11.92s/it, loss=0.5440, lr=6.56e-06]Steps:   7%|▋         | 329/5000 [1:09:45<15:28:19, 11.92s/it, loss=0.8179, lr=6.58e-06]Steps:   7%|▋         | 330/5000 [1:09:57<15:34:59, 12.01s/it, loss=0.8179, lr=6.58e-06]Steps:   7%|▋         | 330/5000 [1:09:57<15:34:59, 12.01s/it, loss=0.7870, lr=6.60e-06]
[Step 330] Training Debug Info:
  Loss: 0.802829
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0064, std: 0.9180
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0061, std: 1.3594
  Model pred mean: -0.0035, std: 1.0234
  Sigmas: [0.453125]... (timesteps: [453.0])

[Step 330] Training Debug Info:
  Loss: 0.870899
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0178, std: 0.9688
  Noise mean: -0.0001, std: 1.0000
  Target mean: -0.0179, std: 1.3906
  Model pred mean: -0.0137, std: 1.0391
  Sigmas: [0.431640625]... (timesteps: [431.0])

[Step 330] Training Debug Info:
  Loss: 0.656129
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0271, std: 0.9375
  Noise mean: -0.0001, std: 1.0000
  Target mean: -0.0271, std: 1.3672
  Model pred mean: -0.0255, std: 1.1172
  Sigmas: [0.921875]... (timesteps: [922.0])

[Step 330] Training Debug Info:
  Loss: 0.646584
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0302, std: 0.8945
  Noise mean: -0.0027, std: 1.0000
  Target mean: -0.0327, std: 1.3438
  Model pred mean: -0.0265, std: 1.0938
  Sigmas: [0.9765625]... (timesteps: [975.0])
Steps:   7%|▋         | 331/5000 [1:10:09<15:32:15, 11.98s/it, loss=0.7870, lr=6.60e-06]Steps:   7%|▋         | 331/5000 [1:10:09<15:32:15, 11.98s/it, loss=0.6466, lr=6.62e-06]Steps:   7%|▋         | 332/5000 [1:10:21<15:29:43, 11.95s/it, loss=0.6466, lr=6.62e-06]Steps:   7%|▋         | 332/5000 [1:10:21<15:29:43, 11.95s/it, loss=1.0906, lr=6.64e-06]Steps:   7%|▋         | 333/5000 [1:10:33<15:28:44, 11.94s/it, loss=1.0906, lr=6.64e-06]Steps:   7%|▋         | 333/5000 [1:10:33<15:28:44, 11.94s/it, loss=1.1603, lr=6.66e-06]Steps:   7%|▋         | 334/5000 [1:10:45<15:31:50, 11.98s/it, loss=1.1603, lr=6.66e-06]Steps:   7%|▋         | 334/5000 [1:10:45<15:31:50, 11.98s/it, loss=0.6472, lr=6.68e-06]Steps:   7%|▋         | 335/5000 [1:10:57<15:29:38, 11.96s/it, loss=0.6472, lr=6.68e-06]Steps:   7%|▋         | 335/5000 [1:10:57<15:29:38, 11.96s/it, loss=0.4099, lr=6.70e-06]Steps:   7%|▋         | 336/5000 [1:11:09<15:28:46, 11.95s/it, loss=0.4099, lr=6.70e-06]Steps:   7%|▋         | 336/5000 [1:11:09<15:28:46, 11.95s/it, loss=0.3944, lr=6.72e-06]Steps:   7%|▋         | 337/5000 [1:11:21<15:28:32, 11.95s/it, loss=0.3944, lr=6.72e-06]Steps:   7%|▋         | 337/5000 [1:11:21<15:28:32, 11.95s/it, loss=0.6025, lr=6.74e-06]Steps:   7%|▋         | 338/5000 [1:11:33<15:27:06, 11.93s/it, loss=0.6025, lr=6.74e-06]Steps:   7%|▋         | 338/5000 [1:11:33<15:27:06, 11.93s/it, loss=0.8188, lr=6.76e-06]Steps:   7%|▋         | 339/5000 [1:11:44<15:24:35, 11.90s/it, loss=0.8188, lr=6.76e-06]Steps:   7%|▋         | 339/5000 [1:11:44<15:24:35, 11.90s/it, loss=1.1445, lr=6.78e-06]Steps:   7%|▋         | 340/5000 [1:11:56<15:24:27, 11.90s/it, loss=1.1445, lr=6.78e-06]Steps:   7%|▋         | 340/5000 [1:11:56<15:24:27, 11.90s/it, loss=0.8952, lr=6.80e-06]
[Step 340] Training Debug Info:
  Loss: 0.687035
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0088, std: 0.9258
  Noise mean: 0.0007, std: 1.0000
  Target mean: -0.0081, std: 1.3672
  Model pred mean: -0.0272, std: 1.0625
  Sigmas: [0.9765625]... (timesteps: [977.0])

[Step 340] Training Debug Info:
  Loss: 0.998054
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0004, std: 0.8672
  Noise mean: -0.0023, std: 1.0000
  Target mean: -0.0027, std: 1.3203
  Model pred mean: -0.0039, std: 0.8672
  Sigmas: [0.369140625]... (timesteps: [369.0])

[Step 340] Training Debug Info:
  Loss: 1.149628
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0072, std: 0.8945
  Noise mean: 0.0033, std: 1.0000
  Target mean: -0.0039, std: 1.3438
  Model pred mean: -0.0116, std: 0.8125
  Sigmas: [0.25390625]... (timesteps: [252.99998474121094])

[Step 340] Training Debug Info:
  Loss: 0.572894
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0286, std: 0.9141
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0276, std: 1.3594
  Model pred mean: -0.0417, std: 1.1250
  Sigmas: [0.9375]... (timesteps: [939.0])
Steps:   7%|▋         | 341/5000 [1:12:09<15:30:01, 11.98s/it, loss=0.8952, lr=6.80e-06]Steps:   7%|▋         | 341/5000 [1:12:09<15:30:01, 11.98s/it, loss=0.5729, lr=6.82e-06]Steps:   7%|▋         | 342/5000 [1:12:20<15:28:36, 11.96s/it, loss=0.5729, lr=6.82e-06]Steps:   7%|▋         | 342/5000 [1:12:20<15:28:36, 11.96s/it, loss=1.0992, lr=6.84e-06]Steps:   7%|▋         | 343/5000 [1:12:32<15:27:50, 11.95s/it, loss=1.0992, lr=6.84e-06]Steps:   7%|▋         | 343/5000 [1:12:32<15:27:50, 11.95s/it, loss=0.7712, lr=6.86e-06]Steps:   7%|▋         | 344/5000 [1:12:44<15:27:18, 11.95s/it, loss=0.7712, lr=6.86e-06]Steps:   7%|▋         | 344/5000 [1:12:44<15:27:18, 11.95s/it, loss=0.7131, lr=6.88e-06]Steps:   7%|▋         | 345/5000 [1:12:56<15:24:30, 11.92s/it, loss=0.7131, lr=6.88e-06]Steps:   7%|▋         | 345/5000 [1:12:56<15:24:30, 11.92s/it, loss=0.5189, lr=6.90e-06]Steps:   7%|▋         | 346/5000 [1:13:08<15:24:15, 11.92s/it, loss=0.5189, lr=6.90e-06]Steps:   7%|▋         | 346/5000 [1:13:08<15:24:15, 11.92s/it, loss=0.7083, lr=6.92e-06]Steps:   7%|▋         | 347/5000 [1:13:20<15:27:31, 11.96s/it, loss=0.7083, lr=6.92e-06]Steps:   7%|▋         | 347/5000 [1:13:20<15:27:31, 11.96s/it, loss=0.7339, lr=6.94e-06]Steps:   7%|▋         | 348/5000 [1:13:32<15:25:18, 11.93s/it, loss=0.7339, lr=6.94e-06]Steps:   7%|▋         | 348/5000 [1:13:32<15:25:18, 11.93s/it, loss=0.5015, lr=6.96e-06]Steps:   7%|▋         | 349/5000 [1:13:44<15:23:10, 11.91s/it, loss=0.5015, lr=6.96e-06]Steps:   7%|▋         | 349/5000 [1:13:44<15:23:10, 11.91s/it, loss=0.8136, lr=6.98e-06]Steps:   7%|▋         | 350/5000 [1:13:56<15:27:22, 11.97s/it, loss=0.8136, lr=6.98e-06]Steps:   7%|▋         | 350/5000 [1:13:56<15:27:22, 11.97s/it, loss=0.6300, lr=7.00e-06]
[Step 350] Training Debug Info:
  Loss: 0.440189
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0203, std: 0.9062
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0200, std: 1.3516
  Model pred mean: 0.0226, std: 1.1797
  Sigmas: [0.734375]... (timesteps: [734.0])

[Step 350] Training Debug Info:
  Loss: 0.768605
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0060, std: 0.8945
  Noise mean: -0.0041, std: 1.0000
  Target mean: 0.0019, std: 1.3438
  Model pred mean: 0.0126, std: 1.0000
  Sigmas: [0.4765625]... (timesteps: [477.0])

[Step 350] Training Debug Info:
  Loss: 0.376641
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0087, std: 0.8945
  Noise mean: -0.0014, std: 1.0000
  Target mean: 0.0073, std: 1.3438
  Model pred mean: 0.0070, std: 1.1953
  Sigmas: [0.828125]... (timesteps: [830.0])

[Step 350] Training Debug Info:
  Loss: 0.505921
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0089, std: 0.9375
  Noise mean: 0.0007, std: 1.0000
  Target mean: -0.0082, std: 1.3672
  Model pred mean: -0.0064, std: 1.1797
  Sigmas: [0.70703125]... (timesteps: [708.0])
Steps:   7%|▋         | 351/5000 [1:14:08<15:27:23, 11.97s/it, loss=0.6300, lr=7.00e-06]Steps:   7%|▋         | 351/5000 [1:14:08<15:27:23, 11.97s/it, loss=0.5059, lr=7.02e-06]Steps:   7%|▋         | 352/5000 [1:14:20<15:25:56, 11.95s/it, loss=0.5059, lr=7.02e-06]Steps:   7%|▋         | 352/5000 [1:14:20<15:25:56, 11.95s/it, loss=0.7647, lr=7.04e-06]Steps:   7%|▋         | 353/5000 [1:14:32<15:25:05, 11.94s/it, loss=0.7647, lr=7.04e-06]Steps:   7%|▋         | 353/5000 [1:14:32<15:25:05, 11.94s/it, loss=0.3927, lr=7.06e-06]Steps:   7%|▋         | 354/5000 [1:14:44<15:29:14, 12.00s/it, loss=0.3927, lr=7.06e-06]Steps:   7%|▋         | 354/5000 [1:14:44<15:29:14, 12.00s/it, loss=0.6919, lr=7.08e-06]Steps:   7%|▋         | 355/5000 [1:14:56<15:26:39, 11.97s/it, loss=0.6919, lr=7.08e-06]Steps:   7%|▋         | 355/5000 [1:14:56<15:26:39, 11.97s/it, loss=1.1473, lr=7.10e-06]Steps:   7%|▋         | 356/5000 [1:15:08<15:24:49, 11.95s/it, loss=1.1473, lr=7.10e-06]Steps:   7%|▋         | 356/5000 [1:15:08<15:24:49, 11.95s/it, loss=0.3266, lr=7.12e-06]Steps:   7%|▋         | 357/5000 [1:15:20<15:28:57, 12.00s/it, loss=0.3266, lr=7.12e-06]Steps:   7%|▋         | 357/5000 [1:15:20<15:28:57, 12.00s/it, loss=0.4317, lr=7.14e-06]Steps:   7%|▋         | 358/5000 [1:15:32<15:26:10, 11.97s/it, loss=0.4317, lr=7.14e-06]Steps:   7%|▋         | 358/5000 [1:15:32<15:26:10, 11.97s/it, loss=1.0638, lr=7.16e-06]Steps:   7%|▋         | 359/5000 [1:15:44<15:23:28, 11.94s/it, loss=1.0638, lr=7.16e-06]Steps:   7%|▋         | 359/5000 [1:15:44<15:23:28, 11.94s/it, loss=1.1955, lr=7.18e-06]Steps:   7%|▋         | 360/5000 [1:15:56<15:22:47, 11.93s/it, loss=1.1955, lr=7.18e-06]Steps:   7%|▋         | 360/5000 [1:15:56<15:22:47, 11.93s/it, loss=0.7193, lr=7.20e-06]
[Step 360] Training Debug Info:
  Loss: 0.379066
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0615, std: 0.9609
  Noise mean: 0.0025, std: 1.0000
  Target mean: -0.0588, std: 1.3828
  Model pred mean: -0.0625, std: 1.2344
  Sigmas: [0.60546875]... (timesteps: [605.0])

[Step 360] Training Debug Info:
  Loss: 0.394907
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0227, std: 0.8711
  Noise mean: 0.0008, std: 1.0000
  Target mean: 0.0234, std: 1.3281
  Model pred mean: 0.0165, std: 1.1641
  Sigmas: [0.91796875]... (timesteps: [918.0])

[Step 360] Training Debug Info:
  Loss: 0.676210
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0170, std: 0.9062
  Noise mean: 0.0007, std: 1.0000
  Target mean: -0.0162, std: 1.3516
  Model pred mean: 0.0119, std: 1.0859
  Sigmas: [0.9765625]... (timesteps: [977.0])

[Step 360] Training Debug Info:
  Loss: 0.408399
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0058, std: 0.9180
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0063, std: 1.3594
  Model pred mean: -0.0070, std: 1.1875
  Sigmas: [0.828125]... (timesteps: [829.0])
Steps:   7%|▋         | 361/5000 [1:16:08<15:28:35, 12.01s/it, loss=0.7193, lr=7.20e-06]Steps:   7%|▋         | 361/5000 [1:16:08<15:28:35, 12.01s/it, loss=0.4084, lr=7.22e-06]Steps:   7%|▋         | 362/5000 [1:16:20<15:25:42, 11.98s/it, loss=0.4084, lr=7.22e-06]Steps:   7%|▋         | 362/5000 [1:16:20<15:25:42, 11.98s/it, loss=0.4271, lr=7.24e-06]Steps:   7%|▋         | 363/5000 [1:16:32<15:23:52, 11.95s/it, loss=0.4271, lr=7.24e-06]Steps:   7%|▋         | 363/5000 [1:16:32<15:23:52, 11.95s/it, loss=1.1038, lr=7.26e-06]Steps:   7%|▋         | 364/5000 [1:16:43<15:21:38, 11.93s/it, loss=1.1038, lr=7.26e-06]Steps:   7%|▋         | 364/5000 [1:16:43<15:21:38, 11.93s/it, loss=1.0734, lr=7.28e-06]Steps:   7%|▋         | 365/5000 [1:16:55<15:20:51, 11.92s/it, loss=1.0734, lr=7.28e-06]Steps:   7%|▋         | 365/5000 [1:16:55<15:20:51, 11.92s/it, loss=1.0642, lr=7.30e-06]Steps:   7%|▋         | 366/5000 [1:17:07<15:19:17, 11.90s/it, loss=1.0642, lr=7.30e-06]Steps:   7%|▋         | 366/5000 [1:17:07<15:19:17, 11.90s/it, loss=0.4229, lr=7.32e-06]Steps:   7%|▋         | 367/5000 [1:17:19<15:17:28, 11.88s/it, loss=0.4229, lr=7.32e-06]Steps:   7%|▋         | 367/5000 [1:17:19<15:17:28, 11.88s/it, loss=1.0897, lr=7.34e-06]Steps:   7%|▋         | 368/5000 [1:17:31<15:23:27, 11.96s/it, loss=1.0897, lr=7.34e-06]Steps:   7%|▋         | 368/5000 [1:17:31<15:23:27, 11.96s/it, loss=1.1860, lr=7.36e-06]Steps:   7%|▋         | 369/5000 [1:17:43<15:23:17, 11.96s/it, loss=1.1860, lr=7.36e-06]Steps:   7%|▋         | 369/5000 [1:17:43<15:23:17, 11.96s/it, loss=0.9363, lr=7.38e-06]Steps:   7%|▋         | 370/5000 [1:17:55<15:22:49, 11.96s/it, loss=0.9363, lr=7.38e-06]Steps:   7%|▋         | 370/5000 [1:17:55<15:22:49, 11.96s/it, loss=1.0163, lr=7.40e-06]
[Step 370] Training Debug Info:
  Loss: 0.391206
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0125, std: 0.9062
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0123, std: 1.3516
  Model pred mean: -0.0101, std: 1.1953
  Sigmas: [0.84765625]... (timesteps: [848.0])

[Step 370] Training Debug Info:
  Loss: 0.337927
  Latent shape: torch.Size([1, 32, 84, 102]), Packed shape: torch.Size([1, 2142, 128])
  Latent mean: -0.0623, std: 0.9219
  Noise mean: 0.0010, std: 1.0000
  Target mean: 0.0630, std: 1.3594
  Model pred mean: 0.0625, std: 1.2266
  Sigmas: [0.81640625]... (timesteps: [817.0])

[Step 370] Training Debug Info:
  Loss: 0.548660
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0066, std: 0.9336
  Noise mean: -0.0015, std: 1.0000
  Target mean: -0.0081, std: 1.3672
  Model pred mean: -0.0089, std: 1.1562
  Sigmas: [0.61328125]... (timesteps: [612.0])

[Step 370] Training Debug Info:
  Loss: 0.491564
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0125, std: 0.8945
  Noise mean: 0.0004, std: 1.0000
  Target mean: 0.0130, std: 1.3438
  Model pred mean: 0.0094, std: 1.1562
  Sigmas: [0.68359375]... (timesteps: [682.0])
Steps:   7%|▋         | 371/5000 [1:18:07<15:23:41, 11.97s/it, loss=1.0163, lr=7.40e-06]Steps:   7%|▋         | 371/5000 [1:18:07<15:23:41, 11.97s/it, loss=0.4916, lr=7.42e-06]Steps:   7%|▋         | 372/5000 [1:18:19<15:22:40, 11.96s/it, loss=0.4916, lr=7.42e-06]Steps:   7%|▋         | 372/5000 [1:18:19<15:22:40, 11.96s/it, loss=1.0538, lr=7.44e-06]Steps:   7%|▋         | 373/5000 [1:18:31<15:23:18, 11.97s/it, loss=1.0538, lr=7.44e-06]Steps:   7%|▋         | 373/5000 [1:18:31<15:23:18, 11.97s/it, loss=0.5001, lr=7.46e-06]Steps:   7%|▋         | 374/5000 [1:18:43<15:26:25, 12.02s/it, loss=0.5001, lr=7.46e-06]Steps:   7%|▋         | 374/5000 [1:18:43<15:26:25, 12.02s/it, loss=0.9703, lr=7.48e-06]Steps:   8%|▊         | 375/5000 [1:18:55<15:24:29, 11.99s/it, loss=0.9703, lr=7.48e-06]Steps:   8%|▊         | 375/5000 [1:18:55<15:24:29, 11.99s/it, loss=0.6671, lr=7.50e-06]Steps:   8%|▊         | 376/5000 [1:19:07<15:23:34, 11.98s/it, loss=0.6671, lr=7.50e-06]Steps:   8%|▊         | 376/5000 [1:19:07<15:23:34, 11.98s/it, loss=0.6858, lr=7.52e-06]Steps:   8%|▊         | 377/5000 [1:19:19<15:22:13, 11.97s/it, loss=0.6858, lr=7.52e-06]Steps:   8%|▊         | 377/5000 [1:19:19<15:22:13, 11.97s/it, loss=0.9988, lr=7.54e-06]Steps:   8%|▊         | 378/5000 [1:19:31<15:21:43, 11.97s/it, loss=0.9988, lr=7.54e-06]Steps:   8%|▊         | 378/5000 [1:19:31<15:21:43, 11.97s/it, loss=1.0809, lr=7.56e-06]Steps:   8%|▊         | 379/5000 [1:19:43<15:19:19, 11.94s/it, loss=1.0809, lr=7.56e-06]Steps:   8%|▊         | 379/5000 [1:19:43<15:19:19, 11.94s/it, loss=1.0625, lr=7.58e-06]Steps:   8%|▊         | 380/5000 [1:19:55<15:16:53, 11.91s/it, loss=1.0625, lr=7.58e-06]Steps:   8%|▊         | 380/5000 [1:19:55<15:16:53, 11.91s/it, loss=0.9321, lr=7.60e-06]
[Step 380] Training Debug Info:
  Loss: 1.001287
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0552, std: 0.9492
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0552, std: 1.3750
  Model pred mean: -0.0581, std: 0.9531
  Sigmas: [0.2001953125]... (timesteps: [200.0])

[Step 380] Training Debug Info:
  Loss: 1.035320
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0136, std: 0.9531
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0138, std: 1.3828
  Model pred mean: -0.0031, std: 0.9375
  Sigmas: [0.01397705078125]... (timesteps: [14.0])

[Step 380] Training Debug Info:
  Loss: 0.916889
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0164, std: 0.9883
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0145, std: 1.4062
  Model pred mean: -0.0171, std: 1.0547
  Sigmas: [0.38671875]... (timesteps: [387.0])

[Step 380] Training Debug Info:
  Loss: 0.458001
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0045, std: 0.9180
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0054, std: 1.3594
  Model pred mean: -0.0068, std: 1.1797
  Sigmas: [0.6875]... (timesteps: [686.0])
Steps:   8%|▊         | 381/5000 [1:20:07<15:23:38, 12.00s/it, loss=0.9321, lr=7.60e-06]Steps:   8%|▊         | 381/5000 [1:20:07<15:23:38, 12.00s/it, loss=0.4580, lr=7.62e-06]Steps:   8%|▊         | 382/5000 [1:20:19<15:23:59, 12.01s/it, loss=0.4580, lr=7.62e-06]Steps:   8%|▊         | 382/5000 [1:20:19<15:23:59, 12.01s/it, loss=0.5747, lr=7.64e-06]Steps:   8%|▊         | 383/5000 [1:20:31<15:19:21, 11.95s/it, loss=0.5747, lr=7.64e-06]Steps:   8%|▊         | 383/5000 [1:20:31<15:19:21, 11.95s/it, loss=0.6333, lr=7.66e-06]Steps:   8%|▊         | 384/5000 [1:20:43<15:18:12, 11.94s/it, loss=0.6333, lr=7.66e-06]Steps:   8%|▊         | 384/5000 [1:20:43<15:18:12, 11.94s/it, loss=0.5199, lr=7.68e-06]Steps:   8%|▊         | 385/5000 [1:20:54<15:17:47, 11.93s/it, loss=0.5199, lr=7.68e-06]Steps:   8%|▊         | 385/5000 [1:20:54<15:17:47, 11.93s/it, loss=1.0463, lr=7.70e-06]Steps:   8%|▊         | 386/5000 [1:21:06<15:14:25, 11.89s/it, loss=1.0463, lr=7.70e-06]Steps:   8%|▊         | 386/5000 [1:21:06<15:14:25, 11.89s/it, loss=0.8932, lr=7.72e-06]Steps:   8%|▊         | 387/5000 [1:21:18<15:13:09, 11.88s/it, loss=0.8932, lr=7.72e-06]Steps:   8%|▊         | 387/5000 [1:21:18<15:13:09, 11.88s/it, loss=0.4050, lr=7.74e-06]Steps:   8%|▊         | 388/5000 [1:21:30<15:17:13, 11.93s/it, loss=0.4050, lr=7.74e-06]Steps:   8%|▊         | 388/5000 [1:21:30<15:17:13, 11.93s/it, loss=1.1234, lr=7.76e-06]Steps:   8%|▊         | 389/5000 [1:21:42<15:18:10, 11.95s/it, loss=1.1234, lr=7.76e-06]Steps:   8%|▊         | 389/5000 [1:21:42<15:18:10, 11.95s/it, loss=0.3450, lr=7.78e-06]Steps:   8%|▊         | 390/5000 [1:21:54<15:17:55, 11.95s/it, loss=0.3450, lr=7.78e-06]Steps:   8%|▊         | 390/5000 [1:21:54<15:17:55, 11.95s/it, loss=1.0562, lr=7.80e-06]
[Step 390] Training Debug Info:
  Loss: 1.097953
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0238, std: 0.9297
  Noise mean: 0.0014, std: 1.0000
  Target mean: -0.0225, std: 1.3672
  Model pred mean: -0.0229, std: 0.8867
  Sigmas: [0.06298828125]... (timesteps: [63.0])

[Step 390] Training Debug Info:
  Loss: 0.461022
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0238, std: 0.9180
  Noise mean: 0.0006, std: 1.0000
  Target mean: -0.0232, std: 1.3594
  Model pred mean: -0.0172, std: 1.1797
  Sigmas: [0.66015625]... (timesteps: [659.0])

[Step 390] Training Debug Info:
  Loss: 0.719010
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0022, std: 0.9062
  Noise mean: 0.0045, std: 1.0000
  Target mean: 0.0067, std: 1.3516
  Model pred mean: 0.0022, std: 1.0469
  Sigmas: [0.5390625]... (timesteps: [540.0])

[Step 390] Training Debug Info:
  Loss: 0.464163
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0245, std: 0.9727
  Noise mean: 0.0017, std: 1.0000
  Target mean: -0.0228, std: 1.3984
  Model pred mean: -0.0226, std: 1.2109
  Sigmas: [0.6484375]... (timesteps: [649.0])
Steps:   8%|▊         | 391/5000 [1:22:06<15:18:28, 11.96s/it, loss=1.0562, lr=7.80e-06]Steps:   8%|▊         | 391/5000 [1:22:06<15:18:28, 11.96s/it, loss=0.4642, lr=7.82e-06]Steps:   8%|▊         | 392/5000 [1:22:18<15:15:34, 11.92s/it, loss=0.4642, lr=7.82e-06]Steps:   8%|▊         | 392/5000 [1:22:18<15:15:34, 11.92s/it, loss=0.7611, lr=7.84e-06]Steps:   8%|▊         | 393/5000 [1:22:30<15:14:40, 11.91s/it, loss=0.7611, lr=7.84e-06]Steps:   8%|▊         | 393/5000 [1:22:30<15:14:40, 11.91s/it, loss=0.4054, lr=7.86e-06]Steps:   8%|▊         | 394/5000 [1:22:42<15:13:45, 11.90s/it, loss=0.4054, lr=7.86e-06]Steps:   8%|▊         | 394/5000 [1:22:42<15:13:45, 11.90s/it, loss=1.0858, lr=7.88e-06]Steps:   8%|▊         | 395/5000 [1:22:54<15:19:16, 11.98s/it, loss=1.0858, lr=7.88e-06]Steps:   8%|▊         | 395/5000 [1:22:54<15:19:16, 11.98s/it, loss=0.6681, lr=7.90e-06]Steps:   8%|▊         | 396/5000 [1:23:06<15:17:34, 11.96s/it, loss=0.6681, lr=7.90e-06]Steps:   8%|▊         | 396/5000 [1:23:06<15:17:34, 11.96s/it, loss=1.1663, lr=7.92e-06]Steps:   8%|▊         | 397/5000 [1:23:18<15:18:17, 11.97s/it, loss=1.1663, lr=7.92e-06]Steps:   8%|▊         | 397/5000 [1:23:18<15:18:17, 11.97s/it, loss=1.1172, lr=7.94e-06]Steps:   8%|▊         | 398/5000 [1:23:30<15:15:08, 11.93s/it, loss=1.1172, lr=7.94e-06]Steps:   8%|▊         | 398/5000 [1:23:30<15:15:08, 11.93s/it, loss=0.3954, lr=7.96e-06]Steps:   8%|▊         | 399/5000 [1:23:42<15:16:05, 11.95s/it, loss=0.3954, lr=7.96e-06]Steps:   8%|▊         | 399/5000 [1:23:42<15:16:05, 11.95s/it, loss=0.4013, lr=7.98e-06]Steps:   8%|▊         | 400/5000 [1:23:54<15:16:11, 11.95s/it, loss=0.4013, lr=7.98e-06]Steps:   8%|▊         | 400/5000 [1:23:54<15:16:11, 11.95s/it, loss=1.0889, lr=8.00e-06]01/22/2026 04:48:32 - INFO - __main__ - 
[Step 400] ✅ Loss in normal range (1.0889)
01/22/2026 04:48:32 - INFO - __main__ -   Loss avg (last 100): 0.7831
01/22/2026 04:48:32 - INFO - __main__ -   Loss range: [0.3266, 1.2023]

[Step 400] Training Debug Info:
  Loss: 0.708560
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0188, std: 0.9180
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0187, std: 1.3594
  Model pred mean: -0.0175, std: 1.1016
  Sigmas: [0.94921875]... (timesteps: [951.0])

[Step 400] Training Debug Info:
  Loss: 0.685293
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0093, std: 0.8555
  Noise mean: -0.0022, std: 1.0000
  Target mean: 0.0071, std: 1.3203
  Model pred mean: 0.0037, std: 1.0156
  Sigmas: [0.5625]... (timesteps: [561.0])

[Step 400] Training Debug Info:
  Loss: 0.818131
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0021, std: 0.8711
  Noise mean: 0.0025, std: 1.0000
  Target mean: 0.0004, std: 1.3281
  Model pred mean: -0.0071, std: 0.9531
  Sigmas: [0.470703125]... (timesteps: [471.0])

[Step 400] Training Debug Info:
  Loss: 1.130970
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0197, std: 0.9023
  Noise mean: -0.0019, std: 0.9961
  Target mean: -0.0215, std: 1.3438
  Model pred mean: -0.0216, std: 0.8164
  Sigmas: [0.14453125]... (timesteps: [145.0])
Steps:   8%|▊         | 401/5000 [1:24:06<15:21:10, 12.02s/it, loss=1.0889, lr=8.00e-06]Steps:   8%|▊         | 401/5000 [1:24:06<15:21:10, 12.02s/it, loss=1.1310, lr=8.02e-06]Steps:   8%|▊         | 402/5000 [1:24:18<15:19:29, 12.00s/it, loss=1.1310, lr=8.02e-06]Steps:   8%|▊         | 402/5000 [1:24:18<15:19:29, 12.00s/it, loss=1.1069, lr=8.04e-06]Steps:   8%|▊         | 403/5000 [1:24:30<15:18:33, 11.99s/it, loss=1.1069, lr=8.04e-06]Steps:   8%|▊         | 403/5000 [1:24:30<15:18:33, 11.99s/it, loss=0.3886, lr=8.06e-06]Steps:   8%|▊         | 404/5000 [1:24:42<15:19:13, 12.00s/it, loss=0.3886, lr=8.06e-06]Steps:   8%|▊         | 404/5000 [1:24:42<15:19:13, 12.00s/it, loss=0.5365, lr=8.08e-06]Steps:   8%|▊         | 405/5000 [1:24:54<15:17:40, 11.98s/it, loss=0.5365, lr=8.08e-06]Steps:   8%|▊         | 405/5000 [1:24:54<15:17:40, 11.98s/it, loss=0.5326, lr=8.10e-06]Steps:   8%|▊         | 406/5000 [1:25:05<15:14:10, 11.94s/it, loss=0.5326, lr=8.10e-06]Steps:   8%|▊         | 406/5000 [1:25:05<15:14:10, 11.94s/it, loss=0.6155, lr=8.12e-06]Steps:   8%|▊         | 407/5000 [1:25:17<15:13:52, 11.94s/it, loss=0.6155, lr=8.12e-06]Steps:   8%|▊         | 407/5000 [1:25:17<15:13:52, 11.94s/it, loss=1.1342, lr=8.14e-06]Steps:   8%|▊         | 408/5000 [1:25:29<15:17:42, 11.99s/it, loss=1.1342, lr=8.14e-06]Steps:   8%|▊         | 408/5000 [1:25:29<15:17:42, 11.99s/it, loss=0.9615, lr=8.16e-06]Steps:   8%|▊         | 409/5000 [1:25:41<15:16:08, 11.97s/it, loss=0.9615, lr=8.16e-06]Steps:   8%|▊         | 409/5000 [1:25:41<15:16:08, 11.97s/it, loss=0.8413, lr=8.18e-06]Steps:   8%|▊         | 410/5000 [1:25:53<15:11:59, 11.92s/it, loss=0.8413, lr=8.18e-06]Steps:   8%|▊         | 410/5000 [1:25:53<15:11:59, 11.92s/it, loss=0.9994, lr=8.20e-06]
[Step 410] Training Debug Info:
  Loss: 0.374000
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0001, std: 0.8555
  Noise mean: 0.0022, std: 1.0000
  Target mean: 0.0021, std: 1.3125
  Model pred mean: -0.0017, std: 1.1719
  Sigmas: [0.81640625]... (timesteps: [815.0])

[Step 410] Training Debug Info:
  Loss: 0.687905
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0427, std: 0.9258
  Noise mean: -0.0015, std: 1.0000
  Target mean: -0.0442, std: 1.3672
  Model pred mean: -0.0417, std: 1.1016
  Sigmas: [0.447265625]... (timesteps: [448.0])

[Step 410] Training Debug Info:
  Loss: 0.544127
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0127, std: 0.9180
  Noise mean: -0.0004, std: 1.0000
  Target mean: -0.0132, std: 1.3594
  Model pred mean: -0.0111, std: 1.1562
  Sigmas: [0.625]... (timesteps: [625.0])

[Step 410] Training Debug Info:
  Loss: 0.417752
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0166, std: 0.8984
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0164, std: 1.3438
  Model pred mean: 0.0189, std: 1.1953
  Sigmas: [0.74609375]... (timesteps: [748.0])
Steps:   8%|▊         | 411/5000 [1:26:05<15:16:53, 11.99s/it, loss=0.9994, lr=8.20e-06]Steps:   8%|▊         | 411/5000 [1:26:05<15:16:53, 11.99s/it, loss=0.4178, lr=8.22e-06]Steps:   8%|▊         | 412/5000 [1:26:17<15:15:26, 11.97s/it, loss=0.4178, lr=8.22e-06]Steps:   8%|▊         | 412/5000 [1:26:17<15:15:26, 11.97s/it, loss=0.4728, lr=8.24e-06]Steps:   8%|▊         | 413/5000 [1:26:29<15:14:32, 11.96s/it, loss=0.4728, lr=8.24e-06]Steps:   8%|▊         | 413/5000 [1:26:29<15:14:32, 11.96s/it, loss=0.4810, lr=8.26e-06]Steps:   8%|▊         | 414/5000 [1:26:41<15:13:37, 11.95s/it, loss=0.4810, lr=8.26e-06]Steps:   8%|▊         | 414/5000 [1:26:41<15:13:37, 11.95s/it, loss=0.3690, lr=8.28e-06]Steps:   8%|▊         | 415/5000 [1:26:53<15:18:03, 12.01s/it, loss=0.3690, lr=8.28e-06]Steps:   8%|▊         | 415/5000 [1:26:53<15:18:03, 12.01s/it, loss=0.6188, lr=8.30e-06]Steps:   8%|▊         | 416/5000 [1:27:05<15:17:09, 12.00s/it, loss=0.6188, lr=8.30e-06]Steps:   8%|▊         | 416/5000 [1:27:05<15:17:09, 12.00s/it, loss=0.8507, lr=8.32e-06]Steps:   8%|▊         | 417/5000 [1:27:17<15:13:06, 11.95s/it, loss=0.8507, lr=8.32e-06]Steps:   8%|▊         | 417/5000 [1:27:17<15:13:06, 11.95s/it, loss=0.5524, lr=8.34e-06]Steps:   8%|▊         | 418/5000 [1:27:29<15:17:00, 12.01s/it, loss=0.5524, lr=8.34e-06]Steps:   8%|▊         | 418/5000 [1:27:29<15:17:00, 12.01s/it, loss=1.1127, lr=8.36e-06]Steps:   8%|▊         | 419/5000 [1:27:41<15:15:24, 11.99s/it, loss=1.1127, lr=8.36e-06]Steps:   8%|▊         | 419/5000 [1:27:41<15:15:24, 11.99s/it, loss=0.6101, lr=8.38e-06]Steps:   8%|▊         | 420/5000 [1:27:53<15:13:30, 11.97s/it, loss=0.6101, lr=8.38e-06]Steps:   8%|▊         | 420/5000 [1:27:53<15:13:30, 11.97s/it, loss=0.6391, lr=8.40e-06]
[Step 420] Training Debug Info:
  Loss: 1.075367
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0344, std: 0.9609
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0332, std: 1.3828
  Model pred mean: -0.0371, std: 0.9219
  Sigmas: [0.134765625]... (timesteps: [135.0])

[Step 420] Training Debug Info:
  Loss: 1.130401
  Latent shape: torch.Size([1, 32, 150, 60]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0309, std: 0.8945
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0300, std: 1.3438
  Model pred mean: -0.0311, std: 0.8242
  Sigmas: [0.1162109375]... (timesteps: [116.0])

[Step 420] Training Debug Info:
  Loss: 0.803866
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0251, std: 0.9531
  Noise mean: -0.0038, std: 1.0000
  Target mean: -0.0289, std: 1.3750
  Model pred mean: -0.0282, std: 1.0469
  Sigmas: [0.423828125]... (timesteps: [423.0])

[Step 420] Training Debug Info:
  Loss: 1.015182
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0349, std: 0.9141
  Noise mean: -0.0027, std: 1.0000
  Target mean: -0.0376, std: 1.3516
  Model pred mean: -0.0344, std: 0.9062
  Sigmas: [0.26171875]... (timesteps: [262.0])
Steps:   8%|▊         | 421/5000 [1:28:05<15:12:09, 11.95s/it, loss=0.6391, lr=8.40e-06]Steps:   8%|▊         | 421/5000 [1:28:05<15:12:09, 11.95s/it, loss=1.0152, lr=8.42e-06]Steps:   8%|▊         | 422/5000 [1:28:17<15:18:42, 12.04s/it, loss=1.0152, lr=8.42e-06]Steps:   8%|▊         | 422/5000 [1:28:17<15:18:42, 12.04s/it, loss=1.0571, lr=8.44e-06]Steps:   8%|▊         | 423/5000 [1:28:29<15:17:10, 12.02s/it, loss=1.0571, lr=8.44e-06]Steps:   8%|▊         | 423/5000 [1:28:29<15:17:10, 12.02s/it, loss=1.0480, lr=8.46e-06]Steps:   8%|▊         | 424/5000 [1:28:41<15:14:08, 11.99s/it, loss=1.0480, lr=8.46e-06]Steps:   8%|▊         | 424/5000 [1:28:41<15:14:08, 11.99s/it, loss=1.0243, lr=8.48e-06]Steps:   8%|▊         | 425/5000 [1:28:53<15:12:13, 11.96s/it, loss=1.0243, lr=8.48e-06]Steps:   8%|▊         | 425/5000 [1:28:53<15:12:13, 11.96s/it, loss=1.1232, lr=8.50e-06]Steps:   9%|▊         | 426/5000 [1:29:05<15:10:46, 11.95s/it, loss=1.1232, lr=8.50e-06]Steps:   9%|▊         | 426/5000 [1:29:05<15:10:46, 11.95s/it, loss=0.9961, lr=8.52e-06]Steps:   9%|▊         | 427/5000 [1:29:17<15:09:39, 11.94s/it, loss=0.9961, lr=8.52e-06]Steps:   9%|▊         | 427/5000 [1:29:17<15:09:39, 11.94s/it, loss=1.1003, lr=8.54e-06]Steps:   9%|▊         | 428/5000 [1:29:29<15:13:27, 11.99s/it, loss=1.1003, lr=8.54e-06]Steps:   9%|▊         | 428/5000 [1:29:29<15:13:27, 11.99s/it, loss=1.0824, lr=8.56e-06]Steps:   9%|▊         | 429/5000 [1:29:41<15:09:35, 11.94s/it, loss=1.0824, lr=8.56e-06]Steps:   9%|▊         | 429/5000 [1:29:41<15:09:35, 11.94s/it, loss=0.5766, lr=8.58e-06]Steps:   9%|▊         | 430/5000 [1:29:53<15:07:08, 11.91s/it, loss=0.5766, lr=8.58e-06]Steps:   9%|▊         | 430/5000 [1:29:53<15:07:08, 11.91s/it, loss=1.1860, lr=8.60e-06]
[Step 430] Training Debug Info:
  Loss: 0.615174
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0002, std: 0.8828
  Noise mean: 0.0023, std: 1.0000
  Target mean: 0.0022, std: 1.3359
  Model pred mean: -0.0231, std: 1.0938
  Sigmas: [0.9609375]... (timesteps: [959.0])

[Step 430] Training Debug Info:
  Loss: 0.449220
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0073, std: 0.8867
  Noise mean: 0.0011, std: 1.0000
  Target mean: -0.0062, std: 1.3359
  Model pred mean: -0.0181, std: 1.1484
  Sigmas: [0.921875]... (timesteps: [923.0])

[Step 430] Training Debug Info:
  Loss: 0.398448
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0115, std: 0.9258
  Noise mean: 0.0007, std: 1.0000
  Target mean: -0.0107, std: 1.3672
  Model pred mean: -0.0150, std: 1.2031
  Sigmas: [0.7578125]... (timesteps: [757.0])

[Step 430] Training Debug Info:
  Loss: 1.144705
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0122, std: 0.9023
  Noise mean: 0.0014, std: 1.0000
  Target mean: 0.0136, std: 1.3438
  Model pred mean: 0.0124, std: 0.8320
  Sigmas: [0.1796875]... (timesteps: [180.0])
Steps:   9%|▊         | 431/5000 [1:30:05<15:05:48, 11.89s/it, loss=1.1860, lr=8.60e-06]Steps:   9%|▊         | 431/5000 [1:30:05<15:05:48, 11.89s/it, loss=1.1447, lr=8.62e-06]Steps:   9%|▊         | 432/5000 [1:30:16<15:04:58, 11.89s/it, loss=1.1447, lr=8.62e-06]Steps:   9%|▊         | 432/5000 [1:30:16<15:04:58, 11.89s/it, loss=0.4775, lr=8.64e-06]Steps:   9%|▊         | 433/5000 [1:30:28<15:05:23, 11.89s/it, loss=0.4775, lr=8.64e-06]Steps:   9%|▊         | 433/5000 [1:30:28<15:05:23, 11.89s/it, loss=0.5601, lr=8.66e-06]Steps:   9%|▊         | 434/5000 [1:30:40<15:07:51, 11.93s/it, loss=0.5601, lr=8.66e-06]Steps:   9%|▊         | 434/5000 [1:30:40<15:07:51, 11.93s/it, loss=0.4488, lr=8.68e-06]Steps:   9%|▊         | 435/5000 [1:30:52<15:10:18, 11.96s/it, loss=0.4488, lr=8.68e-06]Steps:   9%|▊         | 435/5000 [1:30:52<15:10:18, 11.96s/it, loss=1.1149, lr=8.70e-06]Steps:   9%|▊         | 436/5000 [1:31:04<15:08:20, 11.94s/it, loss=1.1149, lr=8.70e-06]Steps:   9%|▊         | 436/5000 [1:31:04<15:08:20, 11.94s/it, loss=1.1754, lr=8.72e-06]Steps:   9%|▊         | 437/5000 [1:31:16<15:07:16, 11.93s/it, loss=1.1754, lr=8.72e-06]Steps:   9%|▊         | 437/5000 [1:31:16<15:07:16, 11.93s/it, loss=0.8397, lr=8.74e-06]Steps:   9%|▉         | 438/5000 [1:31:28<15:05:33, 11.91s/it, loss=0.8397, lr=8.74e-06]Steps:   9%|▉         | 438/5000 [1:31:28<15:05:33, 11.91s/it, loss=1.0802, lr=8.76e-06]Steps:   9%|▉         | 439/5000 [1:31:40<15:03:46, 11.89s/it, loss=1.0802, lr=8.76e-06]Steps:   9%|▉         | 439/5000 [1:31:40<15:03:46, 11.89s/it, loss=0.4268, lr=8.78e-06]Steps:   9%|▉         | 440/5000 [1:31:52<15:03:34, 11.89s/it, loss=0.4268, lr=8.78e-06]Steps:   9%|▉         | 440/5000 [1:31:52<15:03:34, 11.89s/it, loss=0.3569, lr=8.80e-06]
[Step 440] Training Debug Info:
  Loss: 0.707639
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0195, std: 0.9414
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0193, std: 1.3750
  Model pred mean: 0.0022, std: 1.0703
  Sigmas: [0.99609375]... (timesteps: [996.0])

[Step 440] Training Debug Info:
  Loss: 0.624146
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0021, std: 0.8906
  Noise mean: 0.0000, std: 1.0000
  Target mean: -0.0021, std: 1.3359
  Model pred mean: -0.0026, std: 1.0938
  Sigmas: [0.58984375]... (timesteps: [590.0])

[Step 440] Training Debug Info:
  Loss: 0.598711
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0259, std: 0.9023
  Noise mean: -0.0025, std: 1.0000
  Target mean: -0.0284, std: 1.3438
  Model pred mean: -0.0112, std: 1.0781
  Sigmas: [0.953125]... (timesteps: [955.0])

[Step 440] Training Debug Info:
  Loss: 0.537691
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0082, std: 1.0078
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0087, std: 1.4219
  Model pred mean: -0.0043, std: 1.2188
  Sigmas: [0.66796875]... (timesteps: [668.0])
Steps:   9%|▉         | 441/5000 [1:32:04<15:05:26, 11.92s/it, loss=0.3569, lr=8.80e-06]Steps:   9%|▉         | 441/5000 [1:32:04<15:05:26, 11.92s/it, loss=0.5377, lr=8.82e-06]Steps:   9%|▉         | 442/5000 [1:32:16<15:12:11, 12.01s/it, loss=0.5377, lr=8.82e-06]Steps:   9%|▉         | 442/5000 [1:32:16<15:12:11, 12.01s/it, loss=1.1438, lr=8.84e-06]Steps:   9%|▉         | 443/5000 [1:32:28<15:10:47, 11.99s/it, loss=1.1438, lr=8.84e-06]Steps:   9%|▉         | 443/5000 [1:32:28<15:10:47, 11.99s/it, loss=0.6878, lr=8.86e-06]Steps:   9%|▉         | 444/5000 [1:32:40<15:07:08, 11.95s/it, loss=0.6878, lr=8.86e-06]Steps:   9%|▉         | 444/5000 [1:32:40<15:07:08, 11.95s/it, loss=0.4069, lr=8.88e-06]Steps:   9%|▉         | 445/5000 [1:32:52<15:06:46, 11.94s/it, loss=0.4069, lr=8.88e-06]Steps:   9%|▉         | 445/5000 [1:32:52<15:06:46, 11.94s/it, loss=0.5978, lr=8.90e-06]Steps:   9%|▉         | 446/5000 [1:33:04<15:04:58, 11.92s/it, loss=0.5978, lr=8.90e-06]Steps:   9%|▉         | 446/5000 [1:33:04<15:04:58, 11.92s/it, loss=0.6476, lr=8.92e-06]Steps:   9%|▉         | 447/5000 [1:33:16<15:04:29, 11.92s/it, loss=0.6476, lr=8.92e-06]Steps:   9%|▉         | 447/5000 [1:33:16<15:04:29, 11.92s/it, loss=0.4238, lr=8.94e-06]Steps:   9%|▉         | 448/5000 [1:33:27<15:04:16, 11.92s/it, loss=0.4238, lr=8.94e-06]Steps:   9%|▉         | 448/5000 [1:33:27<15:04:16, 11.92s/it, loss=1.0791, lr=8.96e-06]Steps:   9%|▉         | 449/5000 [1:33:40<15:08:15, 11.97s/it, loss=1.0791, lr=8.96e-06]Steps:   9%|▉         | 449/5000 [1:33:40<15:08:15, 11.97s/it, loss=0.3409, lr=8.98e-06]Steps:   9%|▉         | 450/5000 [1:33:51<15:05:27, 11.94s/it, loss=0.3409, lr=8.98e-06]Steps:   9%|▉         | 450/5000 [1:33:51<15:05:27, 11.94s/it, loss=0.3603, lr=9.00e-06]
[Step 450] Training Debug Info:
  Loss: 1.177477
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0518, std: 0.8828
  Noise mean: -0.0026, std: 1.0000
  Target mean: 0.0491, std: 1.3359
  Model pred mean: 0.0476, std: 0.7812
  Sigmas: [0.2080078125]... (timesteps: [208.0])

[Step 450] Training Debug Info:
  Loss: 0.560364
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0009, std: 0.9297
  Noise mean: -0.0026, std: 1.0000
  Target mean: -0.0034, std: 1.3672
  Model pred mean: 0.0006, std: 1.1562
  Sigmas: [0.5703125]... (timesteps: [570.0])

[Step 450] Training Debug Info:
  Loss: 0.632128
  Latent shape: torch.Size([1, 32, 84, 102]), Packed shape: torch.Size([1, 2142, 128])
  Latent mean: 0.0079, std: 0.9141
  Noise mean: 0.0023, std: 0.9961
  Target mean: -0.0056, std: 1.3516
  Model pred mean: -0.0040, std: 1.1094
  Sigmas: [0.5546875]... (timesteps: [553.0])

[Step 450] Training Debug Info:
  Loss: 0.547270
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0042, std: 0.8789
  Noise mean: 0.0029, std: 1.0000
  Target mean: -0.0013, std: 1.3281
  Model pred mean: -0.0025, std: 1.1172
  Sigmas: [0.6328125]... (timesteps: [631.0])
Steps:   9%|▉         | 451/5000 [1:34:03<15:03:48, 11.92s/it, loss=0.3603, lr=9.00e-06]Steps:   9%|▉         | 451/5000 [1:34:03<15:03:48, 11.92s/it, loss=0.5473, lr=9.02e-06]Steps:   9%|▉         | 452/5000 [1:34:15<15:08:13, 11.98s/it, loss=0.5473, lr=9.02e-06]Steps:   9%|▉         | 452/5000 [1:34:15<15:08:13, 11.98s/it, loss=0.3548, lr=9.04e-06]Steps:   9%|▉         | 453/5000 [1:34:27<15:05:13, 11.94s/it, loss=0.3548, lr=9.04e-06]Steps:   9%|▉         | 453/5000 [1:34:27<15:05:13, 11.94s/it, loss=1.1098, lr=9.06e-06]Steps:   9%|▉         | 454/5000 [1:34:39<15:01:29, 11.90s/it, loss=1.1098, lr=9.06e-06]Steps:   9%|▉         | 454/5000 [1:34:39<15:01:29, 11.90s/it, loss=0.4192, lr=9.08e-06]Steps:   9%|▉         | 455/5000 [1:34:51<15:06:29, 11.97s/it, loss=0.4192, lr=9.08e-06]Steps:   9%|▉         | 455/5000 [1:34:51<15:06:29, 11.97s/it, loss=1.0643, lr=9.10e-06]Steps:   9%|▉         | 456/5000 [1:35:03<15:04:30, 11.94s/it, loss=1.0643, lr=9.10e-06]Steps:   9%|▉         | 456/5000 [1:35:03<15:04:30, 11.94s/it, loss=0.5577, lr=9.12e-06]Steps:   9%|▉         | 457/5000 [1:35:15<15:03:53, 11.94s/it, loss=0.5577, lr=9.12e-06]Steps:   9%|▉         | 457/5000 [1:35:15<15:03:53, 11.94s/it, loss=1.0052, lr=9.14e-06]Steps:   9%|▉         | 458/5000 [1:35:27<15:03:06, 11.93s/it, loss=1.0052, lr=9.14e-06]Steps:   9%|▉         | 458/5000 [1:35:27<15:03:06, 11.93s/it, loss=0.4610, lr=9.16e-06]Steps:   9%|▉         | 459/5000 [1:35:39<15:03:17, 11.94s/it, loss=0.4610, lr=9.16e-06]Steps:   9%|▉         | 459/5000 [1:35:39<15:03:17, 11.94s/it, loss=0.8682, lr=9.18e-06]Steps:   9%|▉         | 460/5000 [1:35:51<15:02:46, 11.93s/it, loss=0.8682, lr=9.18e-06]Steps:   9%|▉         | 460/5000 [1:35:51<15:02:46, 11.93s/it, loss=1.1223, lr=9.20e-06]
[Step 460] Training Debug Info:
  Loss: 1.131079
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0141, std: 0.9023
  Noise mean: 0.0008, std: 1.0000
  Target mean: -0.0134, std: 1.3438
  Model pred mean: -0.0137, std: 0.8281
  Sigmas: [0.2177734375]... (timesteps: [218.0])

[Step 460] Training Debug Info:
  Loss: 0.388505
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0183, std: 0.9258
  Noise mean: 0.0025, std: 1.0000
  Target mean: 0.0209, std: 1.3594
  Model pred mean: 0.0175, std: 1.1953
  Sigmas: [0.74609375]... (timesteps: [746.0])

[Step 460] Training Debug Info:
  Loss: 0.588199
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0116, std: 0.8672
  Noise mean: 0.0000, std: 1.0000
  Target mean: 0.0116, std: 1.3203
  Model pred mean: 0.0376, std: 1.1172
  Sigmas: [0.99609375]... (timesteps: [995.0])

[Step 460] Training Debug Info:
  Loss: 0.970006
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0117, std: 0.8203
  Noise mean: 0.0042, std: 1.0000
  Target mean: 0.0160, std: 1.2891
  Model pred mean: 0.0061, std: 0.8281
  Sigmas: [0.408203125]... (timesteps: [408.0])
Steps:   9%|▉         | 461/5000 [1:36:03<15:04:49, 11.96s/it, loss=1.1223, lr=9.20e-06]Steps:   9%|▉         | 461/5000 [1:36:03<15:04:49, 11.96s/it, loss=0.9700, lr=9.22e-06]Steps:   9%|▉         | 462/5000 [1:36:15<15:09:23, 12.02s/it, loss=0.9700, lr=9.22e-06]Steps:   9%|▉         | 462/5000 [1:36:15<15:09:23, 12.02s/it, loss=0.4030, lr=9.24e-06]Steps:   9%|▉         | 463/5000 [1:36:27<15:06:09, 11.98s/it, loss=0.4030, lr=9.24e-06]Steps:   9%|▉         | 463/5000 [1:36:27<15:06:09, 11.98s/it, loss=1.1240, lr=9.26e-06]Steps:   9%|▉         | 464/5000 [1:36:39<15:03:55, 11.96s/it, loss=1.1240, lr=9.26e-06]Steps:   9%|▉         | 464/5000 [1:36:39<15:03:55, 11.96s/it, loss=0.7025, lr=9.28e-06]Steps:   9%|▉         | 465/5000 [1:36:51<15:03:33, 11.95s/it, loss=0.7025, lr=9.28e-06]Steps:   9%|▉         | 465/5000 [1:36:51<15:03:33, 11.95s/it, loss=0.3893, lr=9.30e-06]Steps:   9%|▉         | 466/5000 [1:37:03<15:01:22, 11.93s/it, loss=0.3893, lr=9.30e-06]Steps:   9%|▉         | 466/5000 [1:37:03<15:01:22, 11.93s/it, loss=1.1029, lr=9.32e-06]Steps:   9%|▉         | 467/5000 [1:37:14<15:00:58, 11.93s/it, loss=1.1029, lr=9.32e-06]Steps:   9%|▉         | 467/5000 [1:37:14<15:00:58, 11.93s/it, loss=1.0888, lr=9.34e-06]Steps:   9%|▉         | 468/5000 [1:37:26<14:59:53, 11.91s/it, loss=1.0888, lr=9.34e-06]Steps:   9%|▉         | 468/5000 [1:37:26<14:59:53, 11.91s/it, loss=0.7140, lr=9.36e-06]Steps:   9%|▉         | 469/5000 [1:37:38<15:02:49, 11.96s/it, loss=0.7140, lr=9.36e-06]Steps:   9%|▉         | 469/5000 [1:37:38<15:02:49, 11.96s/it, loss=0.9210, lr=9.38e-06]Steps:   9%|▉         | 470/5000 [1:37:50<15:03:14, 11.96s/it, loss=0.9210, lr=9.38e-06]Steps:   9%|▉         | 470/5000 [1:37:50<15:03:14, 11.96s/it, loss=0.3868, lr=9.40e-06]
[Step 470] Training Debug Info:
  Loss: 1.139341
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0079, std: 0.9258
  Noise mean: -0.0003, std: 1.0000
  Target mean: -0.0082, std: 1.3594
  Model pred mean: -0.0087, std: 0.8516
  Sigmas: [0.130859375]... (timesteps: [131.0])

[Step 470] Training Debug Info:
  Loss: 1.014758
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0217, std: 0.9219
  Noise mean: 0.0022, std: 1.0000
  Target mean: -0.0195, std: 1.3594
  Model pred mean: -0.0178, std: 0.9141
  Sigmas: [0.2578125]... (timesteps: [257.0])

[Step 470] Training Debug Info:
  Loss: 1.171551
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0018, std: 0.8906
  Noise mean: -0.0028, std: 1.0000
  Target mean: -0.0046, std: 1.3359
  Model pred mean: 0.0004, std: 0.7969
  Sigmas: [0.2373046875]... (timesteps: [237.0])

[Step 470] Training Debug Info:
  Loss: 0.521643
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0022, std: 0.9180
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0041, std: 1.3594
  Model pred mean: -0.0007, std: 1.1406
  Sigmas: [0.6484375]... (timesteps: [649.0])
Steps:   9%|▉         | 471/5000 [1:38:02<15:01:35, 11.94s/it, loss=0.3868, lr=9.40e-06]Steps:   9%|▉         | 471/5000 [1:38:02<15:01:35, 11.94s/it, loss=0.5216, lr=9.42e-06]Steps:   9%|▉         | 472/5000 [1:38:14<15:02:42, 11.96s/it, loss=0.5216, lr=9.42e-06]Steps:   9%|▉         | 472/5000 [1:38:14<15:02:42, 11.96s/it, loss=0.4056, lr=9.44e-06]Steps:   9%|▉         | 473/5000 [1:38:26<15:00:23, 11.93s/it, loss=0.4056, lr=9.44e-06]Steps:   9%|▉         | 473/5000 [1:38:26<15:00:23, 11.93s/it, loss=0.7171, lr=9.46e-06]Steps:   9%|▉         | 474/5000 [1:38:38<14:57:48, 11.90s/it, loss=0.7171, lr=9.46e-06]Steps:   9%|▉         | 474/5000 [1:38:38<14:57:48, 11.90s/it, loss=0.5145, lr=9.48e-06]Steps:  10%|▉         | 475/5000 [1:38:50<14:56:33, 11.89s/it, loss=0.5145, lr=9.48e-06]Steps:  10%|▉         | 475/5000 [1:38:50<14:56:33, 11.89s/it, loss=0.4055, lr=9.50e-06]Steps:  10%|▉         | 476/5000 [1:39:02<14:59:55, 11.94s/it, loss=0.4055, lr=9.50e-06]Steps:  10%|▉         | 476/5000 [1:39:02<14:59:55, 11.94s/it, loss=1.1705, lr=9.52e-06]Steps:  10%|▉         | 477/5000 [1:39:14<15:00:19, 11.94s/it, loss=1.1705, lr=9.52e-06]Steps:  10%|▉         | 477/5000 [1:39:14<15:00:19, 11.94s/it, loss=0.7832, lr=9.54e-06]Steps:  10%|▉         | 478/5000 [1:39:26<14:59:50, 11.94s/it, loss=0.7832, lr=9.54e-06]Steps:  10%|▉         | 478/5000 [1:39:26<14:59:50, 11.94s/it, loss=0.6550, lr=9.56e-06]Steps:  10%|▉         | 479/5000 [1:39:38<15:00:29, 11.95s/it, loss=0.6550, lr=9.56e-06]Steps:  10%|▉         | 479/5000 [1:39:38<15:00:29, 11.95s/it, loss=0.6033, lr=9.58e-06]Steps:  10%|▉         | 480/5000 [1:39:50<14:58:43, 11.93s/it, loss=0.6033, lr=9.58e-06]Steps:  10%|▉         | 480/5000 [1:39:50<14:58:43, 11.93s/it, loss=0.5105, lr=9.60e-06]
[Step 480] Training Debug Info:
  Loss: 0.569833
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: -0.0064, std: 0.9219
  Noise mean: 0.0020, std: 1.0000
  Target mean: 0.0084, std: 1.3594
  Model pred mean: 0.0040, std: 1.1328
  Sigmas: [0.59375]... (timesteps: [594.0])

[Step 480] Training Debug Info:
  Loss: 0.565696
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0094, std: 0.8828
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0085, std: 1.3359
  Model pred mean: -0.0074, std: 1.1094
  Sigmas: [0.62890625]... (timesteps: [628.0])

[Step 480] Training Debug Info:
  Loss: 1.174249
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0122, std: 0.9219
  Noise mean: -0.0003, std: 1.0000
  Target mean: 0.0119, std: 1.3594
  Model pred mean: 0.0160, std: 0.8281
  Sigmas: [0.19140625]... (timesteps: [191.0])

[Step 480] Training Debug Info:
  Loss: 0.697117
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0109, std: 0.8984
  Noise mean: -0.0004, std: 1.0000
  Target mean: -0.0112, std: 1.3438
  Model pred mean: -0.0084, std: 1.0703
  Sigmas: [0.52734375]... (timesteps: [529.0])
Steps:  10%|▉         | 481/5000 [1:40:02<14:58:58, 11.94s/it, loss=0.5105, lr=9.60e-06]Steps:  10%|▉         | 481/5000 [1:40:02<14:58:58, 11.94s/it, loss=0.6971, lr=9.62e-06]Steps:  10%|▉         | 482/5000 [1:40:14<15:00:41, 11.96s/it, loss=0.6971, lr=9.62e-06]Steps:  10%|▉         | 482/5000 [1:40:14<15:00:41, 11.96s/it, loss=1.0798, lr=9.64e-06]Steps:  10%|▉         | 483/5000 [1:40:26<15:02:31, 11.99s/it, loss=1.0798, lr=9.64e-06]Steps:  10%|▉         | 483/5000 [1:40:26<15:02:31, 11.99s/it, loss=1.1424, lr=9.66e-06]Steps:  10%|▉         | 484/5000 [1:40:38<15:01:57, 11.98s/it, loss=1.1424, lr=9.66e-06]Steps:  10%|▉         | 484/5000 [1:40:38<15:01:57, 11.98s/it, loss=0.7613, lr=9.68e-06]Steps:  10%|▉         | 485/5000 [1:40:50<15:00:42, 11.97s/it, loss=0.7613, lr=9.68e-06]Steps:  10%|▉         | 485/5000 [1:40:50<15:00:42, 11.97s/it, loss=0.4131, lr=9.70e-06]Steps:  10%|▉         | 486/5000 [1:41:02<15:00:02, 11.96s/it, loss=0.4131, lr=9.70e-06]Steps:  10%|▉         | 486/5000 [1:41:02<15:00:02, 11.96s/it, loss=1.1240, lr=9.72e-06]Steps:  10%|▉         | 487/5000 [1:41:14<15:00:01, 11.97s/it, loss=1.1240, lr=9.72e-06]Steps:  10%|▉         | 487/5000 [1:41:14<15:00:01, 11.97s/it, loss=0.4164, lr=9.74e-06]Steps:  10%|▉         | 488/5000 [1:41:25<15:00:19, 11.97s/it, loss=0.4164, lr=9.74e-06]Steps:  10%|▉         | 488/5000 [1:41:25<15:00:19, 11.97s/it, loss=0.5156, lr=9.76e-06]Steps:  10%|▉         | 489/5000 [1:41:38<15:02:31, 12.00s/it, loss=0.5156, lr=9.76e-06]Steps:  10%|▉         | 489/5000 [1:41:38<15:02:31, 12.00s/it, loss=0.7137, lr=9.78e-06]Steps:  10%|▉         | 490/5000 [1:41:49<14:59:28, 11.97s/it, loss=0.7137, lr=9.78e-06]Steps:  10%|▉         | 490/5000 [1:41:49<14:59:28, 11.97s/it, loss=0.8544, lr=9.80e-06]
[Step 490] Training Debug Info:
  Loss: 0.549744
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0146, std: 0.8711
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0144, std: 1.3281
  Model pred mean: 0.0124, std: 1.1016
  Sigmas: [0.6484375]... (timesteps: [647.0])

[Step 490] Training Debug Info:
  Loss: 0.602709
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0278, std: 0.9219
  Noise mean: 0.0014, std: 1.0000
  Target mean: -0.0264, std: 1.3594
  Model pred mean: -0.0339, std: 1.1172
  Sigmas: [0.59765625]... (timesteps: [596.0])

[Step 490] Training Debug Info:
  Loss: 0.447167
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0136, std: 0.9102
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0150, std: 1.3516
  Model pred mean: -0.0181, std: 1.1797
  Sigmas: [0.7109375]... (timesteps: [711.0])

[Step 490] Training Debug Info:
  Loss: 0.437295
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0325, std: 0.8906
  Noise mean: 0.0007, std: 1.0000
  Target mean: -0.0317, std: 1.3438
  Model pred mean: -0.0308, std: 1.1719
  Sigmas: [0.83984375]... (timesteps: [838.0])
Steps:  10%|▉         | 491/5000 [1:42:01<14:58:35, 11.96s/it, loss=0.8544, lr=9.80e-06]Steps:  10%|▉         | 491/5000 [1:42:01<14:58:35, 11.96s/it, loss=0.4373, lr=9.82e-06]Steps:  10%|▉         | 492/5000 [1:42:13<14:55:05, 11.91s/it, loss=0.4373, lr=9.82e-06]Steps:  10%|▉         | 492/5000 [1:42:13<14:55:05, 11.91s/it, loss=0.9123, lr=9.84e-06]Steps:  10%|▉         | 493/5000 [1:42:25<14:53:42, 11.90s/it, loss=0.9123, lr=9.84e-06]Steps:  10%|▉         | 493/5000 [1:42:25<14:53:42, 11.90s/it, loss=0.3699, lr=9.86e-06]Steps:  10%|▉         | 494/5000 [1:42:37<14:52:53, 11.89s/it, loss=0.3699, lr=9.86e-06]Steps:  10%|▉         | 494/5000 [1:42:37<14:52:53, 11.89s/it, loss=0.6243, lr=9.88e-06]Steps:  10%|▉         | 495/5000 [1:42:49<14:50:59, 11.87s/it, loss=0.6243, lr=9.88e-06]Steps:  10%|▉         | 495/5000 [1:42:49<14:50:59, 11.87s/it, loss=0.8341, lr=9.90e-06]Steps:  10%|▉         | 496/5000 [1:43:01<14:56:11, 11.94s/it, loss=0.8341, lr=9.90e-06]Steps:  10%|▉         | 496/5000 [1:43:01<14:56:11, 11.94s/it, loss=0.5304, lr=9.92e-06]Steps:  10%|▉         | 497/5000 [1:43:13<14:57:17, 11.96s/it, loss=0.5304, lr=9.92e-06]Steps:  10%|▉         | 497/5000 [1:43:13<14:57:17, 11.96s/it, loss=0.8402, lr=9.94e-06]Steps:  10%|▉         | 498/5000 [1:43:25<14:55:24, 11.93s/it, loss=0.8402, lr=9.94e-06]Steps:  10%|▉         | 498/5000 [1:43:25<14:55:24, 11.93s/it, loss=0.4514, lr=9.96e-06]Steps:  10%|▉         | 499/5000 [1:43:37<14:52:34, 11.90s/it, loss=0.4514, lr=9.96e-06]Steps:  10%|▉         | 499/5000 [1:43:37<14:52:34, 11.90s/it, loss=1.1785, lr=9.98e-06]Steps:  10%|█         | 500/5000 [1:43:48<14:51:20, 11.88s/it, loss=1.1785, lr=9.98e-06]Steps:  10%|█         | 500/5000 [1:43:48<14:51:20, 11.88s/it, loss=0.3982, lr=1.00e-05]01/22/2026 05:08:27 - INFO - __main__ - 
[Step 500] ✅ Loss in normal range (0.3982)
01/22/2026 05:08:27 - INFO - __main__ -   Loss avg (last 100): 0.7432
01/22/2026 05:08:27 - INFO - __main__ -   Loss range: [0.3409, 1.1860]
Configuration saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/transformer/config.json
Model weights saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/transformer/diffusion_pytorch_model.safetensors
01/22/2026 05:10:06 - INFO - __main__ - Saved checkpoint to /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500
01/22/2026 05:10:06 - INFO - accelerate.accelerator - Saving current state to /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/accelerator
01/22/2026 05:10:06 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
01/22/2026 05:12:20 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/accelerator/pytorch_model
01/22/2026 05:12:20 - INFO - accelerate.checkpointing - Scheduler state saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/accelerator/scheduler.bin
01/22/2026 05:12:20 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/accelerator/sampler.bin
01/22/2026 05:12:20 - INFO - accelerate.checkpointing - Random states saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-500/accelerator/random_states_0.pkl
01/22/2026 05:12:20 - INFO - __main__ - 
🔍 Running validation at step 500...
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 500 (parquet mode)...
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
============================================================
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - Running validation at step 500...
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Moving VAE and text_encoder to GPU...
01/22/2026 05:12:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 1/12: The figure illustrates a process hooking mechanism using the LD_PRELOAD environm...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.21it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.71it/s][A
 11%|█         | 3/28 [00:01<00:15,  1.59it/s][A
 14%|█▍        | 4/28 [00:02<00:15,  1.54it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.51it/s][A
 21%|██▏       | 6/28 [00:03<00:14,  1.50it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.49it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.48it/s][A
 32%|███▏      | 9/28 [00:05<00:12,  1.48it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.47it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.47it/s][A
 43%|████▎     | 12/28 [00:07<00:10,  1.47it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.46it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.46it/s][A
 54%|█████▎    | 15/28 [00:10<00:08,  1.46it/s][A
 57%|█████▋    | 16/28 [00:10<00:08,  1.46it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.46it/s][A
 64%|██████▍   | 18/28 [00:12<00:06,  1.46it/s][A
 68%|██████▊   | 19/28 [00:12<00:06,  1.46it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.46it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.46it/s][A
 79%|███████▊  | 22/28 [00:14<00:04,  1.46it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.46it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.46it/s][A
 89%|████████▉ | 25/28 [00:16<00:02,  1.46it/s][A
 93%|█████████▎| 26/28 [00:17<00:01,  1.46it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.46it/s][A
100%|██████████| 28/28 [00:18<00:00,  1.46it/s][A100%|██████████| 28/28 [00:18<00:00,  1.48it/s]
01/22/2026 05:12:42 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt00_0_The_figure_illustrates_a_process_hooking_mechanism.png
01/22/2026 05:12:42 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 2/12: The figure presents an overview of four distinct end-to-end Task-Oriented Dialog...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.21it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.69it/s][A
 11%|█         | 3/28 [00:01<00:15,  1.57it/s][A
 14%|█▍        | 4/28 [00:02<00:15,  1.52it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.50it/s][A
 21%|██▏       | 6/28 [00:03<00:14,  1.48it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.47it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.47it/s][A
 32%|███▏      | 9/28 [00:05<00:12,  1.46it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.46it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.46it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.45it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.45it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.45it/s][A
 54%|█████▎    | 15/28 [00:10<00:08,  1.45it/s][A
 57%|█████▋    | 16/28 [00:10<00:08,  1.45it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.45it/s][A
 64%|██████▍   | 18/28 [00:12<00:06,  1.45it/s][A
 68%|██████▊   | 19/28 [00:12<00:06,  1.45it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.45it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.45it/s][A
 79%|███████▊  | 22/28 [00:14<00:04,  1.44it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.44it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.44it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.44it/s][A
 93%|█████████▎| 26/28 [00:17<00:01,  1.44it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.44it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.44it/s][A100%|██████████| 28/28 [00:19<00:00,  1.47it/s]
01/22/2026 05:13:01 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt01_0_The_figure_presents_an_overview_of_four_distinct_e.png
01/22/2026 05:13:01 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 3/12: The figure illustrates a network architecture for a single-step diffusion model ...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.12it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.63it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.52it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.47it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.44it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.43it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.42it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.41it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.41it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.41it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.40it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.40it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.40it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.40it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.40it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.40it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.40it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.40it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.40it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.39it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.40it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.40it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.40it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.40it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.39it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.39it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.39it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.39it/s][A100%|██████████| 28/28 [00:19<00:00,  1.41it/s]
01/22/2026 05:13:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt02_0_The_figure_illustrates_a_network_architecture_for.png
01/22/2026 05:13:22 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 4/12: The figure presents a comparative diagram of four different defect detection tas...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.11it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.51it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.46it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.44it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.42it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.41it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.41it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.40it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.40it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.40it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.40it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.40it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.39it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.39it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.39it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.39it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.39it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.39it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.39it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.39it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.39it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.39it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.39it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.39it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.39it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.39it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.39it/s][A100%|██████████| 28/28 [00:19<00:00,  1.41it/s]
01/22/2026 05:13:42 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt03_0_The_figure_presents_a_comparative_diagram_of_four.png
01/22/2026 05:13:42 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 5/12: The figure illustrates a model evaluation framework for a diffusion-based predic...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.11it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.50it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.46it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.43it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.41it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.40it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.40it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.39it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.39it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.39it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.39it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.39it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.39it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.39it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.39it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.38it/s][A100%|██████████| 28/28 [00:19<00:00,  1.40it/s]
01/22/2026 05:14:03 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt04_0_The_figure_illustrates_a_model_evaluation_framewor.png
01/22/2026 05:14:03 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 6/12: The figure illustrates a linear probing framework applied to a frozen multimodal...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.50it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.43it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.40it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.39it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 05:14:24 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt05_0_The_figure_illustrates_a_linear_probing_framework.png
01/22/2026 05:14:24 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 7/12: The figure presents a comparative architectural diagram illustrating two differe...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.60it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:07<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.40it/s]
01/22/2026 05:14:46 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt06_0_The_figure_presents_a_comparative_architectural_di.png
01/22/2026 05:14:46 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 8/12: The figure illustrates the overall architecture of L-RPCANet, a multi-stage deep...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.09it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.61it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:07<00:13,  1.38it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.37it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.37it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.37it/s][A100%|██████████| 28/28 [00:20<00:00,  1.39it/s]
01/22/2026 05:15:09 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt07_0_The_figure_illustrates_the_overall_architecture_of.png
01/22/2026 05:15:09 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 9/12: The figure illustrates the complete pipeline of a 3D scene reconstruction system...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.10it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.60it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.45it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.41it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.39it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:14<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.37it/s][A100%|██████████| 28/28 [00:20<00:00,  1.39it/s]
01/22/2026 05:15:29 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt08_0_The_figure_illustrates_the_complete_pipeline_of_a.png
01/22/2026 05:15:29 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 10/12: The figure presents seven distinct architectural patterns for fusing multi-modal...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.09it/s][A
  7%|▋         | 2/28 [00:01<00:16,  1.60it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.49it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.44it/s][A
 18%|█▊        | 5/28 [00:03<00:16,  1.42it/s][A
 21%|██▏       | 6/28 [00:04<00:15,  1.40it/s][A
 25%|██▌       | 7/28 [00:04<00:15,  1.40it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.39it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.39it/s][A
 36%|███▌      | 10/28 [00:07<00:13,  1.38it/s][A
 39%|███▉      | 11/28 [00:07<00:12,  1.38it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.38it/s][A
 46%|████▋     | 13/28 [00:09<00:10,  1.38it/s][A
 50%|█████     | 14/28 [00:09<00:10,  1.38it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.38it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.38it/s][A
 61%|██████    | 17/28 [00:12<00:07,  1.38it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.38it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.38it/s][A
 71%|███████▏  | 20/28 [00:14<00:05,  1.38it/s][A
 75%|███████▌  | 21/28 [00:15<00:05,  1.38it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.38it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.38it/s][A
 86%|████████▌ | 24/28 [00:17<00:02,  1.38it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.38it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.38it/s][A
 96%|█████████▋| 27/28 [00:19<00:00,  1.38it/s][A
100%|██████████| 28/28 [00:20<00:00,  1.38it/s][A100%|██████████| 28/28 [00:20<00:00,  1.39it/s]
01/22/2026 05:15:52 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt09_0_The_figure_presents_seven_distinct_architectural_p.png
01/22/2026 05:15:52 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 11/12: The figure presents a comparative analysis between a baseline method and the pro...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.15it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.65it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.53it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.49it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.46it/s][A
 21%|██▏       | 6/28 [00:03<00:15,  1.45it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.44it/s][A
 29%|██▊       | 8/28 [00:05<00:13,  1.43it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.43it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.42it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.42it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.42it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.42it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.42it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.42it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.42it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.42it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.42it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.41it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.41it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.41it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.41it/s][A
 82%|████████▏ | 23/28 [00:15<00:03,  1.41it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.41it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.41it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.41it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.41it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.41it/s][A100%|██████████| 28/28 [00:19<00:00,  1.43it/s]
01/22/2026 05:16:12 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt10_0_The_figure_presents_a_comparative_analysis_between.png
01/22/2026 05:16:12 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Generating image 12/12: The figure presents a conceptual comparison of four different point cloud comple...

  0%|          | 0/28 [00:00<?, ?it/s][A
  4%|▎         | 1/28 [00:00<00:12,  2.15it/s][A
  7%|▋         | 2/28 [00:01<00:15,  1.64it/s][A
 11%|█         | 3/28 [00:01<00:16,  1.53it/s][A
 14%|█▍        | 4/28 [00:02<00:16,  1.48it/s][A
 18%|█▊        | 5/28 [00:03<00:15,  1.46it/s][A
 21%|██▏       | 6/28 [00:03<00:15,  1.44it/s][A
 25%|██▌       | 7/28 [00:04<00:14,  1.44it/s][A
 29%|██▊       | 8/28 [00:05<00:14,  1.43it/s][A
 32%|███▏      | 9/28 [00:06<00:13,  1.42it/s][A
 36%|███▌      | 10/28 [00:06<00:12,  1.42it/s][A
 39%|███▉      | 11/28 [00:07<00:11,  1.42it/s][A
 43%|████▎     | 12/28 [00:08<00:11,  1.42it/s][A
 46%|████▋     | 13/28 [00:08<00:10,  1.42it/s][A
 50%|█████     | 14/28 [00:09<00:09,  1.42it/s][A
 54%|█████▎    | 15/28 [00:10<00:09,  1.42it/s][A
 57%|█████▋    | 16/28 [00:11<00:08,  1.42it/s][A
 61%|██████    | 17/28 [00:11<00:07,  1.42it/s][A
 64%|██████▍   | 18/28 [00:12<00:07,  1.41it/s][A
 68%|██████▊   | 19/28 [00:13<00:06,  1.41it/s][A
 71%|███████▏  | 20/28 [00:13<00:05,  1.41it/s][A
 75%|███████▌  | 21/28 [00:14<00:04,  1.41it/s][A
 79%|███████▊  | 22/28 [00:15<00:04,  1.41it/s][A
 82%|████████▏ | 23/28 [00:16<00:03,  1.41it/s][A
 86%|████████▌ | 24/28 [00:16<00:02,  1.41it/s][A
 89%|████████▉ | 25/28 [00:17<00:02,  1.41it/s][A
 93%|█████████▎| 26/28 [00:18<00:01,  1.41it/s][A
 96%|█████████▋| 27/28 [00:18<00:00,  1.41it/s][A
100%|██████████| 28/28 [00:19<00:00,  1.41it/s][A100%|██████████| 28/28 [00:19<00:00,  1.43it/s]
01/22/2026 05:16:35 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -     Saved to: /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500/step000500_prompt11_0_The_figure_presents_a_conceptual_comparison_of_fou.png
01/22/2026 05:16:35 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -   Moving VAE and text_encoder back to CPU...
01/22/2026 05:16:41 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - 
  ✅ Validation complete! Saved 12 images to:
01/22/2026 05:16:41 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func -      /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/validation_images/step_000500
01/22/2026 05:16:41 - INFO - OpenSciDraw.validation_funcs.Flux2Klein_fulltune_validation_func - ============================================================


[Step 500] Training Debug Info:
  Loss: 1.140504
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0011, std: 0.8711
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0008, std: 1.3281
  Model pred mean: -0.0038, std: 0.8008
  Sigmas: [0.2734375]... (timesteps: [274.0])

[Step 500] Training Debug Info:
  Loss: 0.405314
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0151, std: 0.9414
  Noise mean: -0.0031, std: 1.0000
  Target mean: -0.0182, std: 1.3672
  Model pred mean: -0.0188, std: 1.2266
  Sigmas: [0.7109375]... (timesteps: [709.0])

[Step 500] Training Debug Info:
  Loss: 0.683123
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0164, std: 0.8984
  Noise mean: -0.0013, std: 0.9961
  Target mean: 0.0150, std: 1.3359
  Model pred mean: 0.0161, std: 1.0703
  Sigmas: [0.52734375]... (timesteps: [526.0])

[Step 500] Training Debug Info:
  Loss: 0.350831
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: -0.0026, std: 0.9180
  Noise mean: 0.0006, std: 0.9961
  Target mean: 0.0032, std: 1.3516
  Model pred mean: 0.0000, std: 1.2188
  Sigmas: [0.8515625]... (timesteps: [852.0])
Steps:  10%|█         | 501/5000 [1:52:17<201:04:37, 160.90s/it, loss=0.3982, lr=1.00e-05]Steps:  10%|█         | 501/5000 [1:52:17<201:04:37, 160.90s/it, loss=0.3508, lr=1.00e-05]Steps:  10%|█         | 502/5000 [1:52:29<145:12:20, 116.22s/it, loss=0.3508, lr=1.00e-05]Steps:  10%|█         | 502/5000 [1:52:29<145:12:20, 116.22s/it, loss=0.4973, lr=1.00e-05]Steps:  10%|█         | 503/5000 [1:52:41<106:05:17, 84.93s/it, loss=0.4973, lr=1.00e-05] Steps:  10%|█         | 503/5000 [1:52:41<106:05:17, 84.93s/it, loss=0.3478, lr=1.00e-05]Steps:  10%|█         | 504/5000 [1:52:53<78:44:39, 63.05s/it, loss=0.3478, lr=1.00e-05] Steps:  10%|█         | 504/5000 [1:52:53<78:44:39, 63.05s/it, loss=0.4039, lr=1.00e-05]Steps:  10%|█         | 505/5000 [1:53:05<59:36:00, 47.73s/it, loss=0.4039, lr=1.00e-05]Steps:  10%|█         | 505/5000 [1:53:05<59:36:00, 47.73s/it, loss=0.6359, lr=1.00e-05]Steps:  10%|█         | 506/5000 [1:53:17<46:11:39, 37.00s/it, loss=0.6359, lr=1.00e-05]Steps:  10%|█         | 506/5000 [1:53:17<46:11:39, 37.00s/it, loss=0.7185, lr=1.00e-05]Steps:  10%|█         | 507/5000 [1:53:29<36:49:22, 29.50s/it, loss=0.7185, lr=1.00e-05]Steps:  10%|█         | 507/5000 [1:53:29<36:49:22, 29.50s/it, loss=0.8631, lr=1.00e-05]Steps:  10%|█         | 508/5000 [1:53:41<30:14:44, 24.24s/it, loss=0.8631, lr=1.00e-05]Steps:  10%|█         | 508/5000 [1:53:41<30:14:44, 24.24s/it, loss=0.4552, lr=1.00e-05]Steps:  10%|█         | 509/5000 [1:53:53<25:36:18, 20.53s/it, loss=0.4552, lr=1.00e-05]Steps:  10%|█         | 509/5000 [1:53:53<25:36:18, 20.53s/it, loss=0.5858, lr=1.00e-05]Steps:  10%|█         | 510/5000 [1:54:05<22:24:41, 17.97s/it, loss=0.5858, lr=1.00e-05]Steps:  10%|█         | 510/5000 [1:54:05<22:24:41, 17.97s/it, loss=0.3864, lr=1.00e-05]
[Step 510] Training Debug Info:
  Loss: 0.407343
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0016, std: 0.9570
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0025, std: 1.3828
  Model pred mean: 0.0047, std: 1.2188
  Sigmas: [0.87890625]... (timesteps: [879.0])

[Step 510] Training Debug Info:
  Loss: 0.489773
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0007, std: 0.9258
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0016, std: 1.3594
  Model pred mean: 0.0042, std: 1.1797
  Sigmas: [0.6953125]... (timesteps: [695.0])

[Step 510] Training Debug Info:
  Loss: 0.985569
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0010, std: 0.9023
  Noise mean: -0.0026, std: 1.0000
  Target mean: -0.0016, std: 1.3438
  Model pred mean: 0.0049, std: 0.9062
  Sigmas: [0.35546875]... (timesteps: [355.0])

[Step 510] Training Debug Info:
  Loss: 0.738199
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0006, std: 0.8984
  Noise mean: 0.0019, std: 1.0000
  Target mean: 0.0025, std: 1.3438
  Model pred mean: -0.0011, std: 1.0312
  Sigmas: [0.48828125]... (timesteps: [488.0])
Steps:  10%|█         | 511/5000 [1:54:17<20:09:56, 16.17s/it, loss=0.3864, lr=1.00e-05]Steps:  10%|█         | 511/5000 [1:54:17<20:09:56, 16.17s/it, loss=0.7382, lr=1.00e-05]Steps:  10%|█         | 512/5000 [1:54:29<18:35:42, 14.92s/it, loss=0.7382, lr=1.00e-05]Steps:  10%|█         | 512/5000 [1:54:29<18:35:42, 14.92s/it, loss=0.8769, lr=1.00e-05]Steps:  10%|█         | 513/5000 [1:54:41<17:30:32, 14.05s/it, loss=0.8769, lr=1.00e-05]Steps:  10%|█         | 513/5000 [1:54:41<17:30:32, 14.05s/it, loss=0.5414, lr=1.00e-05]Steps:  10%|█         | 514/5000 [1:54:53<16:42:30, 13.41s/it, loss=0.5414, lr=1.00e-05]Steps:  10%|█         | 514/5000 [1:54:53<16:42:30, 13.41s/it, loss=1.1204, lr=1.00e-05]Steps:  10%|█         | 515/5000 [1:55:04<16:09:21, 12.97s/it, loss=1.1204, lr=1.00e-05]Steps:  10%|█         | 515/5000 [1:55:04<16:09:21, 12.97s/it, loss=0.5728, lr=1.00e-05]Steps:  10%|█         | 516/5000 [1:55:16<15:45:08, 12.65s/it, loss=0.5728, lr=1.00e-05]Steps:  10%|█         | 516/5000 [1:55:16<15:45:08, 12.65s/it, loss=0.7975, lr=1.00e-05]Steps:  10%|█         | 517/5000 [1:55:28<15:28:30, 12.43s/it, loss=0.7975, lr=1.00e-05]Steps:  10%|█         | 517/5000 [1:55:28<15:28:30, 12.43s/it, loss=0.8295, lr=1.00e-05]Steps:  10%|█         | 518/5000 [1:55:40<15:17:26, 12.28s/it, loss=0.8295, lr=1.00e-05]Steps:  10%|█         | 518/5000 [1:55:40<15:17:26, 12.28s/it, loss=1.1179, lr=1.00e-05]Steps:  10%|█         | 519/5000 [1:55:52<15:07:46, 12.16s/it, loss=1.1179, lr=1.00e-05]Steps:  10%|█         | 519/5000 [1:55:52<15:07:46, 12.16s/it, loss=0.7901, lr=1.00e-05]Steps:  10%|█         | 520/5000 [1:56:04<15:01:33, 12.07s/it, loss=0.7901, lr=1.00e-05]Steps:  10%|█         | 520/5000 [1:56:04<15:01:33, 12.07s/it, loss=1.1779, lr=1.00e-05]
[Step 520] Training Debug Info:
  Loss: 0.612241
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0067, std: 0.9023
  Noise mean: -0.0032, std: 1.0000
  Target mean: -0.0099, std: 1.3438
  Model pred mean: -0.0065, std: 1.0859
  Sigmas: [0.57421875]... (timesteps: [575.0])

[Step 520] Training Debug Info:
  Loss: 0.487347
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0126, std: 0.9727
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0131, std: 1.3906
  Model pred mean: -0.0142, std: 1.2109
  Sigmas: [0.6171875]... (timesteps: [617.0])

[Step 520] Training Debug Info:
  Loss: 0.891004
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0013, std: 0.8984
  Noise mean: 0.0012, std: 1.0000
  Target mean: 0.0025, std: 1.3438
  Model pred mean: 0.0017, std: 0.9570
  Sigmas: [0.400390625]... (timesteps: [400.0])

[Step 520] Training Debug Info:
  Loss: 1.136777
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0041, std: 0.9219
  Noise mean: -0.0010, std: 1.0000
  Target mean: -0.0052, std: 1.3594
  Model pred mean: -0.0016, std: 0.8359
  Sigmas: [0.10888671875]... (timesteps: [109.0])
Steps:  10%|█         | 521/5000 [1:56:16<14:56:39, 12.01s/it, loss=1.1779, lr=1.00e-05]Steps:  10%|█         | 521/5000 [1:56:16<14:56:39, 12.01s/it, loss=1.1368, lr=1.00e-05]Steps:  10%|█         | 522/5000 [1:56:28<14:58:30, 12.04s/it, loss=1.1368, lr=1.00e-05]Steps:  10%|█         | 522/5000 [1:56:28<14:58:30, 12.04s/it, loss=0.4678, lr=1.00e-05]Steps:  10%|█         | 523/5000 [1:56:40<14:55:49, 12.01s/it, loss=0.4678, lr=1.00e-05]Steps:  10%|█         | 523/5000 [1:56:40<14:55:49, 12.01s/it, loss=1.1528, lr=1.00e-05]Steps:  10%|█         | 524/5000 [1:56:52<14:55:26, 12.00s/it, loss=1.1528, lr=1.00e-05]Steps:  10%|█         | 524/5000 [1:56:52<14:55:26, 12.00s/it, loss=0.8008, lr=1.00e-05]Steps:  10%|█         | 525/5000 [1:57:04<14:52:57, 11.97s/it, loss=0.8008, lr=1.00e-05]Steps:  10%|█         | 525/5000 [1:57:04<14:52:57, 11.97s/it, loss=0.6427, lr=1.00e-05]Steps:  11%|█         | 526/5000 [1:57:16<14:51:15, 11.95s/it, loss=0.6427, lr=1.00e-05]Steps:  11%|█         | 526/5000 [1:57:16<14:51:15, 11.95s/it, loss=1.0849, lr=1.00e-05]Steps:  11%|█         | 527/5000 [1:57:28<14:49:45, 11.94s/it, loss=1.0849, lr=1.00e-05]Steps:  11%|█         | 527/5000 [1:57:28<14:49:45, 11.94s/it, loss=0.5852, lr=1.00e-05]Steps:  11%|█         | 528/5000 [1:57:39<14:48:36, 11.92s/it, loss=0.5852, lr=1.00e-05]Steps:  11%|█         | 528/5000 [1:57:39<14:48:36, 11.92s/it, loss=1.0123, lr=1.00e-05]Steps:  11%|█         | 529/5000 [1:57:51<14:47:56, 11.92s/it, loss=1.0123, lr=1.00e-05]Steps:  11%|█         | 529/5000 [1:57:51<14:47:56, 11.92s/it, loss=1.1370, lr=1.00e-05]Steps:  11%|█         | 530/5000 [1:58:03<14:49:27, 11.94s/it, loss=1.1370, lr=1.00e-05]Steps:  11%|█         | 530/5000 [1:58:03<14:49:27, 11.94s/it, loss=0.8481, lr=1.00e-05]
[Step 530] Training Debug Info:
  Loss: 1.122345
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0206, std: 0.8906
  Noise mean: 0.0002, std: 1.0000
  Target mean: 0.0209, std: 1.3359
  Model pred mean: 0.0193, std: 0.8125
  Sigmas: [0.0751953125]... (timesteps: [75.0])

[Step 530] Training Debug Info:
  Loss: 1.071702
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0009, std: 0.8477
  Noise mean: 0.0027, std: 0.9961
  Target mean: 0.0036, std: 1.3047
  Model pred mean: 0.0058, std: 0.7891
  Sigmas: [0.0390625]... (timesteps: [39.0])

[Step 530] Training Debug Info:
  Loss: 1.060352
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0001, std: 0.9062
  Noise mean: -0.0034, std: 1.0000
  Target mean: -0.0033, std: 1.3516
  Model pred mean: 0.0020, std: 0.8711
  Sigmas: [0.322265625]... (timesteps: [323.0])

[Step 530] Training Debug Info:
  Loss: 0.748177
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0306, std: 0.8242
  Noise mean: 0.0000, std: 1.0000
  Target mean: 0.0306, std: 1.2969
  Model pred mean: 0.0359, std: 0.9609
  Sigmas: [0.5234375]... (timesteps: [525.0])
Steps:  11%|█         | 531/5000 [1:58:15<14:49:33, 11.94s/it, loss=0.8481, lr=1.00e-05]Steps:  11%|█         | 531/5000 [1:58:15<14:49:33, 11.94s/it, loss=0.7482, lr=1.00e-05]Steps:  11%|█         | 532/5000 [1:58:27<14:49:37, 11.95s/it, loss=0.7482, lr=1.00e-05]Steps:  11%|█         | 532/5000 [1:58:27<14:49:37, 11.95s/it, loss=1.1723, lr=1.00e-05]Steps:  11%|█         | 533/5000 [1:58:39<14:50:19, 11.96s/it, loss=1.1723, lr=1.00e-05]Steps:  11%|█         | 533/5000 [1:58:39<14:50:19, 11.96s/it, loss=1.1759, lr=1.00e-05]Steps:  11%|█         | 534/5000 [1:58:51<14:49:06, 11.94s/it, loss=1.1759, lr=1.00e-05]Steps:  11%|█         | 534/5000 [1:58:51<14:49:06, 11.94s/it, loss=0.9197, lr=1.00e-05]Steps:  11%|█         | 535/5000 [1:59:03<14:48:39, 11.94s/it, loss=0.9197, lr=1.00e-05]Steps:  11%|█         | 535/5000 [1:59:03<14:48:39, 11.94s/it, loss=0.5030, lr=1.00e-05]Steps:  11%|█         | 536/5000 [1:59:15<14:47:27, 11.93s/it, loss=0.5030, lr=1.00e-05]Steps:  11%|█         | 536/5000 [1:59:15<14:47:27, 11.93s/it, loss=1.0424, lr=1.00e-05]Steps:  11%|█         | 537/5000 [1:59:27<14:45:53, 11.91s/it, loss=1.0424, lr=1.00e-05]Steps:  11%|█         | 537/5000 [1:59:27<14:45:53, 11.91s/it, loss=0.6597, lr=1.00e-05]Steps:  11%|█         | 538/5000 [1:59:39<14:46:14, 11.92s/it, loss=0.6597, lr=1.00e-05]Steps:  11%|█         | 538/5000 [1:59:39<14:46:14, 11.92s/it, loss=0.4357, lr=1.00e-05]Steps:  11%|█         | 539/5000 [1:59:51<14:45:28, 11.91s/it, loss=0.4357, lr=1.00e-05]Steps:  11%|█         | 539/5000 [1:59:51<14:45:28, 11.91s/it, loss=1.1360, lr=1.00e-05]Steps:  11%|█         | 540/5000 [2:00:03<14:47:45, 11.94s/it, loss=1.1360, lr=1.00e-05]Steps:  11%|█         | 540/5000 [2:00:03<14:47:45, 11.94s/it, loss=0.4679, lr=1.00e-05]
[Step 540] Training Debug Info:
  Loss: 1.093343
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0165, std: 0.9180
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0167, std: 1.3594
  Model pred mean: -0.0153, std: 0.8594
  Sigmas: [0.11083984375]... (timesteps: [111.0])

[Step 540] Training Debug Info:
  Loss: 1.068252
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0273, std: 0.9688
  Noise mean: 0.0003, std: 1.0000
  Target mean: 0.0276, std: 1.3984
  Model pred mean: 0.0229, std: 0.9297
  Sigmas: [0.330078125]... (timesteps: [331.0])

[Step 540] Training Debug Info:
  Loss: 1.087555
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0581, std: 0.9414
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0579, std: 1.3750
  Model pred mean: -0.0581, std: 0.8906
  Sigmas: [0.10009765625]... (timesteps: [100.0])

[Step 540] Training Debug Info:
  Loss: 0.652478
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0010, std: 0.9141
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0000, std: 1.3594
  Model pred mean: -0.0023, std: 1.0859
  Sigmas: [0.5234375]... (timesteps: [524.0])
Steps:  11%|█         | 541/5000 [2:00:15<14:50:24, 11.98s/it, loss=0.4679, lr=1.00e-05]Steps:  11%|█         | 541/5000 [2:00:15<14:50:24, 11.98s/it, loss=0.6525, lr=1.00e-05]Steps:  11%|█         | 542/5000 [2:00:27<14:50:51, 11.99s/it, loss=0.6525, lr=1.00e-05]Steps:  11%|█         | 542/5000 [2:00:27<14:50:51, 11.99s/it, loss=1.1774, lr=1.00e-05]Steps:  11%|█         | 543/5000 [2:00:39<14:50:20, 11.99s/it, loss=1.1774, lr=1.00e-05]Steps:  11%|█         | 543/5000 [2:00:39<14:50:20, 11.99s/it, loss=0.3900, lr=1.00e-05]Steps:  11%|█         | 544/5000 [2:00:51<14:48:01, 11.96s/it, loss=0.3900, lr=1.00e-05]Steps:  11%|█         | 544/5000 [2:00:51<14:48:01, 11.96s/it, loss=1.0532, lr=1.00e-05]Steps:  11%|█         | 545/5000 [2:01:03<14:47:26, 11.95s/it, loss=1.0532, lr=1.00e-05]Steps:  11%|█         | 545/5000 [2:01:03<14:47:26, 11.95s/it, loss=0.6870, lr=1.00e-05]Steps:  11%|█         | 546/5000 [2:01:15<14:46:03, 11.94s/it, loss=0.6870, lr=1.00e-05]Steps:  11%|█         | 546/5000 [2:01:15<14:46:03, 11.94s/it, loss=0.9651, lr=1.00e-05]Steps:  11%|█         | 547/5000 [2:01:26<14:44:37, 11.92s/it, loss=0.9651, lr=1.00e-05]Steps:  11%|█         | 547/5000 [2:01:26<14:44:37, 11.92s/it, loss=0.7081, lr=1.00e-05]Steps:  11%|█         | 548/5000 [2:01:38<14:41:51, 11.88s/it, loss=0.7081, lr=1.00e-05]Steps:  11%|█         | 548/5000 [2:01:38<14:41:51, 11.88s/it, loss=0.5493, lr=1.00e-05]Steps:  11%|█         | 549/5000 [2:01:50<14:45:02, 11.93s/it, loss=0.5493, lr=1.00e-05]Steps:  11%|█         | 549/5000 [2:01:50<14:45:02, 11.93s/it, loss=0.6447, lr=1.00e-05]Steps:  11%|█         | 550/5000 [2:02:02<14:47:19, 11.96s/it, loss=0.6447, lr=1.00e-05]Steps:  11%|█         | 550/5000 [2:02:02<14:47:19, 11.96s/it, loss=0.4129, lr=1.00e-05]
[Step 550] Training Debug Info:
  Loss: 0.401441
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0120, std: 0.8828
  Noise mean: 0.0019, std: 1.0000
  Target mean: 0.0139, std: 1.3359
  Model pred mean: 0.0126, std: 1.1719
  Sigmas: [0.91015625]... (timesteps: [910.0])

[Step 550] Training Debug Info:
  Loss: 0.532387
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: -0.0025, std: 0.9453
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0042, std: 1.3750
  Model pred mean: 0.0029, std: 1.1641
  Sigmas: [0.62109375]... (timesteps: [622.0])

[Step 550] Training Debug Info:
  Loss: 0.355878
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0762, std: 0.9336
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0757, std: 1.3672
  Model pred mean: -0.0762, std: 1.2266
  Sigmas: [0.75]... (timesteps: [751.0])

[Step 550] Training Debug Info:
  Loss: 1.054890
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0199, std: 0.8867
  Noise mean: -0.0004, std: 1.0000
  Target mean: -0.0203, std: 1.3359
  Model pred mean: -0.0153, std: 0.8555
  Sigmas: [0.326171875]... (timesteps: [326.0])
Steps:  11%|█         | 551/5000 [2:02:14<14:47:10, 11.96s/it, loss=0.4129, lr=1.00e-05]Steps:  11%|█         | 551/5000 [2:02:14<14:47:10, 11.96s/it, loss=1.0549, lr=1.00e-05]Steps:  11%|█         | 552/5000 [2:02:26<14:45:10, 11.94s/it, loss=1.0549, lr=1.00e-05]Steps:  11%|█         | 552/5000 [2:02:26<14:45:10, 11.94s/it, loss=0.4980, lr=1.00e-05]Steps:  11%|█         | 553/5000 [2:02:38<14:43:48, 11.92s/it, loss=0.4980, lr=1.00e-05]Steps:  11%|█         | 553/5000 [2:02:38<14:43:48, 11.92s/it, loss=1.1079, lr=1.00e-05]Steps:  11%|█         | 554/5000 [2:02:50<14:42:39, 11.91s/it, loss=1.1079, lr=1.00e-05]Steps:  11%|█         | 554/5000 [2:02:50<14:42:39, 11.91s/it, loss=0.5839, lr=1.00e-05]Steps:  11%|█         | 555/5000 [2:03:02<14:41:43, 11.90s/it, loss=0.5839, lr=1.00e-05]Steps:  11%|█         | 555/5000 [2:03:02<14:41:43, 11.90s/it, loss=0.9914, lr=1.00e-05]Steps:  11%|█         | 556/5000 [2:03:14<14:40:54, 11.89s/it, loss=0.9914, lr=1.00e-05]Steps:  11%|█         | 556/5000 [2:03:14<14:40:54, 11.89s/it, loss=0.5911, lr=1.00e-05]Steps:  11%|█         | 557/5000 [2:03:25<14:39:34, 11.88s/it, loss=0.5911, lr=1.00e-05]Steps:  11%|█         | 557/5000 [2:03:25<14:39:34, 11.88s/it, loss=0.4714, lr=1.00e-05]Steps:  11%|█         | 558/5000 [2:03:37<14:41:28, 11.91s/it, loss=0.4714, lr=1.00e-05]Steps:  11%|█         | 558/5000 [2:03:37<14:41:28, 11.91s/it, loss=0.4374, lr=1.00e-05]Steps:  11%|█         | 559/5000 [2:03:49<14:41:01, 11.90s/it, loss=0.4374, lr=1.00e-05]Steps:  11%|█         | 559/5000 [2:03:49<14:41:01, 11.90s/it, loss=1.1427, lr=1.00e-05]Steps:  11%|█         | 560/5000 [2:04:01<14:40:59, 11.91s/it, loss=1.1427, lr=1.00e-05]Steps:  11%|█         | 560/5000 [2:04:01<14:40:59, 11.91s/it, loss=0.5091, lr=1.00e-05]
[Step 560] Training Debug Info:
  Loss: 1.031847
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0091, std: 0.8750
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0096, std: 1.3281
  Model pred mean: -0.0105, std: 0.8516
  Sigmas: [0.353515625]... (timesteps: [354.0])

[Step 560] Training Debug Info:
  Loss: 0.338232
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0116, std: 0.9219
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0118, std: 1.3594
  Model pred mean: -0.0030, std: 1.2266
  Sigmas: [0.83984375]... (timesteps: [839.0])

[Step 560] Training Debug Info:
  Loss: 0.959711
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0069, std: 0.9570
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0071, std: 1.3828
  Model pred mean: -0.0103, std: 0.9883
  Sigmas: [0.296875]... (timesteps: [296.0])

[Step 560] Training Debug Info:
  Loss: 0.491496
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0079, std: 0.9023
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0099, std: 1.3516
  Model pred mean: -0.0026, std: 1.1484
  Sigmas: [0.93359375]... (timesteps: [932.0])
Steps:  11%|█         | 561/5000 [2:04:13<14:40:17, 11.90s/it, loss=0.5091, lr=1.00e-05]Steps:  11%|█         | 561/5000 [2:04:13<14:40:17, 11.90s/it, loss=0.4915, lr=1.00e-05]Steps:  11%|█         | 562/5000 [2:04:25<14:39:34, 11.89s/it, loss=0.4915, lr=1.00e-05]Steps:  11%|█         | 562/5000 [2:04:25<14:39:34, 11.89s/it, loss=0.7672, lr=1.00e-05]Steps:  11%|█▏        | 563/5000 [2:04:37<14:38:48, 11.88s/it, loss=0.7672, lr=1.00e-05]Steps:  11%|█▏        | 563/5000 [2:04:37<14:38:48, 11.88s/it, loss=0.7441, lr=1.00e-05]Steps:  11%|█▏        | 564/5000 [2:04:49<14:40:05, 11.90s/it, loss=0.7441, lr=1.00e-05]Steps:  11%|█▏        | 564/5000 [2:04:49<14:40:05, 11.90s/it, loss=0.4779, lr=1.00e-05]Steps:  11%|█▏        | 565/5000 [2:05:01<14:40:08, 11.91s/it, loss=0.4779, lr=1.00e-05]Steps:  11%|█▏        | 565/5000 [2:05:01<14:40:08, 11.91s/it, loss=0.6464, lr=9.99e-06]Steps:  11%|█▏        | 566/5000 [2:05:13<14:39:18, 11.90s/it, loss=0.6464, lr=9.99e-06]Steps:  11%|█▏        | 566/5000 [2:05:13<14:39:18, 11.90s/it, loss=0.4645, lr=9.99e-06]Steps:  11%|█▏        | 567/5000 [2:05:25<14:41:14, 11.93s/it, loss=0.4645, lr=9.99e-06]Steps:  11%|█▏        | 567/5000 [2:05:25<14:41:14, 11.93s/it, loss=1.0374, lr=9.99e-06]Steps:  11%|█▏        | 568/5000 [2:05:37<14:41:18, 11.93s/it, loss=1.0374, lr=9.99e-06]Steps:  11%|█▏        | 568/5000 [2:05:37<14:41:18, 11.93s/it, loss=0.9740, lr=9.99e-06]Steps:  11%|█▏        | 569/5000 [2:05:48<14:40:54, 11.93s/it, loss=0.9740, lr=9.99e-06]Steps:  11%|█▏        | 569/5000 [2:05:48<14:40:54, 11.93s/it, loss=0.4460, lr=9.99e-06]Steps:  11%|█▏        | 570/5000 [2:06:00<14:40:04, 11.92s/it, loss=0.4460, lr=9.99e-06]Steps:  11%|█▏        | 570/5000 [2:06:00<14:40:04, 11.92s/it, loss=0.4424, lr=9.99e-06]
[Step 570] Training Debug Info:
  Loss: 0.725752
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0223, std: 0.9375
  Noise mean: -0.0012, std: 1.0000
  Target mean: -0.0236, std: 1.3672
  Model pred mean: -0.0359, std: 1.0703
  Sigmas: [0.9609375]... (timesteps: [961.0])

[Step 570] Training Debug Info:
  Loss: 0.725687
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0109, std: 0.9258
  Noise mean: -0.0016, std: 1.0078
  Target mean: -0.0125, std: 1.3672
  Model pred mean: -0.0109, std: 1.0625
  Sigmas: [0.5]... (timesteps: [500.0])

[Step 570] Training Debug Info:
  Loss: 0.693017
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0505, std: 0.9727
  Noise mean: 0.0014, std: 1.0000
  Target mean: -0.0493, std: 1.3984
  Model pred mean: -0.0500, std: 1.1172
  Sigmas: [0.427734375]... (timesteps: [428.0])

[Step 570] Training Debug Info:
  Loss: 0.362281
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0067, std: 0.9297
  Noise mean: -0.0010, std: 1.0000
  Target mean: -0.0077, std: 1.3672
  Model pred mean: -0.0114, std: 1.2188
  Sigmas: [0.79296875]... (timesteps: [793.0])
Steps:  11%|█▏        | 571/5000 [2:06:13<14:45:08, 11.99s/it, loss=0.4424, lr=9.99e-06]Steps:  11%|█▏        | 571/5000 [2:06:13<14:45:08, 11.99s/it, loss=0.3623, lr=9.99e-06]Steps:  11%|█▏        | 572/5000 [2:06:24<14:43:09, 11.97s/it, loss=0.3623, lr=9.99e-06]Steps:  11%|█▏        | 572/5000 [2:06:24<14:43:09, 11.97s/it, loss=1.1547, lr=9.99e-06]Steps:  11%|█▏        | 573/5000 [2:06:36<14:41:35, 11.95s/it, loss=1.1547, lr=9.99e-06]Steps:  11%|█▏        | 573/5000 [2:06:36<14:41:35, 11.95s/it, loss=0.4601, lr=9.99e-06]Steps:  11%|█▏        | 574/5000 [2:06:48<14:42:09, 11.96s/it, loss=0.4601, lr=9.99e-06]Steps:  11%|█▏        | 574/5000 [2:06:48<14:42:09, 11.96s/it, loss=0.4596, lr=9.99e-06]Steps:  12%|█▏        | 575/5000 [2:07:00<14:39:43, 11.93s/it, loss=0.4596, lr=9.99e-06]Steps:  12%|█▏        | 575/5000 [2:07:00<14:39:43, 11.93s/it, loss=0.7071, lr=9.99e-06]Steps:  12%|█▏        | 576/5000 [2:07:12<14:42:16, 11.97s/it, loss=0.7071, lr=9.99e-06]Steps:  12%|█▏        | 576/5000 [2:07:12<14:42:16, 11.97s/it, loss=0.4717, lr=9.99e-06]Steps:  12%|█▏        | 577/5000 [2:07:24<14:41:19, 11.96s/it, loss=0.4717, lr=9.99e-06]Steps:  12%|█▏        | 577/5000 [2:07:24<14:41:19, 11.96s/it, loss=1.0906, lr=9.99e-06]Steps:  12%|█▏        | 578/5000 [2:07:36<14:44:46, 12.01s/it, loss=1.0906, lr=9.99e-06]Steps:  12%|█▏        | 578/5000 [2:07:36<14:44:46, 12.01s/it, loss=0.8910, lr=9.99e-06]Steps:  12%|█▏        | 579/5000 [2:07:48<14:42:08, 11.97s/it, loss=0.8910, lr=9.99e-06]Steps:  12%|█▏        | 579/5000 [2:07:48<14:42:08, 11.97s/it, loss=0.3780, lr=9.99e-06]Steps:  12%|█▏        | 580/5000 [2:08:00<14:39:45, 11.94s/it, loss=0.3780, lr=9.99e-06]Steps:  12%|█▏        | 580/5000 [2:08:00<14:39:45, 11.94s/it, loss=0.4670, lr=9.99e-06]
[Step 580] Training Debug Info:
  Loss: 1.029519
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0053, std: 0.9297
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0058, std: 1.3672
  Model pred mean: -0.0107, std: 0.9102
  Sigmas: [0.010009765625]... (timesteps: [10.0])

[Step 580] Training Debug Info:
  Loss: 1.147335
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0027, std: 0.9102
  Noise mean: -0.0013, std: 1.0000
  Target mean: 0.0015, std: 1.3516
  Model pred mean: -0.0029, std: 0.8125
  Sigmas: [0.09423828125]... (timesteps: [94.0])

[Step 580] Training Debug Info:
  Loss: 1.215348
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0025, std: 0.8711
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0029, std: 1.3281
  Model pred mean: -0.0039, std: 0.7305
  Sigmas: [0.2099609375]... (timesteps: [210.0])

[Step 580] Training Debug Info:
  Loss: 0.754211
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0033, std: 0.9219
  Noise mean: 0.0011, std: 1.0000
  Target mean: -0.0022, std: 1.3594
  Model pred mean: 0.0007, std: 1.0703
  Sigmas: [0.47265625]... (timesteps: [472.0])
Steps:  12%|█▏        | 581/5000 [2:08:12<14:39:44, 11.94s/it, loss=0.4670, lr=9.99e-06]Steps:  12%|█▏        | 581/5000 [2:08:12<14:39:44, 11.94s/it, loss=0.7542, lr=9.99e-06]Steps:  12%|█▏        | 582/5000 [2:08:24<14:40:20, 11.96s/it, loss=0.7542, lr=9.99e-06]Steps:  12%|█▏        | 582/5000 [2:08:24<14:40:20, 11.96s/it, loss=0.4700, lr=9.99e-06]Steps:  12%|█▏        | 583/5000 [2:08:36<14:39:19, 11.94s/it, loss=0.4700, lr=9.99e-06]Steps:  12%|█▏        | 583/5000 [2:08:36<14:39:19, 11.94s/it, loss=1.1051, lr=9.99e-06]Steps:  12%|█▏        | 584/5000 [2:08:48<14:39:10, 11.95s/it, loss=1.1051, lr=9.99e-06]Steps:  12%|█▏        | 584/5000 [2:08:48<14:39:10, 11.95s/it, loss=0.6088, lr=9.99e-06]Steps:  12%|█▏        | 585/5000 [2:09:00<14:39:45, 11.96s/it, loss=0.6088, lr=9.99e-06]Steps:  12%|█▏        | 585/5000 [2:09:00<14:39:45, 11.96s/it, loss=0.9193, lr=9.99e-06]Steps:  12%|█▏        | 586/5000 [2:09:12<14:38:20, 11.94s/it, loss=0.9193, lr=9.99e-06]Steps:  12%|█▏        | 586/5000 [2:09:12<14:38:20, 11.94s/it, loss=0.9861, lr=9.99e-06]Steps:  12%|█▏        | 587/5000 [2:09:24<14:36:28, 11.92s/it, loss=0.9861, lr=9.99e-06]Steps:  12%|█▏        | 587/5000 [2:09:24<14:36:28, 11.92s/it, loss=0.5224, lr=9.99e-06]Steps:  12%|█▏        | 588/5000 [2:09:36<14:35:17, 11.90s/it, loss=0.5224, lr=9.99e-06]Steps:  12%|█▏        | 588/5000 [2:09:36<14:35:17, 11.90s/it, loss=1.1882, lr=9.99e-06]Steps:  12%|█▏        | 589/5000 [2:09:47<14:34:33, 11.90s/it, loss=1.1882, lr=9.99e-06]Steps:  12%|█▏        | 589/5000 [2:09:47<14:34:33, 11.90s/it, loss=1.1438, lr=9.99e-06]Steps:  12%|█▏        | 590/5000 [2:09:59<14:35:07, 11.91s/it, loss=1.1438, lr=9.99e-06]Steps:  12%|█▏        | 590/5000 [2:09:59<14:35:07, 11.91s/it, loss=0.4981, lr=9.99e-06]
[Step 590] Training Debug Info:
  Loss: 0.683509
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0349, std: 0.9141
  Noise mean: 0.0019, std: 1.0000
  Target mean: -0.0332, std: 1.3594
  Model pred mean: -0.0330, std: 1.0859
  Sigmas: [0.48046875]... (timesteps: [481.0])

[Step 590] Training Debug Info:
  Loss: 0.562155
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0173, std: 0.9102
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0168, std: 1.3516
  Model pred mean: -0.0183, std: 1.1328
  Sigmas: [0.609375]... (timesteps: [610.0])

[Step 590] Training Debug Info:
  Loss: 1.085346
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0074, std: 0.8984
  Noise mean: -0.0004, std: 1.0000
  Target mean: 0.0069, std: 1.3438
  Model pred mean: 0.0193, std: 0.8516
  Sigmas: [0.049072265625]... (timesteps: [49.0])

[Step 590] Training Debug Info:
  Loss: 1.092336
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0211, std: 0.8750
  Noise mean: -0.0009, std: 1.0000
  Target mean: 0.0201, std: 1.3281
  Model pred mean: 0.0317, std: 0.8203
  Sigmas: [0.049072265625]... (timesteps: [49.0])
Steps:  12%|█▏        | 591/5000 [2:10:11<14:35:45, 11.92s/it, loss=0.4981, lr=9.99e-06]Steps:  12%|█▏        | 591/5000 [2:10:11<14:35:45, 11.92s/it, loss=1.0923, lr=9.99e-06]Steps:  12%|█▏        | 592/5000 [2:10:23<14:35:37, 11.92s/it, loss=1.0923, lr=9.99e-06]Steps:  12%|█▏        | 592/5000 [2:10:23<14:35:37, 11.92s/it, loss=1.1061, lr=9.99e-06]Steps:  12%|█▏        | 593/5000 [2:10:35<14:34:12, 11.90s/it, loss=1.1061, lr=9.99e-06]Steps:  12%|█▏        | 593/5000 [2:10:35<14:34:12, 11.90s/it, loss=0.4495, lr=9.99e-06]Steps:  12%|█▏        | 594/5000 [2:10:47<14:35:47, 11.93s/it, loss=0.4495, lr=9.99e-06]Steps:  12%|█▏        | 594/5000 [2:10:47<14:35:47, 11.93s/it, loss=0.8933, lr=9.99e-06]Steps:  12%|█▏        | 595/5000 [2:10:59<14:33:36, 11.90s/it, loss=0.8933, lr=9.99e-06]Steps:  12%|█▏        | 595/5000 [2:10:59<14:33:36, 11.90s/it, loss=0.6024, lr=9.99e-06]Steps:  12%|█▏        | 596/5000 [2:11:11<14:32:25, 11.89s/it, loss=0.6024, lr=9.99e-06]Steps:  12%|█▏        | 596/5000 [2:11:11<14:32:25, 11.89s/it, loss=1.2063, lr=9.99e-06]Steps:  12%|█▏        | 597/5000 [2:11:23<14:33:15, 11.90s/it, loss=1.2063, lr=9.99e-06]Steps:  12%|█▏        | 597/5000 [2:11:23<14:33:15, 11.90s/it, loss=0.3871, lr=9.99e-06]Steps:  12%|█▏        | 598/5000 [2:11:35<14:34:12, 11.92s/it, loss=0.3871, lr=9.99e-06]Steps:  12%|█▏        | 598/5000 [2:11:35<14:34:12, 11.92s/it, loss=0.4779, lr=9.99e-06]Steps:  12%|█▏        | 599/5000 [2:11:47<14:33:52, 11.91s/it, loss=0.4779, lr=9.99e-06]Steps:  12%|█▏        | 599/5000 [2:11:47<14:33:52, 11.91s/it, loss=1.1385, lr=9.99e-06]Steps:  12%|█▏        | 600/5000 [2:11:58<14:34:32, 11.93s/it, loss=1.1385, lr=9.99e-06]Steps:  12%|█▏        | 600/5000 [2:11:58<14:34:32, 11.93s/it, loss=0.4011, lr=9.99e-06]01/22/2026 05:36:37 - INFO - __main__ - 
[Step 600] ✅ Loss in normal range (0.4011)
01/22/2026 05:36:37 - INFO - __main__ -   Loss avg (last 100): 0.7445
01/22/2026 05:36:37 - INFO - __main__ -   Loss range: [0.3478, 1.2063]

[Step 600] Training Debug Info:
  Loss: 0.392150
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0347, std: 0.9297
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0337, std: 1.3672
  Model pred mean: -0.0332, std: 1.2188
  Sigmas: [0.70703125]... (timesteps: [706.0])

[Step 600] Training Debug Info:
  Loss: 1.173880
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0006, std: 0.8984
  Noise mean: -0.0017, std: 1.0000
  Target mean: -0.0012, std: 1.3438
  Model pred mean: 0.0063, std: 0.8008
  Sigmas: [0.2001953125]... (timesteps: [200.0])

[Step 600] Training Debug Info:
  Loss: 0.978126
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0110, std: 0.9180
  Noise mean: -0.0009, std: 1.0000
  Target mean: 0.0101, std: 1.3594
  Model pred mean: 0.0166, std: 0.9531
  Sigmas: [0.337890625]... (timesteps: [337.0])

[Step 600] Training Debug Info:
  Loss: 0.459752
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0204, std: 0.8945
  Noise mean: -0.0002, std: 1.0000
  Target mean: 0.0203, std: 1.3438
  Model pred mean: 0.0225, std: 1.1797
  Sigmas: [0.69140625]... (timesteps: [693.0])
Steps:  12%|█▏        | 601/5000 [2:12:10<14:36:33, 11.96s/it, loss=0.4011, lr=9.99e-06]Steps:  12%|█▏        | 601/5000 [2:12:10<14:36:33, 11.96s/it, loss=0.4598, lr=9.99e-06]Steps:  12%|█▏        | 602/5000 [2:12:22<14:34:33, 11.93s/it, loss=0.4598, lr=9.99e-06]Steps:  12%|█▏        | 602/5000 [2:12:22<14:34:33, 11.93s/it, loss=0.7069, lr=9.99e-06]Steps:  12%|█▏        | 603/5000 [2:12:34<14:36:11, 11.96s/it, loss=0.7069, lr=9.99e-06]Steps:  12%|█▏        | 603/5000 [2:12:34<14:36:11, 11.96s/it, loss=0.4500, lr=9.99e-06]Steps:  12%|█▏        | 604/5000 [2:12:46<14:35:46, 11.95s/it, loss=0.4500, lr=9.99e-06]Steps:  12%|█▏        | 604/5000 [2:12:46<14:35:46, 11.95s/it, loss=1.1033, lr=9.99e-06]Steps:  12%|█▏        | 605/5000 [2:12:58<14:34:34, 11.94s/it, loss=1.1033, lr=9.99e-06]Steps:  12%|█▏        | 605/5000 [2:12:58<14:34:34, 11.94s/it, loss=0.3806, lr=9.99e-06]Steps:  12%|█▏        | 606/5000 [2:13:10<14:33:20, 11.93s/it, loss=0.3806, lr=9.99e-06]Steps:  12%|█▏        | 606/5000 [2:13:10<14:33:20, 11.93s/it, loss=1.1132, lr=9.99e-06]Steps:  12%|█▏        | 607/5000 [2:13:22<14:31:02, 11.90s/it, loss=1.1132, lr=9.99e-06]Steps:  12%|█▏        | 607/5000 [2:13:22<14:31:02, 11.90s/it, loss=0.5449, lr=9.99e-06]Steps:  12%|█▏        | 608/5000 [2:13:34<14:31:15, 11.90s/it, loss=0.5449, lr=9.99e-06]Steps:  12%|█▏        | 608/5000 [2:13:34<14:31:15, 11.90s/it, loss=1.1523, lr=9.99e-06]Steps:  12%|█▏        | 609/5000 [2:13:46<14:31:43, 11.91s/it, loss=1.1523, lr=9.99e-06]Steps:  12%|█▏        | 609/5000 [2:13:46<14:31:43, 11.91s/it, loss=0.9031, lr=9.99e-06]Steps:  12%|█▏        | 610/5000 [2:13:58<14:30:09, 11.89s/it, loss=0.9031, lr=9.99e-06]Steps:  12%|█▏        | 610/5000 [2:13:58<14:30:09, 11.89s/it, loss=0.8642, lr=9.99e-06]
[Step 610] Training Debug Info:
  Loss: 0.962816
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0027, std: 0.9336
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0012, std: 1.3672
  Model pred mean: -0.0019, std: 0.9688
  Sigmas: [0.330078125]... (timesteps: [330.0])

[Step 610] Training Debug Info:
  Loss: 0.743499
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0092, std: 0.9375
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0101, std: 1.3750
  Model pred mean: -0.0092, std: 1.0703
  Sigmas: [0.98828125]... (timesteps: [987.0])

[Step 610] Training Debug Info:
  Loss: 1.047825
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0007, std: 0.9023
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0013, std: 1.3438
  Model pred mean: 0.0001, std: 0.8906
  Sigmas: [0.322265625]... (timesteps: [323.0])

[Step 610] Training Debug Info:
  Loss: 0.386308
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0098, std: 0.8789
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0107, std: 1.3359
  Model pred mean: 0.0109, std: 1.1719
  Sigmas: [0.796875]... (timesteps: [795.0])
Steps:  12%|█▏        | 611/5000 [2:14:10<14:31:29, 11.91s/it, loss=0.8642, lr=9.99e-06]Steps:  12%|█▏        | 611/5000 [2:14:10<14:31:29, 11.91s/it, loss=0.3863, lr=9.98e-06]Steps:  12%|█▏        | 612/5000 [2:14:22<14:31:32, 11.92s/it, loss=0.3863, lr=9.98e-06]Steps:  12%|█▏        | 612/5000 [2:14:22<14:31:32, 11.92s/it, loss=0.7548, lr=9.98e-06]Steps:  12%|█▏        | 613/5000 [2:14:33<14:30:03, 11.90s/it, loss=0.7548, lr=9.98e-06]Steps:  12%|█▏        | 613/5000 [2:14:33<14:30:03, 11.90s/it, loss=0.7518, lr=9.98e-06]Steps:  12%|█▏        | 614/5000 [2:14:45<14:31:29, 11.92s/it, loss=0.7518, lr=9.98e-06]Steps:  12%|█▏        | 614/5000 [2:14:45<14:31:29, 11.92s/it, loss=1.1099, lr=9.98e-06]Steps:  12%|█▏        | 615/5000 [2:14:57<14:29:36, 11.90s/it, loss=1.1099, lr=9.98e-06]Steps:  12%|█▏        | 615/5000 [2:14:57<14:29:36, 11.90s/it, loss=0.6110, lr=9.98e-06]Steps:  12%|█▏        | 616/5000 [2:15:09<14:29:17, 11.90s/it, loss=0.6110, lr=9.98e-06]Steps:  12%|█▏        | 616/5000 [2:15:09<14:29:17, 11.90s/it, loss=0.4863, lr=9.98e-06]Steps:  12%|█▏        | 617/5000 [2:15:21<14:29:04, 11.90s/it, loss=0.4863, lr=9.98e-06]Steps:  12%|█▏        | 617/5000 [2:15:21<14:29:04, 11.90s/it, loss=0.7570, lr=9.98e-06]Steps:  12%|█▏        | 618/5000 [2:15:33<14:29:03, 11.90s/it, loss=0.7570, lr=9.98e-06]Steps:  12%|█▏        | 618/5000 [2:15:33<14:29:03, 11.90s/it, loss=0.9279, lr=9.98e-06]Steps:  12%|█▏        | 619/5000 [2:15:45<14:28:38, 11.90s/it, loss=0.9279, lr=9.98e-06]Steps:  12%|█▏        | 619/5000 [2:15:45<14:28:38, 11.90s/it, loss=1.1077, lr=9.98e-06]Steps:  12%|█▏        | 620/5000 [2:15:57<14:28:14, 11.89s/it, loss=1.1077, lr=9.98e-06]Steps:  12%|█▏        | 620/5000 [2:15:57<14:28:14, 11.89s/it, loss=0.4904, lr=9.98e-06]
[Step 620] Training Debug Info:
  Loss: 1.113354
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0170, std: 0.9258
  Noise mean: 0.0020, std: 1.0000
  Target mean: -0.0150, std: 1.3672
  Model pred mean: -0.0184, std: 0.8672
  Sigmas: [0.1923828125]... (timesteps: [192.0])

[Step 620] Training Debug Info:
  Loss: 0.508357
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0182, std: 0.9297
  Noise mean: -0.0007, std: 1.0000
  Target mean: -0.0189, std: 1.3672
  Model pred mean: -0.0150, std: 1.1797
  Sigmas: [0.6640625]... (timesteps: [664.0])

[Step 620] Training Debug Info:
  Loss: 0.428554
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0583, std: 0.9414
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0579, std: 1.3750
  Model pred mean: -0.0566, std: 1.2109
  Sigmas: [0.64453125]... (timesteps: [645.0])

[Step 620] Training Debug Info:
  Loss: 0.439478
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0130, std: 0.8984
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0151, std: 1.3438
  Model pred mean: -0.0080, std: 1.1719
  Sigmas: [0.85546875]... (timesteps: [855.0])
Steps:  12%|█▏        | 621/5000 [2:16:09<14:31:28, 11.94s/it, loss=0.4904, lr=9.98e-06]Steps:  12%|█▏        | 621/5000 [2:16:09<14:31:28, 11.94s/it, loss=0.4395, lr=9.98e-06]Steps:  12%|█▏        | 622/5000 [2:16:21<14:30:43, 11.93s/it, loss=0.4395, lr=9.98e-06]Steps:  12%|█▏        | 622/5000 [2:16:21<14:30:43, 11.93s/it, loss=0.8333, lr=9.98e-06]Steps:  12%|█▏        | 623/5000 [2:16:33<14:29:39, 11.92s/it, loss=0.8333, lr=9.98e-06]Steps:  12%|█▏        | 623/5000 [2:16:33<14:29:39, 11.92s/it, loss=0.4638, lr=9.98e-06]Steps:  12%|█▏        | 624/5000 [2:16:44<14:27:24, 11.89s/it, loss=0.4638, lr=9.98e-06]Steps:  12%|█▏        | 624/5000 [2:16:44<14:27:24, 11.89s/it, loss=0.9784, lr=9.98e-06]Steps:  12%|█▎        | 625/5000 [2:16:56<14:25:48, 11.87s/it, loss=0.9784, lr=9.98e-06]Steps:  12%|█▎        | 625/5000 [2:16:56<14:25:48, 11.87s/it, loss=0.4696, lr=9.98e-06]Steps:  13%|█▎        | 626/5000 [2:17:08<14:26:31, 11.89s/it, loss=0.4696, lr=9.98e-06]Steps:  13%|█▎        | 626/5000 [2:17:08<14:26:31, 11.89s/it, loss=0.4329, lr=9.98e-06]Steps:  13%|█▎        | 627/5000 [2:17:20<14:27:35, 11.90s/it, loss=0.4329, lr=9.98e-06]Steps:  13%|█▎        | 627/5000 [2:17:20<14:27:35, 11.90s/it, loss=0.5302, lr=9.98e-06]Steps:  13%|█▎        | 628/5000 [2:17:32<14:26:43, 11.89s/it, loss=0.5302, lr=9.98e-06]Steps:  13%|█▎        | 628/5000 [2:17:32<14:26:43, 11.89s/it, loss=1.0446, lr=9.98e-06]Steps:  13%|█▎        | 629/5000 [2:17:44<14:26:30, 11.89s/it, loss=1.0446, lr=9.98e-06]Steps:  13%|█▎        | 629/5000 [2:17:44<14:26:30, 11.89s/it, loss=1.0728, lr=9.98e-06]Steps:  13%|█▎        | 630/5000 [2:17:56<14:27:35, 11.91s/it, loss=1.0728, lr=9.98e-06]Steps:  13%|█▎        | 630/5000 [2:17:56<14:27:35, 11.91s/it, loss=0.4521, lr=9.98e-06]
[Step 630] Training Debug Info:
  Loss: 0.470421
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0461, std: 0.9727
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0459, std: 1.3984
  Model pred mean: -0.0488, std: 1.2109
  Sigmas: [0.82421875]... (timesteps: [825.0])

[Step 630] Training Debug Info:
  Loss: 1.101320
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0075, std: 0.9375
  Noise mean: 0.0005, std: 1.0000
  Target mean: 0.0080, std: 1.3750
  Model pred mean: 0.0059, std: 0.8828
  Sigmas: [0.0888671875]... (timesteps: [89.0])

[Step 630] Training Debug Info:
  Loss: 0.590096
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0302, std: 0.9219
  Noise mean: -0.0003, std: 1.0000
  Target mean: -0.0305, std: 1.3594
  Model pred mean: -0.0276, std: 1.1328
  Sigmas: [0.546875]... (timesteps: [548.0])

[Step 630] Training Debug Info:
  Loss: 1.022921
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0250, std: 0.9219
  Noise mean: -0.0020, std: 1.0000
  Target mean: -0.0271, std: 1.3594
  Model pred mean: -0.0308, std: 0.9180
  Sigmas: [0.006988525390625]... (timesteps: [7.0])
Steps:  13%|█▎        | 631/5000 [2:18:08<14:27:46, 11.92s/it, loss=0.4521, lr=9.98e-06]Steps:  13%|█▎        | 631/5000 [2:18:08<14:27:46, 11.92s/it, loss=1.0229, lr=9.98e-06]Steps:  13%|█▎        | 632/5000 [2:18:20<14:27:41, 11.92s/it, loss=1.0229, lr=9.98e-06]Steps:  13%|█▎        | 632/5000 [2:18:20<14:27:41, 11.92s/it, loss=1.0841, lr=9.98e-06]Steps:  13%|█▎        | 633/5000 [2:18:31<14:26:01, 11.90s/it, loss=1.0841, lr=9.98e-06]Steps:  13%|█▎        | 633/5000 [2:18:31<14:26:01, 11.90s/it, loss=0.4103, lr=9.98e-06]Steps:  13%|█▎        | 634/5000 [2:18:43<14:24:27, 11.88s/it, loss=0.4103, lr=9.98e-06]Steps:  13%|█▎        | 634/5000 [2:18:43<14:24:27, 11.88s/it, loss=1.1879, lr=9.98e-06]Steps:  13%|█▎        | 635/5000 [2:18:55<14:23:41, 11.87s/it, loss=1.1879, lr=9.98e-06]Steps:  13%|█▎        | 635/5000 [2:18:55<14:23:41, 11.87s/it, loss=1.0931, lr=9.98e-06]Steps:  13%|█▎        | 636/5000 [2:19:07<14:23:30, 11.87s/it, loss=1.0931, lr=9.98e-06]Steps:  13%|█▎        | 636/5000 [2:19:07<14:23:30, 11.87s/it, loss=0.6428, lr=9.98e-06]Steps:  13%|█▎        | 637/5000 [2:19:19<14:25:01, 11.90s/it, loss=0.6428, lr=9.98e-06]Steps:  13%|█▎        | 637/5000 [2:19:19<14:25:01, 11.90s/it, loss=1.0832, lr=9.98e-06]Steps:  13%|█▎        | 638/5000 [2:19:31<14:24:05, 11.89s/it, loss=1.0832, lr=9.98e-06]Steps:  13%|█▎        | 638/5000 [2:19:31<14:24:05, 11.89s/it, loss=0.4892, lr=9.98e-06]Steps:  13%|█▎        | 639/5000 [2:19:43<14:26:27, 11.92s/it, loss=0.4892, lr=9.98e-06]Steps:  13%|█▎        | 639/5000 [2:19:43<14:26:27, 11.92s/it, loss=1.0822, lr=9.98e-06]Steps:  13%|█▎        | 640/5000 [2:19:55<14:26:14, 11.92s/it, loss=1.0822, lr=9.98e-06]Steps:  13%|█▎        | 640/5000 [2:19:55<14:26:14, 11.92s/it, loss=1.1776, lr=9.98e-06]
[Step 640] Training Debug Info:
  Loss: 1.166898
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0109, std: 0.9844
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0108, std: 1.4062
  Model pred mean: -0.0142, std: 0.9023
  Sigmas: [0.1689453125]... (timesteps: [169.0])

[Step 640] Training Debug Info:
  Loss: 0.738402
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0298, std: 0.9062
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0295, std: 1.3516
  Model pred mean: -0.0272, std: 1.0391
  Sigmas: [0.419921875]... (timesteps: [420.0])

[Step 640] Training Debug Info:
  Loss: 0.447411
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0240, std: 0.8945
  Noise mean: -0.0003, std: 1.0000
  Target mean: 0.0237, std: 1.3438
  Model pred mean: 0.0251, std: 1.1641
  Sigmas: [0.73046875]... (timesteps: [730.0])

[Step 640] Training Debug Info:
  Loss: 0.950143
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0020, std: 0.8906
  Noise mean: 0.0025, std: 1.0000
  Target mean: 0.0045, std: 1.3438
  Model pred mean: 0.0030, std: 0.9180
  Sigmas: [0.39453125]... (timesteps: [394.0])
Steps:  13%|█▎        | 641/5000 [2:20:07<14:23:25, 11.88s/it, loss=1.1776, lr=9.98e-06]Steps:  13%|█▎        | 641/5000 [2:20:07<14:23:25, 11.88s/it, loss=0.9501, lr=9.98e-06]Steps:  13%|█▎        | 642/5000 [2:20:19<14:25:09, 11.91s/it, loss=0.9501, lr=9.98e-06]Steps:  13%|█▎        | 642/5000 [2:20:19<14:25:09, 11.91s/it, loss=0.7777, lr=9.98e-06]Steps:  13%|█▎        | 643/5000 [2:20:30<14:23:19, 11.89s/it, loss=0.7777, lr=9.98e-06]Steps:  13%|█▎        | 643/5000 [2:20:30<14:23:19, 11.89s/it, loss=1.0960, lr=9.98e-06]Steps:  13%|█▎        | 644/5000 [2:20:42<14:23:36, 11.90s/it, loss=1.0960, lr=9.98e-06]Steps:  13%|█▎        | 644/5000 [2:20:42<14:23:36, 11.90s/it, loss=0.7320, lr=9.97e-06]Steps:  13%|█▎        | 645/5000 [2:20:54<14:23:35, 11.90s/it, loss=0.7320, lr=9.97e-06]Steps:  13%|█▎        | 645/5000 [2:20:54<14:23:35, 11.90s/it, loss=0.6887, lr=9.97e-06]Steps:  13%|█▎        | 646/5000 [2:21:06<14:23:50, 11.90s/it, loss=0.6887, lr=9.97e-06]Steps:  13%|█▎        | 646/5000 [2:21:06<14:23:50, 11.90s/it, loss=0.6026, lr=9.97e-06]Steps:  13%|█▎        | 647/5000 [2:21:18<14:22:49, 11.89s/it, loss=0.6026, lr=9.97e-06]Steps:  13%|█▎        | 647/5000 [2:21:18<14:22:49, 11.89s/it, loss=0.3936, lr=9.97e-06]Steps:  13%|█▎        | 648/5000 [2:21:30<14:25:54, 11.94s/it, loss=0.3936, lr=9.97e-06]Steps:  13%|█▎        | 648/5000 [2:21:30<14:25:54, 11.94s/it, loss=0.9079, lr=9.97e-06]Steps:  13%|█▎        | 649/5000 [2:21:42<14:25:17, 11.93s/it, loss=0.9079, lr=9.97e-06]Steps:  13%|█▎        | 649/5000 [2:21:42<14:25:17, 11.93s/it, loss=1.2034, lr=9.97e-06]Steps:  13%|█▎        | 650/5000 [2:21:54<14:24:57, 11.93s/it, loss=1.2034, lr=9.97e-06]Steps:  13%|█▎        | 650/5000 [2:21:54<14:24:57, 11.93s/it, loss=0.4452, lr=9.97e-06]
[Step 650] Training Debug Info:
  Loss: 0.885720
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0493, std: 0.9336
  Noise mean: 0.0027, std: 1.0000
  Target mean: -0.0466, std: 1.3672
  Model pred mean: -0.0498, std: 0.9922
  Sigmas: [0.330078125]... (timesteps: [330.0])

[Step 650] Training Debug Info:
  Loss: 1.186467
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0168, std: 0.8945
  Noise mean: -0.0016, std: 1.0000
  Target mean: 0.0152, std: 1.3438
  Model pred mean: 0.0145, std: 0.7773
  Sigmas: [0.150390625]... (timesteps: [150.0])

[Step 650] Training Debug Info:
  Loss: 0.490708
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0131, std: 0.9336
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0136, std: 1.3672
  Model pred mean: -0.0172, std: 1.1719
  Sigmas: [0.6953125]... (timesteps: [696.0])

[Step 650] Training Debug Info:
  Loss: 1.135213
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0056, std: 0.8750
  Noise mean: 0.0026, std: 1.0000
  Target mean: -0.0030, std: 1.3281
  Model pred mean: -0.0070, std: 0.7969
  Sigmas: [0.0869140625]... (timesteps: [87.0])
Steps:  13%|█▎        | 651/5000 [2:22:06<14:25:16, 11.94s/it, loss=0.4452, lr=9.97e-06]Steps:  13%|█▎        | 651/5000 [2:22:06<14:25:16, 11.94s/it, loss=1.1352, lr=9.97e-06]Steps:  13%|█▎        | 652/5000 [2:22:18<14:24:25, 11.93s/it, loss=1.1352, lr=9.97e-06]Steps:  13%|█▎        | 652/5000 [2:22:18<14:24:25, 11.93s/it, loss=0.5245, lr=9.97e-06]Steps:  13%|█▎        | 653/5000 [2:22:30<14:24:10, 11.93s/it, loss=0.5245, lr=9.97e-06]Steps:  13%|█▎        | 653/5000 [2:22:30<14:24:10, 11.93s/it, loss=0.4377, lr=9.97e-06]Steps:  13%|█▎        | 654/5000 [2:22:42<14:24:44, 11.94s/it, loss=0.4377, lr=9.97e-06]Steps:  13%|█▎        | 654/5000 [2:22:42<14:24:44, 11.94s/it, loss=1.0078, lr=9.97e-06]Steps:  13%|█▎        | 655/5000 [2:22:54<14:23:29, 11.92s/it, loss=1.0078, lr=9.97e-06]Steps:  13%|█▎        | 655/5000 [2:22:54<14:23:29, 11.92s/it, loss=0.3678, lr=9.97e-06]Steps:  13%|█▎        | 656/5000 [2:23:05<14:22:13, 11.91s/it, loss=0.3678, lr=9.97e-06]Steps:  13%|█▎        | 656/5000 [2:23:05<14:22:13, 11.91s/it, loss=1.0788, lr=9.97e-06]Steps:  13%|█▎        | 657/5000 [2:23:17<14:24:20, 11.94s/it, loss=1.0788, lr=9.97e-06]Steps:  13%|█▎        | 657/5000 [2:23:17<14:24:20, 11.94s/it, loss=0.3462, lr=9.97e-06]Steps:  13%|█▎        | 658/5000 [2:23:29<14:22:23, 11.92s/it, loss=0.3462, lr=9.97e-06]Steps:  13%|█▎        | 658/5000 [2:23:29<14:22:23, 11.92s/it, loss=1.0409, lr=9.97e-06]Steps:  13%|█▎        | 659/5000 [2:23:41<14:22:09, 11.92s/it, loss=1.0409, lr=9.97e-06]Steps:  13%|█▎        | 659/5000 [2:23:41<14:22:09, 11.92s/it, loss=0.8580, lr=9.97e-06]Steps:  13%|█▎        | 660/5000 [2:23:53<14:21:15, 11.91s/it, loss=0.8580, lr=9.97e-06]Steps:  13%|█▎        | 660/5000 [2:23:53<14:21:15, 11.91s/it, loss=0.6977, lr=9.97e-06]
[Step 660] Training Debug Info:
  Loss: 1.108562
  Latent shape: torch.Size([1, 32, 78, 108]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: -0.0024, std: 0.9570
  Noise mean: -0.0014, std: 1.0000
  Target mean: 0.0010, std: 1.3828
  Model pred mean: 0.0006, std: 0.8984
  Sigmas: [0.1572265625]... (timesteps: [157.0])

[Step 660] Training Debug Info:
  Loss: 1.088600
  Latent shape: torch.Size([1, 32, 108, 84]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0057, std: 0.8984
  Noise mean: -0.0009, std: 1.0000
  Target mean: -0.0066, std: 1.3438
  Model pred mean: -0.0064, std: 0.8477
  Sigmas: [0.3046875]... (timesteps: [305.0])

[Step 660] Training Debug Info:
  Loss: 0.353341
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0349, std: 0.9141
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0352, std: 1.3516
  Model pred mean: -0.0294, std: 1.2109
  Sigmas: [0.82421875]... (timesteps: [824.0])

[Step 660] Training Debug Info:
  Loss: 0.876317
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0201, std: 0.9102
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0208, std: 1.3516
  Model pred mean: -0.0201, std: 0.9844
  Sigmas: [0.37109375]... (timesteps: [371.0])
Steps:  13%|█▎        | 661/5000 [2:24:05<14:19:54, 11.89s/it, loss=0.6977, lr=9.97e-06]Steps:  13%|█▎        | 661/5000 [2:24:05<14:19:54, 11.89s/it, loss=0.8763, lr=9.97e-06]Steps:  13%|█▎        | 662/5000 [2:24:17<14:20:14, 11.90s/it, loss=0.8763, lr=9.97e-06]Steps:  13%|█▎        | 662/5000 [2:24:17<14:20:14, 11.90s/it, loss=0.4639, lr=9.97e-06]Steps:  13%|█▎        | 663/5000 [2:24:29<14:20:14, 11.90s/it, loss=0.4639, lr=9.97e-06]Steps:  13%|█▎        | 663/5000 [2:24:29<14:20:14, 11.90s/it, loss=0.4744, lr=9.97e-06]Steps:  13%|█▎        | 664/5000 [2:24:41<14:19:17, 11.89s/it, loss=0.4744, lr=9.97e-06]Steps:  13%|█▎        | 664/5000 [2:24:41<14:19:17, 11.89s/it, loss=0.3795, lr=9.97e-06]Steps:  13%|█▎        | 665/5000 [2:24:53<14:19:05, 11.89s/it, loss=0.3795, lr=9.97e-06]Steps:  13%|█▎        | 665/5000 [2:24:53<14:19:05, 11.89s/it, loss=0.5155, lr=9.97e-06]Steps:  13%|█▎        | 666/5000 [2:25:05<14:21:19, 11.92s/it, loss=0.5155, lr=9.97e-06]Steps:  13%|█▎        | 666/5000 [2:25:05<14:21:19, 11.92s/it, loss=0.6284, lr=9.97e-06]Steps:  13%|█▎        | 667/5000 [2:25:16<14:20:55, 11.92s/it, loss=0.6284, lr=9.97e-06]Steps:  13%|█▎        | 667/5000 [2:25:16<14:20:55, 11.92s/it, loss=0.4387, lr=9.97e-06]Steps:  13%|█▎        | 668/5000 [2:25:28<14:19:58, 11.91s/it, loss=0.4387, lr=9.97e-06]Steps:  13%|█▎        | 668/5000 [2:25:28<14:19:58, 11.91s/it, loss=0.3698, lr=9.97e-06]Steps:  13%|█▎        | 669/5000 [2:25:40<14:21:41, 11.94s/it, loss=0.3698, lr=9.97e-06]Steps:  13%|█▎        | 669/5000 [2:25:40<14:21:41, 11.94s/it, loss=0.5038, lr=9.97e-06]Steps:  13%|█▎        | 670/5000 [2:25:52<14:21:40, 11.94s/it, loss=0.5038, lr=9.97e-06]Steps:  13%|█▎        | 670/5000 [2:25:52<14:21:40, 11.94s/it, loss=0.3519, lr=9.96e-06]
[Step 670] Training Debug Info:
  Loss: 0.578008
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0417, std: 0.9102
  Noise mean: 0.0021, std: 1.0000
  Target mean: -0.0398, std: 1.3516
  Model pred mean: -0.0378, std: 1.1094
  Sigmas: [0.5703125]... (timesteps: [569.0])

[Step 670] Training Debug Info:
  Loss: 0.915123
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0058, std: 0.8789
  Noise mean: -0.0016, std: 1.0000
  Target mean: -0.0074, std: 1.3281
  Model pred mean: -0.0053, std: 0.9258
  Sigmas: [0.40625]... (timesteps: [407.0])

[Step 670] Training Debug Info:
  Loss: 0.435715
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0236, std: 0.9023
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0234, std: 1.3438
  Model pred mean: -0.0200, std: 1.1719
  Sigmas: [0.72265625]... (timesteps: [723.0])

[Step 670] Training Debug Info:
  Loss: 1.066118
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0130, std: 0.9609
  Noise mean: 0.0016, std: 1.0000
  Target mean: 0.0146, std: 1.3906
  Model pred mean: 0.0128, std: 0.9336
  Sigmas: [0.041015625]... (timesteps: [41.0])
Steps:  13%|█▎        | 671/5000 [2:26:04<14:20:28, 11.93s/it, loss=0.3519, lr=9.96e-06]Steps:  13%|█▎        | 671/5000 [2:26:04<14:20:28, 11.93s/it, loss=1.0661, lr=9.96e-06]Steps:  13%|█▎        | 672/5000 [2:26:16<14:20:55, 11.94s/it, loss=1.0661, lr=9.96e-06]Steps:  13%|█▎        | 672/5000 [2:26:16<14:20:55, 11.94s/it, loss=0.9982, lr=9.96e-06]Steps:  13%|█▎        | 673/5000 [2:26:28<14:19:18, 11.92s/it, loss=0.9982, lr=9.96e-06]Steps:  13%|█▎        | 673/5000 [2:26:28<14:19:18, 11.92s/it, loss=0.4170, lr=9.96e-06]Steps:  13%|█▎        | 674/5000 [2:26:40<14:20:10, 11.93s/it, loss=0.4170, lr=9.96e-06]Steps:  13%|█▎        | 674/5000 [2:26:40<14:20:10, 11.93s/it, loss=0.4746, lr=9.96e-06]Steps:  14%|█▎        | 675/5000 [2:26:52<14:20:11, 11.93s/it, loss=0.4746, lr=9.96e-06]Steps:  14%|█▎        | 675/5000 [2:26:52<14:20:11, 11.93s/it, loss=0.9109, lr=9.96e-06]Steps:  14%|█▎        | 676/5000 [2:27:04<14:20:18, 11.94s/it, loss=0.9109, lr=9.96e-06]Steps:  14%|█▎        | 676/5000 [2:27:04<14:20:18, 11.94s/it, loss=0.7969, lr=9.96e-06]Steps:  14%|█▎        | 677/5000 [2:27:16<14:18:57, 11.92s/it, loss=0.7969, lr=9.96e-06]Steps:  14%|█▎        | 677/5000 [2:27:16<14:18:57, 11.92s/it, loss=0.3650, lr=9.96e-06]Steps:  14%|█▎        | 678/5000 [2:27:28<14:20:08, 11.94s/it, loss=0.3650, lr=9.96e-06]Steps:  14%|█▎        | 678/5000 [2:27:28<14:20:08, 11.94s/it, loss=1.0181, lr=9.96e-06]Steps:  14%|█▎        | 679/5000 [2:27:40<14:18:03, 11.91s/it, loss=1.0181, lr=9.96e-06]Steps:  14%|█▎        | 679/5000 [2:27:40<14:18:03, 11.91s/it, loss=1.0258, lr=9.96e-06]Steps:  14%|█▎        | 680/5000 [2:27:51<14:16:27, 11.90s/it, loss=1.0258, lr=9.96e-06]Steps:  14%|█▎        | 680/5000 [2:27:51<14:16:27, 11.90s/it, loss=1.0917, lr=9.96e-06]
[Step 680] Training Debug Info:
  Loss: 1.111420
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0052, std: 0.9297
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0058, std: 1.3672
  Model pred mean: -0.0045, std: 0.8672
  Sigmas: [0.07080078125]... (timesteps: [71.0])

[Step 680] Training Debug Info:
  Loss: 0.362865
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0113, std: 0.9570
  Noise mean: 0.0036, std: 1.0000
  Target mean: -0.0078, std: 1.3828
  Model pred mean: -0.0119, std: 1.2422
  Sigmas: [0.734375]... (timesteps: [736.0])

[Step 680] Training Debug Info:
  Loss: 0.742960
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0072, std: 0.8672
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0069, std: 1.3203
  Model pred mean: -0.0071, std: 0.9961
  Sigmas: [0.51953125]... (timesteps: [521.0])

[Step 680] Training Debug Info:
  Loss: 0.392770
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0091, std: 0.9336
  Noise mean: -0.0019, std: 1.0000
  Target mean: 0.0072, std: 1.3672
  Model pred mean: 0.0092, std: 1.2109
  Sigmas: [0.75390625]... (timesteps: [752.0])
Steps:  14%|█▎        | 681/5000 [2:28:03<14:16:28, 11.90s/it, loss=1.0917, lr=9.96e-06]Steps:  14%|█▎        | 681/5000 [2:28:03<14:16:28, 11.90s/it, loss=0.3928, lr=9.96e-06]Steps:  14%|█▎        | 682/5000 [2:28:15<14:16:44, 11.90s/it, loss=0.3928, lr=9.96e-06]Steps:  14%|█▎        | 682/5000 [2:28:15<14:16:44, 11.90s/it, loss=0.5513, lr=9.96e-06]Steps:  14%|█▎        | 683/5000 [2:28:27<14:17:25, 11.92s/it, loss=0.5513, lr=9.96e-06]Steps:  14%|█▎        | 683/5000 [2:28:27<14:17:25, 11.92s/it, loss=1.1190, lr=9.96e-06]Steps:  14%|█▎        | 684/5000 [2:28:39<14:17:40, 11.92s/it, loss=1.1190, lr=9.96e-06]Steps:  14%|█▎        | 684/5000 [2:28:39<14:17:40, 11.92s/it, loss=1.1061, lr=9.96e-06]Steps:  14%|█▎        | 685/5000 [2:28:51<14:17:29, 11.92s/it, loss=1.1061, lr=9.96e-06]Steps:  14%|█▎        | 685/5000 [2:28:51<14:17:29, 11.92s/it, loss=0.4223, lr=9.96e-06]Steps:  14%|█▎        | 686/5000 [2:29:03<14:18:02, 11.93s/it, loss=0.4223, lr=9.96e-06]Steps:  14%|█▎        | 686/5000 [2:29:03<14:18:02, 11.93s/it, loss=0.8746, lr=9.96e-06]Steps:  14%|█▎        | 687/5000 [2:29:15<14:17:24, 11.93s/it, loss=0.8746, lr=9.96e-06]Steps:  14%|█▎        | 687/5000 [2:29:15<14:17:24, 11.93s/it, loss=1.1046, lr=9.96e-06]Steps:  14%|█▍        | 688/5000 [2:29:27<14:15:45, 11.91s/it, loss=1.1046, lr=9.96e-06]Steps:  14%|█▍        | 688/5000 [2:29:27<14:15:45, 11.91s/it, loss=1.0455, lr=9.96e-06]Steps:  14%|█▍        | 689/5000 [2:29:39<14:15:30, 11.91s/it, loss=1.0455, lr=9.96e-06]Steps:  14%|█▍        | 689/5000 [2:29:39<14:15:30, 11.91s/it, loss=1.0265, lr=9.96e-06]Steps:  14%|█▍        | 690/5000 [2:29:51<14:13:55, 11.89s/it, loss=1.0265, lr=9.96e-06]Steps:  14%|█▍        | 690/5000 [2:29:51<14:13:55, 11.89s/it, loss=1.0852, lr=9.96e-06]
[Step 690] Training Debug Info:
  Loss: 0.518701
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0542, std: 0.9023
  Noise mean: -0.0006, std: 1.0000
  Target mean: -0.0547, std: 1.3438
  Model pred mean: -0.0547, std: 1.1406
  Sigmas: [0.8828125]... (timesteps: [884.0])

[Step 690] Training Debug Info:
  Loss: 1.115999
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0557, std: 0.9414
  Noise mean: 0.0041, std: 1.0000
  Target mean: -0.0518, std: 1.3750
  Model pred mean: -0.0522, std: 0.8789
  Sigmas: [0.0908203125]... (timesteps: [91.0])

[Step 690] Training Debug Info:
  Loss: 0.397613
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0039, std: 0.8906
  Noise mean: -0.0013, std: 1.0000
  Target mean: -0.0052, std: 1.3359
  Model pred mean: -0.0078, std: 1.1875
  Sigmas: [0.83203125]... (timesteps: [831.0])

[Step 690] Training Debug Info:
  Loss: 0.394707
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0388, std: 0.9102
  Noise mean: -0.0011, std: 1.0000
  Target mean: -0.0398, std: 1.3516
  Model pred mean: -0.0388, std: 1.1953
  Sigmas: [0.84375]... (timesteps: [845.0])
Steps:  14%|█▍        | 691/5000 [2:30:02<14:11:58, 11.86s/it, loss=1.0852, lr=9.96e-06]Steps:  14%|█▍        | 691/5000 [2:30:02<14:11:58, 11.86s/it, loss=0.3947, lr=9.96e-06]Steps:  14%|█▍        | 692/5000 [2:30:14<14:15:39, 11.92s/it, loss=0.3947, lr=9.96e-06]Steps:  14%|█▍        | 692/5000 [2:30:14<14:15:39, 11.92s/it, loss=0.4885, lr=9.96e-06]Steps:  14%|█▍        | 693/5000 [2:30:26<14:18:39, 11.96s/it, loss=0.4885, lr=9.96e-06]Steps:  14%|█▍        | 693/5000 [2:30:26<14:18:39, 11.96s/it, loss=0.4648, lr=9.95e-06]Steps:  14%|█▍        | 694/5000 [2:30:38<14:16:58, 11.94s/it, loss=0.4648, lr=9.95e-06]Steps:  14%|█▍        | 694/5000 [2:30:38<14:16:58, 11.94s/it, loss=0.7171, lr=9.95e-06]Steps:  14%|█▍        | 695/5000 [2:30:50<14:17:03, 11.95s/it, loss=0.7171, lr=9.95e-06]Steps:  14%|█▍        | 695/5000 [2:30:50<14:17:03, 11.95s/it, loss=0.3930, lr=9.95e-06]Steps:  14%|█▍        | 696/5000 [2:31:02<14:16:30, 11.94s/it, loss=0.3930, lr=9.95e-06]Steps:  14%|█▍        | 696/5000 [2:31:02<14:16:30, 11.94s/it, loss=1.1003, lr=9.95e-06]Steps:  14%|█▍        | 697/5000 [2:31:14<14:14:55, 11.92s/it, loss=1.1003, lr=9.95e-06]Steps:  14%|█▍        | 697/5000 [2:31:14<14:14:55, 11.92s/it, loss=0.5053, lr=9.95e-06]Steps:  14%|█▍        | 698/5000 [2:31:26<14:13:30, 11.90s/it, loss=0.5053, lr=9.95e-06]Steps:  14%|█▍        | 698/5000 [2:31:26<14:13:30, 11.90s/it, loss=0.5797, lr=9.95e-06]Steps:  14%|█▍        | 699/5000 [2:31:38<14:13:59, 11.91s/it, loss=0.5797, lr=9.95e-06]Steps:  14%|█▍        | 699/5000 [2:31:38<14:13:59, 11.91s/it, loss=1.1637, lr=9.95e-06]Steps:  14%|█▍        | 700/5000 [2:31:50<14:15:28, 11.94s/it, loss=1.1637, lr=9.95e-06]Steps:  14%|█▍        | 700/5000 [2:31:50<14:15:28, 11.94s/it, loss=1.0663, lr=9.95e-06]01/22/2026 05:56:28 - INFO - __main__ - 
[Step 700] ✅ Loss in normal range (1.0663)
01/22/2026 05:56:28 - INFO - __main__ -   Loss avg (last 100): 0.7511
01/22/2026 05:56:28 - INFO - __main__ -   Loss range: [0.3462, 1.2034]

[Step 700] Training Debug Info:
  Loss: 1.137773
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0019, std: 0.8516
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0021, std: 1.3125
  Model pred mean: -0.0058, std: 0.7578
  Sigmas: [0.08984375]... (timesteps: [90.0])

[Step 700] Training Debug Info:
  Loss: 0.437711
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0043, std: 0.9102
  Noise mean: -0.0003, std: 1.0000
  Target mean: -0.0046, std: 1.3516
  Model pred mean: -0.0145, std: 1.1719
  Sigmas: [0.88671875]... (timesteps: [885.0])

[Step 700] Training Debug Info:
  Loss: 0.510864
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0178, std: 0.9375
  Noise mean: 0.0038, std: 1.0000
  Target mean: -0.0140, std: 1.3750
  Model pred mean: -0.0232, std: 1.1641
  Sigmas: [0.61328125]... (timesteps: [613.0])

[Step 700] Training Debug Info:
  Loss: 0.750305
  Latent shape: torch.Size([1, 32, 48, 174]), Packed shape: torch.Size([1, 2088, 128])
  Latent mean: 0.0220, std: 0.9336
  Noise mean: 0.0024, std: 1.0000
  Target mean: -0.0195, std: 1.3672
  Model pred mean: -0.0035, std: 1.0625
  Sigmas: [0.984375]... (timesteps: [983.0])
Steps:  14%|█▍        | 701/5000 [2:32:02<14:14:45, 11.93s/it, loss=1.0663, lr=9.95e-06]Steps:  14%|█▍        | 701/5000 [2:32:02<14:14:45, 11.93s/it, loss=0.7503, lr=9.95e-06]Steps:  14%|█▍        | 702/5000 [2:32:14<14:17:17, 11.97s/it, loss=0.7503, lr=9.95e-06]Steps:  14%|█▍        | 702/5000 [2:32:14<14:17:17, 11.97s/it, loss=0.9204, lr=9.95e-06]Steps:  14%|█▍        | 703/5000 [2:32:26<14:17:26, 11.97s/it, loss=0.9204, lr=9.95e-06]Steps:  14%|█▍        | 703/5000 [2:32:26<14:17:26, 11.97s/it, loss=0.7861, lr=9.95e-06]Steps:  14%|█▍        | 704/5000 [2:32:38<14:15:06, 11.94s/it, loss=0.7861, lr=9.95e-06]Steps:  14%|█▍        | 704/5000 [2:32:38<14:15:06, 11.94s/it, loss=0.4270, lr=9.95e-06]Steps:  14%|█▍        | 705/5000 [2:32:50<14:13:52, 11.93s/it, loss=0.4270, lr=9.95e-06]Steps:  14%|█▍        | 705/5000 [2:32:50<14:13:52, 11.93s/it, loss=0.4209, lr=9.95e-06]Steps:  14%|█▍        | 706/5000 [2:33:02<14:13:23, 11.92s/it, loss=0.4209, lr=9.95e-06]Steps:  14%|█▍        | 706/5000 [2:33:02<14:13:23, 11.92s/it, loss=0.9391, lr=9.95e-06]Steps:  14%|█▍        | 707/5000 [2:33:13<14:12:44, 11.92s/it, loss=0.9391, lr=9.95e-06]Steps:  14%|█▍        | 707/5000 [2:33:13<14:12:44, 11.92s/it, loss=0.4214, lr=9.95e-06]Steps:  14%|█▍        | 708/5000 [2:33:25<14:12:05, 11.91s/it, loss=0.4214, lr=9.95e-06]Steps:  14%|█▍        | 708/5000 [2:33:25<14:12:05, 11.91s/it, loss=1.1526, lr=9.95e-06]Steps:  14%|█▍        | 709/5000 [2:33:37<14:12:39, 11.92s/it, loss=1.1526, lr=9.95e-06]Steps:  14%|█▍        | 709/5000 [2:33:37<14:12:39, 11.92s/it, loss=0.4919, lr=9.95e-06]Steps:  14%|█▍        | 710/5000 [2:33:49<14:10:51, 11.90s/it, loss=0.4919, lr=9.95e-06]Steps:  14%|█▍        | 710/5000 [2:33:49<14:10:51, 11.90s/it, loss=0.6297, lr=9.95e-06]
[Step 710] Training Debug Info:
  Loss: 1.138304
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0084, std: 0.9375
  Noise mean: 0.0009, std: 0.9961
  Target mean: 0.0093, std: 1.3672
  Model pred mean: 0.0077, std: 0.8633
  Sigmas: [0.1484375]... (timesteps: [148.0])

[Step 710] Training Debug Info:
  Loss: 1.000836
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0104, std: 0.9062
  Noise mean: 0.0008, std: 1.0000
  Target mean: 0.0112, std: 1.3516
  Model pred mean: 0.0125, std: 0.9062
  Sigmas: [0.32421875]... (timesteps: [325.0])

[Step 710] Training Debug Info:
  Loss: 0.854758
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0369, std: 0.9258
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0354, std: 1.3672
  Model pred mean: -0.0342, std: 1.0078
  Sigmas: [0.357421875]... (timesteps: [358.0])

[Step 710] Training Debug Info:
  Loss: 0.380635
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0265, std: 0.8789
  Noise mean: 0.0009, std: 1.0000
  Target mean: 0.0273, std: 1.3359
  Model pred mean: 0.0308, std: 1.1719
  Sigmas: [0.8203125]... (timesteps: [819.0])
Steps:  14%|█▍        | 711/5000 [2:34:01<14:12:59, 11.93s/it, loss=0.6297, lr=9.95e-06]Steps:  14%|█▍        | 711/5000 [2:34:01<14:12:59, 11.93s/it, loss=0.3806, lr=9.95e-06]Steps:  14%|█▍        | 712/5000 [2:34:13<14:11:00, 11.91s/it, loss=0.3806, lr=9.95e-06]Steps:  14%|█▍        | 712/5000 [2:34:13<14:11:00, 11.91s/it, loss=1.0971, lr=9.95e-06]Steps:  14%|█▍        | 713/5000 [2:34:25<14:11:37, 11.92s/it, loss=1.0971, lr=9.95e-06]Steps:  14%|█▍        | 713/5000 [2:34:25<14:11:37, 11.92s/it, loss=0.6112, lr=9.94e-06]Steps:  14%|█▍        | 714/5000 [2:34:37<14:11:37, 11.92s/it, loss=0.6112, lr=9.94e-06]Steps:  14%|█▍        | 714/5000 [2:34:37<14:11:37, 11.92s/it, loss=0.5090, lr=9.94e-06]Steps:  14%|█▍        | 715/5000 [2:34:49<14:12:02, 11.93s/it, loss=0.5090, lr=9.94e-06]Steps:  14%|█▍        | 715/5000 [2:34:49<14:12:02, 11.93s/it, loss=0.4156, lr=9.94e-06]Steps:  14%|█▍        | 716/5000 [2:35:01<14:11:35, 11.93s/it, loss=0.4156, lr=9.94e-06]Steps:  14%|█▍        | 716/5000 [2:35:01<14:11:35, 11.93s/it, loss=1.1394, lr=9.94e-06]Steps:  14%|█▍        | 717/5000 [2:35:13<14:11:02, 11.92s/it, loss=1.1394, lr=9.94e-06]Steps:  14%|█▍        | 717/5000 [2:35:13<14:11:02, 11.92s/it, loss=0.4329, lr=9.94e-06]Steps:  14%|█▍        | 718/5000 [2:35:25<14:12:32, 11.95s/it, loss=0.4329, lr=9.94e-06]Steps:  14%|█▍        | 718/5000 [2:35:25<14:12:32, 11.95s/it, loss=1.0385, lr=9.94e-06]Steps:  14%|█▍        | 719/5000 [2:35:37<14:10:55, 11.93s/it, loss=1.0385, lr=9.94e-06]Steps:  14%|█▍        | 719/5000 [2:35:37<14:10:55, 11.93s/it, loss=0.7863, lr=9.94e-06]Steps:  14%|█▍        | 720/5000 [2:35:49<14:12:31, 11.95s/it, loss=0.7863, lr=9.94e-06]Steps:  14%|█▍        | 720/5000 [2:35:49<14:12:31, 11.95s/it, loss=0.4481, lr=9.94e-06]
[Step 720] Training Debug Info:
  Loss: 0.678447
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0168, std: 0.9297
  Noise mean: 0.0006, std: 0.9961
  Target mean: -0.0162, std: 1.3672
  Model pred mean: 0.0119, std: 1.1016
  Sigmas: [0.9609375]... (timesteps: [962.0])

[Step 720] Training Debug Info:
  Loss: 0.464170
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0206, std: 0.9062
  Noise mean: 0.0026, std: 1.0000
  Target mean: 0.0232, std: 1.3516
  Model pred mean: 0.0229, std: 1.1797
  Sigmas: [0.90625]... (timesteps: [908.0])

[Step 720] Training Debug Info:
  Loss: 0.391430
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0025, std: 0.9727
  Noise mean: 0.0007, std: 1.0000
  Target mean: 0.0032, std: 1.3984
  Model pred mean: -0.0003, std: 1.2500
  Sigmas: [0.8125]... (timesteps: [814.0])

[Step 720] Training Debug Info:
  Loss: 1.049429
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0017, std: 0.9102
  Noise mean: 0.0014, std: 1.0000
  Target mean: 0.0031, std: 1.3516
  Model pred mean: 0.0094, std: 0.8945
  Sigmas: [0.02294921875]... (timesteps: [23.0])
Steps:  14%|█▍        | 721/5000 [2:36:00<14:12:07, 11.95s/it, loss=0.4481, lr=9.94e-06]Steps:  14%|█▍        | 721/5000 [2:36:00<14:12:07, 11.95s/it, loss=1.0494, lr=9.94e-06]Steps:  14%|█▍        | 722/5000 [2:36:12<14:10:50, 11.93s/it, loss=1.0494, lr=9.94e-06]Steps:  14%|█▍        | 722/5000 [2:36:12<14:10:50, 11.93s/it, loss=0.9425, lr=9.94e-06]Steps:  14%|█▍        | 723/5000 [2:36:24<14:11:00, 11.94s/it, loss=0.9425, lr=9.94e-06]Steps:  14%|█▍        | 723/5000 [2:36:24<14:11:00, 11.94s/it, loss=0.5625, lr=9.94e-06]Steps:  14%|█▍        | 724/5000 [2:36:36<14:10:27, 11.93s/it, loss=0.5625, lr=9.94e-06]Steps:  14%|█▍        | 724/5000 [2:36:36<14:10:27, 11.93s/it, loss=1.1272, lr=9.94e-06]Steps:  14%|█▍        | 725/5000 [2:36:48<14:09:22, 11.92s/it, loss=1.1272, lr=9.94e-06]Steps:  14%|█▍        | 725/5000 [2:36:48<14:09:22, 11.92s/it, loss=0.6479, lr=9.94e-06]Steps:  15%|█▍        | 726/5000 [2:37:00<14:07:11, 11.89s/it, loss=0.6479, lr=9.94e-06]Steps:  15%|█▍        | 726/5000 [2:37:00<14:07:11, 11.89s/it, loss=0.5167, lr=9.94e-06]Steps:  15%|█▍        | 727/5000 [2:37:12<14:06:21, 11.88s/it, loss=0.5167, lr=9.94e-06]Steps:  15%|█▍        | 727/5000 [2:37:12<14:06:21, 11.88s/it, loss=1.0718, lr=9.94e-06]Steps:  15%|█▍        | 728/5000 [2:37:24<14:05:22, 11.87s/it, loss=1.0718, lr=9.94e-06]Steps:  15%|█▍        | 728/5000 [2:37:24<14:05:22, 11.87s/it, loss=0.4323, lr=9.94e-06]Steps:  15%|█▍        | 729/5000 [2:37:36<14:07:55, 11.91s/it, loss=0.4323, lr=9.94e-06]Steps:  15%|█▍        | 729/5000 [2:37:36<14:07:55, 11.91s/it, loss=1.1425, lr=9.94e-06]Steps:  15%|█▍        | 730/5000 [2:37:48<14:06:38, 11.90s/it, loss=1.1425, lr=9.94e-06]Steps:  15%|█▍        | 730/5000 [2:37:48<14:06:38, 11.90s/it, loss=1.1701, lr=9.94e-06]
[Step 730] Training Debug Info:
  Loss: 1.079462
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0052, std: 0.9102
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0049, std: 1.3516
  Model pred mean: -0.0042, std: 0.8711
  Sigmas: [0.28515625]... (timesteps: [285.0])

[Step 730] Training Debug Info:
  Loss: 1.090665
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0216, std: 0.9219
  Noise mean: -0.0016, std: 1.0000
  Target mean: -0.0232, std: 1.3594
  Model pred mean: -0.0186, std: 0.8672
  Sigmas: [0.2412109375]... (timesteps: [241.0])

[Step 730] Training Debug Info:
  Loss: 0.477316
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0104, std: 0.8945
  Noise mean: 0.0017, std: 1.0000
  Target mean: 0.0121, std: 1.3438
  Model pred mean: 0.0148, std: 1.1484
  Sigmas: [0.71484375]... (timesteps: [713.0])

[Step 730] Training Debug Info:
  Loss: 0.388905
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0250, std: 0.9023
  Noise mean: 0.0006, std: 1.0000
  Target mean: -0.0244, std: 1.3438
  Model pred mean: -0.0198, std: 1.1875
  Sigmas: [0.765625]... (timesteps: [766.0])
Steps:  15%|█▍        | 731/5000 [2:37:59<14:07:04, 11.91s/it, loss=1.1701, lr=9.94e-06]Steps:  15%|█▍        | 731/5000 [2:37:59<14:07:04, 11.91s/it, loss=0.3889, lr=9.94e-06]Steps:  15%|█▍        | 732/5000 [2:38:11<14:08:24, 11.93s/it, loss=0.3889, lr=9.94e-06]Steps:  15%|█▍        | 732/5000 [2:38:11<14:08:24, 11.93s/it, loss=1.1356, lr=9.93e-06]Steps:  15%|█▍        | 733/5000 [2:38:23<14:06:42, 11.91s/it, loss=1.1356, lr=9.93e-06]Steps:  15%|█▍        | 733/5000 [2:38:23<14:06:42, 11.91s/it, loss=1.2279, lr=9.93e-06]Steps:  15%|█▍        | 734/5000 [2:38:35<14:05:54, 11.90s/it, loss=1.2279, lr=9.93e-06]Steps:  15%|█▍        | 734/5000 [2:38:35<14:05:54, 11.90s/it, loss=1.0095, lr=9.93e-06]Steps:  15%|█▍        | 735/5000 [2:38:47<14:05:29, 11.89s/it, loss=1.0095, lr=9.93e-06]Steps:  15%|█▍        | 735/5000 [2:38:47<14:05:29, 11.89s/it, loss=1.1513, lr=9.93e-06]Steps:  15%|█▍        | 736/5000 [2:38:59<14:05:37, 11.90s/it, loss=1.1513, lr=9.93e-06]Steps:  15%|█▍        | 736/5000 [2:38:59<14:05:37, 11.90s/it, loss=0.7720, lr=9.93e-06]Steps:  15%|█▍        | 737/5000 [2:39:11<14:04:18, 11.88s/it, loss=0.7720, lr=9.93e-06]Steps:  15%|█▍        | 737/5000 [2:39:11<14:04:18, 11.88s/it, loss=1.0561, lr=9.93e-06]Steps:  15%|█▍        | 738/5000 [2:39:23<14:05:14, 11.90s/it, loss=1.0561, lr=9.93e-06]Steps:  15%|█▍        | 738/5000 [2:39:23<14:05:14, 11.90s/it, loss=1.0601, lr=9.93e-06]Steps:  15%|█▍        | 739/5000 [2:39:35<14:04:07, 11.89s/it, loss=1.0601, lr=9.93e-06]Steps:  15%|█▍        | 739/5000 [2:39:35<14:04:07, 11.89s/it, loss=0.3148, lr=9.93e-06]Steps:  15%|█▍        | 740/5000 [2:39:46<14:04:18, 11.89s/it, loss=0.3148, lr=9.93e-06]Steps:  15%|█▍        | 740/5000 [2:39:46<14:04:18, 11.89s/it, loss=0.3738, lr=9.93e-06]
[Step 740] Training Debug Info:
  Loss: 1.168257
  Latent shape: torch.Size([1, 32, 108, 78]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0120, std: 0.9336
  Noise mean: -0.0022, std: 1.0000
  Target mean: -0.0142, std: 1.3750
  Model pred mean: -0.0043, std: 0.8398
  Sigmas: [0.2265625]... (timesteps: [227.0])

[Step 740] Training Debug Info:
  Loss: 1.007189
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0114, std: 0.8867
  Noise mean: 0.0028, std: 0.9961
  Target mean: 0.0143, std: 1.3359
  Model pred mean: 0.0188, std: 0.8633
  Sigmas: [0.0019989013671875]... (timesteps: [2.0])

[Step 740] Training Debug Info:
  Loss: 0.531191
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0371, std: 0.9062
  Noise mean: -0.0014, std: 1.0000
  Target mean: -0.0386, std: 1.3516
  Model pred mean: -0.0337, std: 1.1328
  Sigmas: [0.609375]... (timesteps: [611.0])

[Step 740] Training Debug Info:
  Loss: 0.400177
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: -0.0031, std: 0.9023
  Noise mean: -0.0005, std: 1.0000
  Target mean: 0.0025, std: 1.3516
  Model pred mean: 0.0071, std: 1.1875
  Sigmas: [0.8359375]... (timesteps: [837.0])
Steps:  15%|█▍        | 741/5000 [2:39:58<14:04:28, 11.90s/it, loss=0.3738, lr=9.93e-06]Steps:  15%|█▍        | 741/5000 [2:39:58<14:04:28, 11.90s/it, loss=0.4002, lr=9.93e-06]Steps:  15%|█▍        | 742/5000 [2:40:10<14:02:35, 11.87s/it, loss=0.4002, lr=9.93e-06]Steps:  15%|█▍        | 742/5000 [2:40:10<14:02:35, 11.87s/it, loss=0.4466, lr=9.93e-06]Steps:  15%|█▍        | 743/5000 [2:40:22<14:04:07, 11.90s/it, loss=0.4466, lr=9.93e-06]Steps:  15%|█▍        | 743/5000 [2:40:22<14:04:07, 11.90s/it, loss=0.4051, lr=9.93e-06]Steps:  15%|█▍        | 744/5000 [2:40:34<14:04:01, 11.90s/it, loss=0.4051, lr=9.93e-06]Steps:  15%|█▍        | 744/5000 [2:40:34<14:04:01, 11.90s/it, loss=0.4929, lr=9.93e-06]Steps:  15%|█▍        | 745/5000 [2:40:46<14:04:19, 11.91s/it, loss=0.4929, lr=9.93e-06]Steps:  15%|█▍        | 745/5000 [2:40:46<14:04:19, 11.91s/it, loss=0.9742, lr=9.93e-06]Steps:  15%|█▍        | 746/5000 [2:40:58<14:03:01, 11.89s/it, loss=0.9742, lr=9.93e-06]Steps:  15%|█▍        | 746/5000 [2:40:58<14:03:01, 11.89s/it, loss=1.1760, lr=9.93e-06]Steps:  15%|█▍        | 747/5000 [2:41:10<14:04:27, 11.91s/it, loss=1.1760, lr=9.93e-06]Steps:  15%|█▍        | 747/5000 [2:41:10<14:04:27, 11.91s/it, loss=0.4749, lr=9.93e-06]Steps:  15%|█▍        | 748/5000 [2:41:22<14:04:36, 11.92s/it, loss=0.4749, lr=9.93e-06]Steps:  15%|█▍        | 748/5000 [2:41:22<14:04:36, 11.92s/it, loss=0.5296, lr=9.93e-06]Steps:  15%|█▍        | 749/5000 [2:41:34<14:03:50, 11.91s/it, loss=0.5296, lr=9.93e-06]Steps:  15%|█▍        | 749/5000 [2:41:34<14:03:50, 11.91s/it, loss=1.1589, lr=9.92e-06]Steps:  15%|█▌        | 750/5000 [2:41:45<14:01:42, 11.88s/it, loss=1.1589, lr=9.92e-06]Steps:  15%|█▌        | 750/5000 [2:41:45<14:01:42, 11.88s/it, loss=1.1141, lr=9.92e-06]
[Step 750] Training Debug Info:
  Loss: 1.094505
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0023, std: 0.8516
  Noise mean: 0.0014, std: 1.0000
  Target mean: 0.0038, std: 1.3125
  Model pred mean: 0.0023, std: 0.7969
  Sigmas: [0.330078125]... (timesteps: [330.0])

[Step 750] Training Debug Info:
  Loss: 0.726063
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0227, std: 0.9648
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0215, std: 1.3906
  Model pred mean: -0.0165, std: 1.0859
  Sigmas: [0.95703125]... (timesteps: [956.0])

[Step 750] Training Debug Info:
  Loss: 0.720465
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0374, std: 0.9141
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0371, std: 1.3516
  Model pred mean: -0.0388, std: 1.0625
  Sigmas: [0.435546875]... (timesteps: [436.0])

[Step 750] Training Debug Info:
  Loss: 0.432793
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0391, std: 0.9258
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0386, std: 1.3672
  Model pred mean: -0.0388, std: 1.1953
  Sigmas: [0.67578125]... (timesteps: [676.0])
Steps:  15%|█▌        | 751/5000 [2:41:57<14:00:52, 11.87s/it, loss=1.1141, lr=9.92e-06]Steps:  15%|█▌        | 751/5000 [2:41:57<14:00:52, 11.87s/it, loss=0.4328, lr=9.92e-06]Steps:  15%|█▌        | 752/5000 [2:42:09<14:00:52, 11.88s/it, loss=0.4328, lr=9.92e-06]Steps:  15%|█▌        | 752/5000 [2:42:09<14:00:52, 11.88s/it, loss=0.3789, lr=9.92e-06]Steps:  15%|█▌        | 753/5000 [2:42:21<14:00:01, 11.87s/it, loss=0.3789, lr=9.92e-06]Steps:  15%|█▌        | 753/5000 [2:42:21<14:00:01, 11.87s/it, loss=1.0176, lr=9.92e-06]Steps:  15%|█▌        | 754/5000 [2:42:33<14:00:49, 11.88s/it, loss=1.0176, lr=9.92e-06]Steps:  15%|█▌        | 754/5000 [2:42:33<14:00:49, 11.88s/it, loss=1.0659, lr=9.92e-06]Steps:  15%|█▌        | 755/5000 [2:42:45<14:00:45, 11.88s/it, loss=1.0659, lr=9.92e-06]Steps:  15%|█▌        | 755/5000 [2:42:45<14:00:45, 11.88s/it, loss=0.4003, lr=9.92e-06]Steps:  15%|█▌        | 756/5000 [2:42:57<14:01:43, 11.90s/it, loss=0.4003, lr=9.92e-06]Steps:  15%|█▌        | 756/5000 [2:42:57<14:01:43, 11.90s/it, loss=0.5839, lr=9.92e-06]Steps:  15%|█▌        | 757/5000 [2:43:09<14:01:31, 11.90s/it, loss=0.5839, lr=9.92e-06]Steps:  15%|█▌        | 757/5000 [2:43:09<14:01:31, 11.90s/it, loss=0.4898, lr=9.92e-06]Steps:  15%|█▌        | 758/5000 [2:43:21<14:01:10, 11.90s/it, loss=0.4898, lr=9.92e-06]Steps:  15%|█▌        | 758/5000 [2:43:21<14:01:10, 11.90s/it, loss=0.3926, lr=9.92e-06]Steps:  15%|█▌        | 759/5000 [2:43:32<14:00:27, 11.89s/it, loss=0.3926, lr=9.92e-06]Steps:  15%|█▌        | 759/5000 [2:43:32<14:00:27, 11.89s/it, loss=0.9977, lr=9.92e-06]Steps:  15%|█▌        | 760/5000 [2:43:44<13:59:48, 11.88s/it, loss=0.9977, lr=9.92e-06]Steps:  15%|█▌        | 760/5000 [2:43:44<13:59:48, 11.88s/it, loss=0.6560, lr=9.92e-06]
[Step 760] Training Debug Info:
  Loss: 0.409551
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0312, std: 0.9414
  Noise mean: -0.0025, std: 1.0000
  Target mean: -0.0337, std: 1.3750
  Model pred mean: -0.0270, std: 1.2188
  Sigmas: [0.78515625]... (timesteps: [785.0])

[Step 760] Training Debug Info:
  Loss: 1.018852
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0135, std: 0.9141
  Noise mean: 0.0001, std: 1.0000
  Target mean: 0.0136, std: 1.3516
  Model pred mean: 0.0071, std: 0.8984
  Sigmas: [0.330078125]... (timesteps: [330.0])

[Step 760] Training Debug Info:
  Loss: 0.639280
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0090, std: 0.9141
  Noise mean: 0.0027, std: 0.9961
  Target mean: -0.0063, std: 1.3516
  Model pred mean: -0.0229, std: 1.0938
  Sigmas: [0.984375]... (timesteps: [985.0])

[Step 760] Training Debug Info:
  Loss: 1.188143
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0053, std: 0.8906
  Noise mean: 0.0002, std: 0.9961
  Target mean: 0.0056, std: 1.3359
  Model pred mean: 0.0034, std: 0.7734
  Sigmas: [0.1943359375]... (timesteps: [194.0])
Steps:  15%|█▌        | 761/5000 [2:43:56<13:59:43, 11.89s/it, loss=0.6560, lr=9.92e-06]Steps:  15%|█▌        | 761/5000 [2:43:56<13:59:43, 11.89s/it, loss=1.1881, lr=9.92e-06]Steps:  15%|█▌        | 762/5000 [2:44:08<13:59:25, 11.88s/it, loss=1.1881, lr=9.92e-06]Steps:  15%|█▌        | 762/5000 [2:44:08<13:59:25, 11.88s/it, loss=0.6738, lr=9.92e-06]Steps:  15%|█▌        | 763/5000 [2:44:20<13:59:44, 11.89s/it, loss=0.6738, lr=9.92e-06]Steps:  15%|█▌        | 763/5000 [2:44:20<13:59:44, 11.89s/it, loss=0.7465, lr=9.92e-06]Steps:  15%|█▌        | 764/5000 [2:44:32<14:02:56, 11.94s/it, loss=0.7465, lr=9.92e-06]Steps:  15%|█▌        | 764/5000 [2:44:32<14:02:56, 11.94s/it, loss=0.4100, lr=9.92e-06]Steps:  15%|█▌        | 765/5000 [2:44:44<14:03:12, 11.95s/it, loss=0.4100, lr=9.92e-06]Steps:  15%|█▌        | 765/5000 [2:44:44<14:03:12, 11.95s/it, loss=1.1000, lr=9.91e-06]Steps:  15%|█▌        | 766/5000 [2:44:56<14:02:23, 11.94s/it, loss=1.1000, lr=9.91e-06]Steps:  15%|█▌        | 766/5000 [2:44:56<14:02:23, 11.94s/it, loss=0.5054, lr=9.91e-06]Steps:  15%|█▌        | 767/5000 [2:45:08<13:59:40, 11.90s/it, loss=0.5054, lr=9.91e-06]Steps:  15%|█▌        | 767/5000 [2:45:08<13:59:40, 11.90s/it, loss=1.1038, lr=9.91e-06]Steps:  15%|█▌        | 768/5000 [2:45:20<13:58:35, 11.89s/it, loss=1.1038, lr=9.91e-06]Steps:  15%|█▌        | 768/5000 [2:45:20<13:58:35, 11.89s/it, loss=0.7910, lr=9.91e-06]Steps:  15%|█▌        | 769/5000 [2:45:31<13:58:01, 11.88s/it, loss=0.7910, lr=9.91e-06]Steps:  15%|█▌        | 769/5000 [2:45:31<13:58:01, 11.88s/it, loss=1.0920, lr=9.91e-06]Steps:  15%|█▌        | 770/5000 [2:45:43<13:58:19, 11.89s/it, loss=1.0920, lr=9.91e-06]Steps:  15%|█▌        | 770/5000 [2:45:43<13:58:19, 11.89s/it, loss=0.8531, lr=9.91e-06]
[Step 770] Training Debug Info:
  Loss: 0.495915
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0062, std: 0.9297
  Noise mean: -0.0023, std: 1.0000
  Target mean: -0.0085, std: 1.3594
  Model pred mean: -0.0043, std: 1.1719
  Sigmas: [0.640625]... (timesteps: [640.0])

[Step 770] Training Debug Info:
  Loss: 0.427969
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0576, std: 0.9297
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0574, std: 1.3594
  Model pred mean: -0.0508, std: 1.2031
  Sigmas: [0.88671875]... (timesteps: [888.0])

[Step 770] Training Debug Info:
  Loss: 0.461050
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0496, std: 0.9219
  Noise mean: -0.0039, std: 1.0000
  Target mean: -0.0535, std: 1.3594
  Model pred mean: -0.0476, std: 1.1875
  Sigmas: [0.76953125]... (timesteps: [769.0])

[Step 770] Training Debug Info:
  Loss: 0.587650
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0081, std: 0.9180
  Noise mean: 0.0034, std: 1.0000
  Target mean: -0.0046, std: 1.3594
  Model pred mean: -0.0047, std: 1.1172
  Sigmas: [0.58203125]... (timesteps: [582.0])
Steps:  15%|█▌        | 771/5000 [2:45:55<13:59:58, 11.92s/it, loss=0.8531, lr=9.91e-06]Steps:  15%|█▌        | 771/5000 [2:45:55<13:59:58, 11.92s/it, loss=0.5876, lr=9.91e-06]Steps:  15%|█▌        | 772/5000 [2:46:07<14:00:24, 11.93s/it, loss=0.5876, lr=9.91e-06]Steps:  15%|█▌        | 772/5000 [2:46:07<14:00:24, 11.93s/it, loss=0.9916, lr=9.91e-06]Steps:  15%|█▌        | 773/5000 [2:46:19<13:58:20, 11.90s/it, loss=0.9916, lr=9.91e-06]Steps:  15%|█▌        | 773/5000 [2:46:19<13:58:20, 11.90s/it, loss=1.1490, lr=9.91e-06]Steps:  15%|█▌        | 774/5000 [2:46:31<14:00:18, 11.93s/it, loss=1.1490, lr=9.91e-06]Steps:  15%|█▌        | 774/5000 [2:46:31<14:00:18, 11.93s/it, loss=1.1397, lr=9.91e-06]Steps:  16%|█▌        | 775/5000 [2:46:43<14:00:35, 11.94s/it, loss=1.1397, lr=9.91e-06]Steps:  16%|█▌        | 775/5000 [2:46:43<14:00:35, 11.94s/it, loss=0.9298, lr=9.91e-06]Steps:  16%|█▌        | 776/5000 [2:46:55<13:58:36, 11.91s/it, loss=0.9298, lr=9.91e-06]Steps:  16%|█▌        | 776/5000 [2:46:55<13:58:36, 11.91s/it, loss=1.1364, lr=9.91e-06]Steps:  16%|█▌        | 777/5000 [2:47:07<13:58:37, 11.92s/it, loss=1.1364, lr=9.91e-06]Steps:  16%|█▌        | 777/5000 [2:47:07<13:58:37, 11.92s/it, loss=0.4076, lr=9.91e-06]Steps:  16%|█▌        | 778/5000 [2:47:19<13:57:18, 11.90s/it, loss=0.4076, lr=9.91e-06]Steps:  16%|█▌        | 778/5000 [2:47:19<13:57:18, 11.90s/it, loss=1.1632, lr=9.91e-06]Steps:  16%|█▌        | 779/5000 [2:47:31<13:55:21, 11.87s/it, loss=1.1632, lr=9.91e-06]Steps:  16%|█▌        | 779/5000 [2:47:31<13:55:21, 11.87s/it, loss=1.1251, lr=9.91e-06]Steps:  16%|█▌        | 780/5000 [2:47:42<13:54:55, 11.87s/it, loss=1.1251, lr=9.91e-06]Steps:  16%|█▌        | 780/5000 [2:47:42<13:54:55, 11.87s/it, loss=0.7757, lr=9.90e-06]
[Step 780] Training Debug Info:
  Loss: 0.909977
  Latent shape: torch.Size([1, 32, 114, 78]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0250, std: 0.9414
  Noise mean: 0.0019, std: 1.0000
  Target mean: -0.0232, std: 1.3750
  Model pred mean: -0.0245, std: 0.9922
  Sigmas: [0.361328125]... (timesteps: [362.0])

[Step 780] Training Debug Info:
  Loss: 0.656457
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0136, std: 0.9062
  Noise mean: -0.0029, std: 1.0000
  Target mean: 0.0107, std: 1.3516
  Model pred mean: 0.0148, std: 1.0859
  Sigmas: [0.5546875]... (timesteps: [555.0])

[Step 780] Training Debug Info:
  Loss: 1.146604
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0022, std: 0.9297
  Noise mean: -0.0007, std: 1.0000
  Target mean: -0.0029, std: 1.3672
  Model pred mean: 0.0026, std: 0.8516
  Sigmas: [0.185546875]... (timesteps: [186.0])

[Step 780] Training Debug Info:
  Loss: 0.502478
  Latent shape: torch.Size([1, 32, 108, 78]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0087, std: 0.8867
  Noise mean: -0.0008, std: 1.0000
  Target mean: -0.0096, std: 1.3359
  Model pred mean: -0.0095, std: 1.1328
  Sigmas: [0.671875]... (timesteps: [673.0])
Steps:  16%|█▌        | 781/5000 [2:47:54<13:56:15, 11.89s/it, loss=0.7757, lr=9.90e-06]Steps:  16%|█▌        | 781/5000 [2:47:54<13:56:15, 11.89s/it, loss=0.5025, lr=9.90e-06]Steps:  16%|█▌        | 782/5000 [2:48:06<13:56:30, 11.90s/it, loss=0.5025, lr=9.90e-06]Steps:  16%|█▌        | 782/5000 [2:48:06<13:56:30, 11.90s/it, loss=1.0440, lr=9.90e-06]Steps:  16%|█▌        | 783/5000 [2:48:18<13:58:02, 11.92s/it, loss=1.0440, lr=9.90e-06]Steps:  16%|█▌        | 783/5000 [2:48:18<13:58:02, 11.92s/it, loss=0.4365, lr=9.90e-06]Steps:  16%|█▌        | 784/5000 [2:48:30<13:57:59, 11.93s/it, loss=0.4365, lr=9.90e-06]Steps:  16%|█▌        | 784/5000 [2:48:30<13:57:59, 11.93s/it, loss=0.4894, lr=9.90e-06]Steps:  16%|█▌        | 785/5000 [2:48:42<13:56:07, 11.90s/it, loss=0.4894, lr=9.90e-06]Steps:  16%|█▌        | 785/5000 [2:48:42<13:56:07, 11.90s/it, loss=0.4180, lr=9.90e-06]Steps:  16%|█▌        | 786/5000 [2:48:54<13:55:58, 11.90s/it, loss=0.4180, lr=9.90e-06]Steps:  16%|█▌        | 786/5000 [2:48:54<13:55:58, 11.90s/it, loss=0.8253, lr=9.90e-06]Steps:  16%|█▌        | 787/5000 [2:49:06<13:55:28, 11.90s/it, loss=0.8253, lr=9.90e-06]Steps:  16%|█▌        | 787/5000 [2:49:06<13:55:28, 11.90s/it, loss=0.6404, lr=9.90e-06]Steps:  16%|█▌        | 788/5000 [2:49:18<13:55:20, 11.90s/it, loss=0.6404, lr=9.90e-06]Steps:  16%|█▌        | 788/5000 [2:49:18<13:55:20, 11.90s/it, loss=1.0941, lr=9.90e-06]Steps:  16%|█▌        | 789/5000 [2:49:30<13:56:07, 11.91s/it, loss=1.0941, lr=9.90e-06]Steps:  16%|█▌        | 789/5000 [2:49:30<13:56:07, 11.91s/it, loss=1.0295, lr=9.90e-06]Steps:  16%|█▌        | 790/5000 [2:49:42<13:55:16, 11.90s/it, loss=1.0295, lr=9.90e-06]Steps:  16%|█▌        | 790/5000 [2:49:42<13:55:16, 11.90s/it, loss=1.2160, lr=9.90e-06]
[Step 790] Training Debug Info:
  Loss: 0.406879
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0092, std: 0.9102
  Noise mean: 0.0005, std: 1.0000
  Target mean: 0.0097, std: 1.3516
  Model pred mean: 0.0152, std: 1.1797
  Sigmas: [0.9296875]... (timesteps: [929.0])

[Step 790] Training Debug Info:
  Loss: 1.139285
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0095, std: 0.9219
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0098, std: 1.3594
  Model pred mean: -0.0063, std: 0.8398
  Sigmas: [0.205078125]... (timesteps: [205.0])

[Step 790] Training Debug Info:
  Loss: 0.413087
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0084, std: 0.8750
  Noise mean: -0.0007, std: 1.0000
  Target mean: 0.0077, std: 1.3281
  Model pred mean: 0.0097, std: 1.1641
  Sigmas: [0.81640625]... (timesteps: [815.0])

[Step 790] Training Debug Info:
  Loss: 1.060732
  Latent shape: torch.Size([1, 32, 102, 90]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0013, std: 0.9336
  Noise mean: -0.0038, std: 1.0000
  Target mean: -0.0050, std: 1.3672
  Model pred mean: -0.0041, std: 0.8984
  Sigmas: [0.035888671875]... (timesteps: [36.0])
Steps:  16%|█▌        | 791/5000 [2:49:53<13:55:43, 11.91s/it, loss=1.2160, lr=9.90e-06]Steps:  16%|█▌        | 791/5000 [2:49:54<13:55:43, 11.91s/it, loss=1.0607, lr=9.90e-06]Steps:  16%|█▌        | 792/5000 [2:50:05<13:57:08, 11.94s/it, loss=1.0607, lr=9.90e-06]Steps:  16%|█▌        | 792/5000 [2:50:05<13:57:08, 11.94s/it, loss=0.4651, lr=9.90e-06]Steps:  16%|█▌        | 793/5000 [2:50:17<13:55:36, 11.92s/it, loss=0.4651, lr=9.90e-06]Steps:  16%|█▌        | 793/5000 [2:50:17<13:55:36, 11.92s/it, loss=0.5974, lr=9.90e-06]Steps:  16%|█▌        | 794/5000 [2:50:29<13:57:58, 11.95s/it, loss=0.5974, lr=9.90e-06]Steps:  16%|█▌        | 794/5000 [2:50:29<13:57:58, 11.95s/it, loss=0.3927, lr=9.90e-06]Steps:  16%|█▌        | 795/5000 [2:50:41<13:56:58, 11.94s/it, loss=0.3927, lr=9.90e-06]Steps:  16%|█▌        | 795/5000 [2:50:41<13:56:58, 11.94s/it, loss=0.6651, lr=9.89e-06]Steps:  16%|█▌        | 796/5000 [2:50:53<13:57:58, 11.96s/it, loss=0.6651, lr=9.89e-06]Steps:  16%|█▌        | 796/5000 [2:50:53<13:57:58, 11.96s/it, loss=0.8683, lr=9.89e-06]Steps:  16%|█▌        | 797/5000 [2:51:05<13:58:22, 11.97s/it, loss=0.8683, lr=9.89e-06]Steps:  16%|█▌        | 797/5000 [2:51:05<13:58:22, 11.97s/it, loss=0.4695, lr=9.89e-06]Steps:  16%|█▌        | 798/5000 [2:51:17<13:57:37, 11.96s/it, loss=0.4695, lr=9.89e-06]Steps:  16%|█▌        | 798/5000 [2:51:17<13:57:37, 11.96s/it, loss=1.1405, lr=9.89e-06]Steps:  16%|█▌        | 799/5000 [2:51:29<13:55:05, 11.93s/it, loss=1.1405, lr=9.89e-06]Steps:  16%|█▌        | 799/5000 [2:51:29<13:55:05, 11.93s/it, loss=0.3513, lr=9.89e-06]Steps:  16%|█▌        | 800/5000 [2:51:41<13:54:11, 11.92s/it, loss=0.3513, lr=9.89e-06]Steps:  16%|█▌        | 800/5000 [2:51:41<13:54:11, 11.92s/it, loss=0.9665, lr=9.89e-06]01/22/2026 06:16:19 - INFO - __main__ - 
[Step 800] ✅ Loss in normal range (0.9665)
01/22/2026 06:16:19 - INFO - __main__ -   Loss avg (last 100): 0.7708
01/22/2026 06:16:19 - INFO - __main__ -   Loss range: [0.3148, 1.2279]

[Step 800] Training Debug Info:
  Loss: 1.104442
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0085, std: 0.9688
  Noise mean: -0.0022, std: 1.0000
  Target mean: -0.0107, std: 1.3906
  Model pred mean: -0.0087, std: 0.9141
  Sigmas: [0.216796875]... (timesteps: [217.0])

[Step 800] Training Debug Info:
  Loss: 1.081423
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0398, std: 0.9258
  Noise mean: -0.0028, std: 1.0000
  Target mean: -0.0425, std: 1.3594
  Model pred mean: -0.0352, std: 0.8867
  Sigmas: [0.06005859375]... (timesteps: [60.0])

[Step 800] Training Debug Info:
  Loss: 0.799613
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0166, std: 1.0000
  Noise mean: -0.0023, std: 1.0000
  Target mean: -0.0188, std: 1.4141
  Model pred mean: -0.0173, std: 1.0938
  Sigmas: [0.9765625]... (timesteps: [976.0])

[Step 800] Training Debug Info:
  Loss: 1.075829
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0311, std: 0.9570
  Noise mean: -0.0013, std: 1.0000
  Target mean: -0.0325, std: 1.3828
  Model pred mean: -0.0291, std: 0.9141
  Sigmas: [0.049072265625]... (timesteps: [49.0])
Steps:  16%|█▌        | 801/5000 [2:51:53<13:56:04, 11.95s/it, loss=0.9665, lr=9.89e-06]Steps:  16%|█▌        | 801/5000 [2:51:53<13:56:04, 11.95s/it, loss=1.0758, lr=9.89e-06]Steps:  16%|█▌        | 802/5000 [2:52:05<13:54:45, 11.93s/it, loss=1.0758, lr=9.89e-06]Steps:  16%|█▌        | 802/5000 [2:52:05<13:54:45, 11.93s/it, loss=0.4486, lr=9.89e-06]Steps:  16%|█▌        | 803/5000 [2:52:17<13:53:31, 11.92s/it, loss=0.4486, lr=9.89e-06]Steps:  16%|█▌        | 803/5000 [2:52:17<13:53:31, 11.92s/it, loss=0.6399, lr=9.89e-06]Steps:  16%|█▌        | 804/5000 [2:52:29<13:53:08, 11.91s/it, loss=0.6399, lr=9.89e-06]Steps:  16%|█▌        | 804/5000 [2:52:29<13:53:08, 11.91s/it, loss=1.1247, lr=9.89e-06]Steps:  16%|█▌        | 805/5000 [2:52:41<13:53:07, 11.92s/it, loss=1.1247, lr=9.89e-06]Steps:  16%|█▌        | 805/5000 [2:52:41<13:53:07, 11.92s/it, loss=1.1716, lr=9.89e-06]Steps:  16%|█▌        | 806/5000 [2:52:53<13:52:21, 11.91s/it, loss=1.1716, lr=9.89e-06]Steps:  16%|█▌        | 806/5000 [2:52:53<13:52:21, 11.91s/it, loss=0.3956, lr=9.89e-06]Steps:  16%|█▌        | 807/5000 [2:53:04<13:51:26, 11.90s/it, loss=0.3956, lr=9.89e-06]Steps:  16%|█▌        | 807/5000 [2:53:04<13:51:26, 11.90s/it, loss=1.1398, lr=9.89e-06]Steps:  16%|█▌        | 808/5000 [2:53:16<13:51:20, 11.90s/it, loss=1.1398, lr=9.89e-06]Steps:  16%|█▌        | 808/5000 [2:53:16<13:51:20, 11.90s/it, loss=1.1541, lr=9.88e-06]Steps:  16%|█▌        | 809/5000 [2:53:28<13:51:47, 11.91s/it, loss=1.1541, lr=9.88e-06]Steps:  16%|█▌        | 809/5000 [2:53:28<13:51:47, 11.91s/it, loss=1.1249, lr=9.88e-06]Steps:  16%|█▌        | 810/5000 [2:53:40<13:54:57, 11.96s/it, loss=1.1249, lr=9.88e-06]Steps:  16%|█▌        | 810/5000 [2:53:40<13:54:57, 11.96s/it, loss=0.6033, lr=9.88e-06]
[Step 810] Training Debug Info:
  Loss: 0.583777
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0001, std: 0.8828
  Noise mean: -0.0011, std: 1.0000
  Target mean: -0.0010, std: 1.3359
  Model pred mean: 0.0033, std: 1.0859
  Sigmas: [0.60546875]... (timesteps: [604.0])

[Step 810] Training Debug Info:
  Loss: 0.873506
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0027, std: 0.9688
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0018, std: 1.3984
  Model pred mean: -0.0016, std: 1.0312
  Sigmas: [0.3984375]... (timesteps: [399.0])

[Step 810] Training Debug Info:
  Loss: 1.069420
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0009, std: 0.9375
  Noise mean: -0.0005, std: 1.0000
  Target mean: 0.0005, std: 1.3672
  Model pred mean: 0.0020, std: 0.9141
  Sigmas: [0.052978515625]... (timesteps: [53.0])

[Step 810] Training Debug Info:
  Loss: 1.087436
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0173, std: 0.8945
  Noise mean: 0.0026, std: 1.0000
  Target mean: 0.0199, std: 1.3359
  Model pred mean: 0.0199, std: 0.8555
  Sigmas: [0.275390625]... (timesteps: [276.0])
Steps:  16%|█▌        | 811/5000 [2:53:52<13:52:51, 11.93s/it, loss=0.6033, lr=9.88e-06]Steps:  16%|█▌        | 811/5000 [2:53:52<13:52:51, 11.93s/it, loss=1.0874, lr=9.88e-06]Steps:  16%|█▌        | 812/5000 [2:54:04<13:50:57, 11.90s/it, loss=1.0874, lr=9.88e-06]Steps:  16%|█▌        | 812/5000 [2:54:04<13:50:57, 11.90s/it, loss=0.7534, lr=9.88e-06]Steps:  16%|█▋        | 813/5000 [2:54:16<13:49:15, 11.88s/it, loss=0.7534, lr=9.88e-06]Steps:  16%|█▋        | 813/5000 [2:54:16<13:49:15, 11.88s/it, loss=0.9402, lr=9.88e-06]Steps:  16%|█▋        | 814/5000 [2:54:28<13:51:45, 11.92s/it, loss=0.9402, lr=9.88e-06]Steps:  16%|█▋        | 814/5000 [2:54:28<13:51:45, 11.92s/it, loss=0.3956, lr=9.88e-06]Steps:  16%|█▋        | 815/5000 [2:54:40<13:51:36, 11.92s/it, loss=0.3956, lr=9.88e-06]Steps:  16%|█▋        | 815/5000 [2:54:40<13:51:36, 11.92s/it, loss=0.4620, lr=9.88e-06]Steps:  16%|█▋        | 816/5000 [2:54:52<13:51:18, 11.92s/it, loss=0.4620, lr=9.88e-06]Steps:  16%|█▋        | 816/5000 [2:54:52<13:51:18, 11.92s/it, loss=0.4235, lr=9.88e-06]Steps:  16%|█▋        | 817/5000 [2:55:04<13:50:20, 11.91s/it, loss=0.4235, lr=9.88e-06]Steps:  16%|█▋        | 817/5000 [2:55:04<13:50:20, 11.91s/it, loss=1.1161, lr=9.88e-06]Steps:  16%|█▋        | 818/5000 [2:55:15<13:49:58, 11.91s/it, loss=1.1161, lr=9.88e-06]Steps:  16%|█▋        | 818/5000 [2:55:15<13:49:58, 11.91s/it, loss=0.5776, lr=9.88e-06]Steps:  16%|█▋        | 819/5000 [2:55:27<13:51:09, 11.93s/it, loss=0.5776, lr=9.88e-06]Steps:  16%|█▋        | 819/5000 [2:55:27<13:51:09, 11.93s/it, loss=1.1118, lr=9.88e-06]Steps:  16%|█▋        | 820/5000 [2:55:39<13:49:46, 11.91s/it, loss=1.1118, lr=9.88e-06]Steps:  16%|█▋        | 820/5000 [2:55:39<13:49:46, 11.91s/it, loss=0.5423, lr=9.88e-06]
[Step 820] Training Debug Info:
  Loss: 0.757035
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0081, std: 0.9258
  Noise mean: 0.0002, std: 1.0000
  Target mean: -0.0079, std: 1.3672
  Model pred mean: -0.0059, std: 1.0547
  Sigmas: [0.439453125]... (timesteps: [440.0])

[Step 820] Training Debug Info:
  Loss: 1.082037
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0167, std: 0.9453
  Noise mean: -0.0027, std: 1.0000
  Target mean: -0.0194, std: 1.3750
  Model pred mean: -0.0164, std: 0.8789
  Sigmas: [0.08203125]... (timesteps: [82.0])

[Step 820] Training Debug Info:
  Loss: 0.587958
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0208, std: 0.8086
  Noise mean: 0.0007, std: 1.0078
  Target mean: 0.0214, std: 1.2891
  Model pred mean: 0.0214, std: 1.0391
  Sigmas: [0.63671875]... (timesteps: [635.0])

[Step 820] Training Debug Info:
  Loss: 1.176637
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0077, std: 0.8945
  Noise mean: 0.0033, std: 1.0000
  Target mean: -0.0045, std: 1.3438
  Model pred mean: -0.0090, std: 0.7812
  Sigmas: [0.2109375]... (timesteps: [211.0])
Steps:  16%|█▋        | 821/5000 [2:55:51<13:48:37, 11.90s/it, loss=0.5423, lr=9.88e-06]Steps:  16%|█▋        | 821/5000 [2:55:51<13:48:37, 11.90s/it, loss=1.1766, lr=9.87e-06]Steps:  16%|█▋        | 822/5000 [2:56:03<13:46:54, 11.88s/it, loss=1.1766, lr=9.87e-06]Steps:  16%|█▋        | 822/5000 [2:56:03<13:46:54, 11.88s/it, loss=0.9505, lr=9.87e-06]Steps:  16%|█▋        | 823/5000 [2:56:15<13:47:25, 11.89s/it, loss=0.9505, lr=9.87e-06]Steps:  16%|█▋        | 823/5000 [2:56:15<13:47:25, 11.89s/it, loss=1.1807, lr=9.87e-06]Steps:  16%|█▋        | 824/5000 [2:56:27<13:45:48, 11.87s/it, loss=1.1807, lr=9.87e-06]Steps:  16%|█▋        | 824/5000 [2:56:27<13:45:48, 11.87s/it, loss=1.1658, lr=9.87e-06]Steps:  16%|█▋        | 825/5000 [2:56:39<13:48:01, 11.90s/it, loss=1.1658, lr=9.87e-06]Steps:  16%|█▋        | 825/5000 [2:56:39<13:48:01, 11.90s/it, loss=0.3705, lr=9.87e-06]Steps:  17%|█▋        | 826/5000 [2:56:51<13:47:12, 11.89s/it, loss=0.3705, lr=9.87e-06]Steps:  17%|█▋        | 826/5000 [2:56:51<13:47:12, 11.89s/it, loss=0.6022, lr=9.87e-06]Steps:  17%|█▋        | 827/5000 [2:57:02<13:46:45, 11.89s/it, loss=0.6022, lr=9.87e-06]Steps:  17%|█▋        | 827/5000 [2:57:02<13:46:45, 11.89s/it, loss=0.9800, lr=9.87e-06]Steps:  17%|█▋        | 828/5000 [2:57:14<13:49:12, 11.93s/it, loss=0.9800, lr=9.87e-06]Steps:  17%|█▋        | 828/5000 [2:57:14<13:49:12, 11.93s/it, loss=0.6613, lr=9.87e-06]Steps:  17%|█▋        | 829/5000 [2:57:26<13:47:51, 11.91s/it, loss=0.6613, lr=9.87e-06]Steps:  17%|█▋        | 829/5000 [2:57:26<13:47:51, 11.91s/it, loss=0.8242, lr=9.87e-06]Steps:  17%|█▋        | 830/5000 [2:57:38<13:48:12, 11.92s/it, loss=0.8242, lr=9.87e-06]Steps:  17%|█▋        | 830/5000 [2:57:38<13:48:12, 11.92s/it, loss=0.8491, lr=9.87e-06]
[Step 830] Training Debug Info:
  Loss: 0.655162
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0071, std: 0.9141
  Noise mean: 0.0017, std: 1.0000
  Target mean: -0.0054, std: 1.3516
  Model pred mean: -0.0106, std: 1.0703
  Sigmas: [0.98828125]... (timesteps: [987.0])

[Step 830] Training Debug Info:
  Loss: 0.515487
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0261, std: 0.9961
  Noise mean: -0.0001, std: 1.0000
  Target mean: -0.0262, std: 1.4141
  Model pred mean: -0.0251, std: 1.2188
  Sigmas: [0.58984375]... (timesteps: [589.0])

[Step 830] Training Debug Info:
  Loss: 0.523240
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: -0.0063, std: 0.9062
  Noise mean: -0.0012, std: 1.0000
  Target mean: 0.0052, std: 1.3516
  Model pred mean: 0.0041, std: 1.1328
  Sigmas: [0.63671875]... (timesteps: [636.0])

[Step 830] Training Debug Info:
  Loss: 0.490370
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0194, std: 0.8828
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0176, std: 1.3359
  Model pred mean: -0.0255, std: 1.1328
  Sigmas: [0.71484375]... (timesteps: [714.0])
Steps:  17%|█▋        | 831/5000 [2:57:50<13:47:48, 11.91s/it, loss=0.8491, lr=9.87e-06]Steps:  17%|█▋        | 831/5000 [2:57:50<13:47:48, 11.91s/it, loss=0.4904, lr=9.87e-06]Steps:  17%|█▋        | 832/5000 [2:58:02<13:45:42, 11.89s/it, loss=0.4904, lr=9.87e-06]Steps:  17%|█▋        | 832/5000 [2:58:02<13:45:42, 11.89s/it, loss=0.5925, lr=9.87e-06]Steps:  17%|█▋        | 833/5000 [2:58:14<13:45:13, 11.88s/it, loss=0.5925, lr=9.87e-06]Steps:  17%|█▋        | 833/5000 [2:58:14<13:45:13, 11.88s/it, loss=0.4669, lr=9.87e-06]Steps:  17%|█▋        | 834/5000 [2:58:26<13:46:01, 11.90s/it, loss=0.4669, lr=9.87e-06]Steps:  17%|█▋        | 834/5000 [2:58:26<13:46:01, 11.90s/it, loss=0.3806, lr=9.86e-06]Steps:  17%|█▋        | 835/5000 [2:58:38<13:47:12, 11.92s/it, loss=0.3806, lr=9.86e-06]Steps:  17%|█▋        | 835/5000 [2:58:38<13:47:12, 11.92s/it, loss=0.9638, lr=9.86e-06]Steps:  17%|█▋        | 836/5000 [2:58:50<13:46:14, 11.91s/it, loss=0.9638, lr=9.86e-06]Steps:  17%|█▋        | 836/5000 [2:58:50<13:46:14, 11.91s/it, loss=0.9586, lr=9.86e-06]Steps:  17%|█▋        | 837/5000 [2:59:02<13:48:43, 11.94s/it, loss=0.9586, lr=9.86e-06]Steps:  17%|█▋        | 837/5000 [2:59:02<13:48:43, 11.94s/it, loss=0.4274, lr=9.86e-06]Steps:  17%|█▋        | 838/5000 [2:59:14<13:47:59, 11.94s/it, loss=0.4274, lr=9.86e-06]Steps:  17%|█▋        | 838/5000 [2:59:14<13:47:59, 11.94s/it, loss=0.8152, lr=9.86e-06]Steps:  17%|█▋        | 839/5000 [2:59:26<13:47:35, 11.93s/it, loss=0.8152, lr=9.86e-06]Steps:  17%|█▋        | 839/5000 [2:59:26<13:47:35, 11.93s/it, loss=0.5111, lr=9.86e-06]Steps:  17%|█▋        | 840/5000 [2:59:37<13:45:43, 11.91s/it, loss=0.5111, lr=9.86e-06]Steps:  17%|█▋        | 840/5000 [2:59:37<13:45:43, 11.91s/it, loss=0.5131, lr=9.86e-06]
[Step 840] Training Debug Info:
  Loss: 1.028792
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0286, std: 0.9180
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0284, std: 1.3594
  Model pred mean: -0.0291, std: 0.8945
  Sigmas: [0.244140625]... (timesteps: [244.0])

[Step 840] Training Debug Info:
  Loss: 1.110327
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0425, std: 0.9102
  Noise mean: 0.0012, std: 1.0000
  Target mean: 0.0435, std: 1.3516
  Model pred mean: 0.0369, std: 0.8359
  Sigmas: [0.302734375]... (timesteps: [302.0])

[Step 840] Training Debug Info:
  Loss: 0.606281
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0322, std: 0.9336
  Noise mean: -0.0010, std: 1.0000
  Target mean: -0.0332, std: 1.3672
  Model pred mean: -0.0278, std: 1.1250
  Sigmas: [0.91015625]... (timesteps: [912.0])

[Step 840] Training Debug Info:
  Loss: 1.089535
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0149, std: 0.8906
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0145, std: 1.3359
  Model pred mean: -0.0201, std: 0.8359
  Sigmas: [0.220703125]... (timesteps: [221.0])
Steps:  17%|█▋        | 841/5000 [2:59:49<13:45:41, 11.91s/it, loss=0.5131, lr=9.86e-06]Steps:  17%|█▋        | 841/5000 [2:59:49<13:45:41, 11.91s/it, loss=1.0895, lr=9.86e-06]Steps:  17%|█▋        | 842/5000 [3:00:01<13:45:52, 11.92s/it, loss=1.0895, lr=9.86e-06]Steps:  17%|█▋        | 842/5000 [3:00:01<13:45:52, 11.92s/it, loss=1.0953, lr=9.86e-06]Steps:  17%|█▋        | 843/5000 [3:00:13<13:44:49, 11.91s/it, loss=1.0953, lr=9.86e-06]Steps:  17%|█▋        | 843/5000 [3:00:13<13:44:49, 11.91s/it, loss=0.4577, lr=9.86e-06]Steps:  17%|█▋        | 844/5000 [3:00:25<13:44:58, 11.91s/it, loss=0.4577, lr=9.86e-06]Steps:  17%|█▋        | 844/5000 [3:00:25<13:44:58, 11.91s/it, loss=1.0808, lr=9.86e-06]Steps:  17%|█▋        | 845/5000 [3:00:37<13:46:51, 11.94s/it, loss=1.0808, lr=9.86e-06]Steps:  17%|█▋        | 845/5000 [3:00:37<13:46:51, 11.94s/it, loss=0.4160, lr=9.86e-06]Steps:  17%|█▋        | 846/5000 [3:00:49<13:46:56, 11.94s/it, loss=0.4160, lr=9.86e-06]Steps:  17%|█▋        | 846/5000 [3:00:49<13:46:56, 11.94s/it, loss=0.4320, lr=9.85e-06]Steps:  17%|█▋        | 847/5000 [3:01:01<13:46:36, 11.94s/it, loss=0.4320, lr=9.85e-06]Steps:  17%|█▋        | 847/5000 [3:01:01<13:46:36, 11.94s/it, loss=0.3819, lr=9.85e-06]Steps:  17%|█▋        | 848/5000 [3:01:13<13:46:23, 11.94s/it, loss=0.3819, lr=9.85e-06]Steps:  17%|█▋        | 848/5000 [3:01:13<13:46:23, 11.94s/it, loss=0.7943, lr=9.85e-06]Steps:  17%|█▋        | 849/5000 [3:01:25<13:46:31, 11.95s/it, loss=0.7943, lr=9.85e-06]Steps:  17%|█▋        | 849/5000 [3:01:25<13:46:31, 11.95s/it, loss=0.6353, lr=9.85e-06]Steps:  17%|█▋        | 850/5000 [3:01:37<13:45:47, 11.94s/it, loss=0.6353, lr=9.85e-06]Steps:  17%|█▋        | 850/5000 [3:01:37<13:45:47, 11.94s/it, loss=0.4430, lr=9.85e-06]
[Step 850] Training Debug Info:
  Loss: 1.047368
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0032, std: 1.0000
  Noise mean: 0.0019, std: 1.0000
  Target mean: -0.0012, std: 1.4141
  Model pred mean: -0.0077, std: 0.9844
  Sigmas: [0.28125]... (timesteps: [281.0])

[Step 850] Training Debug Info:
  Loss: 0.342787
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0320, std: 0.9180
  Noise mean: 0.0024, std: 1.0000
  Target mean: 0.0344, std: 1.3594
  Model pred mean: 0.0254, std: 1.2266
  Sigmas: [0.86328125]... (timesteps: [862.0])

[Step 850] Training Debug Info:
  Loss: 1.142525
  Latent shape: torch.Size([1, 32, 138, 66]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0332, std: 0.9531
  Noise mean: 0.0023, std: 1.0000
  Target mean: 0.0354, std: 1.3828
  Model pred mean: 0.0310, std: 0.8867
  Sigmas: [0.17578125]... (timesteps: [176.0])

[Step 850] Training Debug Info:
  Loss: 1.140390
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0151, std: 0.8867
  Noise mean: 0.0022, std: 1.0000
  Target mean: -0.0129, std: 1.3359
  Model pred mean: -0.0173, std: 0.8164
  Sigmas: [0.26953125]... (timesteps: [270.0])
Steps:  17%|█▋        | 851/5000 [3:01:49<13:45:28, 11.94s/it, loss=0.4430, lr=9.85e-06]Steps:  17%|█▋        | 851/5000 [3:01:49<13:45:28, 11.94s/it, loss=1.1404, lr=9.85e-06]Steps:  17%|█▋        | 852/5000 [3:02:01<13:45:04, 11.93s/it, loss=1.1404, lr=9.85e-06]Steps:  17%|█▋        | 852/5000 [3:02:01<13:45:04, 11.93s/it, loss=0.4392, lr=9.85e-06]Steps:  17%|█▋        | 853/5000 [3:02:12<13:43:21, 11.91s/it, loss=0.4392, lr=9.85e-06]Steps:  17%|█▋        | 853/5000 [3:02:12<13:43:21, 11.91s/it, loss=0.5455, lr=9.85e-06]Steps:  17%|█▋        | 854/5000 [3:02:24<13:43:27, 11.92s/it, loss=0.5455, lr=9.85e-06]Steps:  17%|█▋        | 854/5000 [3:02:24<13:43:27, 11.92s/it, loss=0.4196, lr=9.85e-06]Steps:  17%|█▋        | 855/5000 [3:02:37<13:47:05, 11.97s/it, loss=0.4196, lr=9.85e-06]Steps:  17%|█▋        | 855/5000 [3:02:37<13:47:05, 11.97s/it, loss=1.1113, lr=9.85e-06]Steps:  17%|█▋        | 856/5000 [3:02:48<13:44:51, 11.94s/it, loss=1.1113, lr=9.85e-06]Steps:  17%|█▋        | 856/5000 [3:02:48<13:44:51, 11.94s/it, loss=0.4643, lr=9.85e-06]Steps:  17%|█▋        | 857/5000 [3:03:00<13:42:49, 11.92s/it, loss=0.4643, lr=9.85e-06]Steps:  17%|█▋        | 857/5000 [3:03:00<13:42:49, 11.92s/it, loss=0.4219, lr=9.85e-06]Steps:  17%|█▋        | 858/5000 [3:03:12<13:42:18, 11.91s/it, loss=0.4219, lr=9.85e-06]Steps:  17%|█▋        | 858/5000 [3:03:12<13:42:18, 11.91s/it, loss=0.8388, lr=9.84e-06]Steps:  17%|█▋        | 859/5000 [3:03:24<13:41:57, 11.91s/it, loss=0.8388, lr=9.84e-06]Steps:  17%|█▋        | 859/5000 [3:03:24<13:41:57, 11.91s/it, loss=0.3809, lr=9.84e-06]Steps:  17%|█▋        | 860/5000 [3:03:36<13:40:03, 11.88s/it, loss=0.3809, lr=9.84e-06]Steps:  17%|█▋        | 860/5000 [3:03:36<13:40:03, 11.88s/it, loss=0.6624, lr=9.84e-06]
[Step 860] Training Debug Info:
  Loss: 0.422895
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0106, std: 0.9258
  Noise mean: 0.0005, std: 1.0000
  Target mean: -0.0102, std: 1.3594
  Model pred mean: -0.0103, std: 1.1953
  Sigmas: [0.92578125]... (timesteps: [926.0])

[Step 860] Training Debug Info:
  Loss: 0.612476
  Latent shape: torch.Size([1, 32, 114, 78]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0060, std: 0.8633
  Noise mean: 0.0007, std: 1.0000
  Target mean: 0.0067, std: 1.3203
  Model pred mean: 0.0083, std: 1.0625
  Sigmas: [0.62890625]... (timesteps: [630.0])

[Step 860] Training Debug Info:
  Loss: 0.924214
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: 0.0042, std: 0.9141
  Noise mean: 0.0009, std: 1.0000
  Target mean: -0.0033, std: 1.3594
  Model pred mean: -0.0062, std: 0.9531
  Sigmas: [0.361328125]... (timesteps: [362.0])

[Step 860] Training Debug Info:
  Loss: 1.094520
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0081, std: 0.9219
  Noise mean: 0.0010, std: 1.0000
  Target mean: -0.0071, std: 1.3594
  Model pred mean: -0.0100, std: 0.8672
  Sigmas: [0.06103515625]... (timesteps: [61.0])
Steps:  17%|█▋        | 861/5000 [3:03:48<13:38:12, 11.86s/it, loss=0.6624, lr=9.84e-06]Steps:  17%|█▋        | 861/5000 [3:03:48<13:38:12, 11.86s/it, loss=1.0945, lr=9.84e-06]Steps:  17%|█▋        | 862/5000 [3:04:00<13:38:39, 11.87s/it, loss=1.0945, lr=9.84e-06]Steps:  17%|█▋        | 862/5000 [3:04:00<13:38:39, 11.87s/it, loss=0.3980, lr=9.84e-06]Steps:  17%|█▋        | 863/5000 [3:04:11<13:38:24, 11.87s/it, loss=0.3980, lr=9.84e-06]Steps:  17%|█▋        | 863/5000 [3:04:11<13:38:24, 11.87s/it, loss=0.4786, lr=9.84e-06]Steps:  17%|█▋        | 864/5000 [3:04:23<13:40:40, 11.91s/it, loss=0.4786, lr=9.84e-06]Steps:  17%|█▋        | 864/5000 [3:04:23<13:40:40, 11.91s/it, loss=0.7097, lr=9.84e-06]Steps:  17%|█▋        | 865/5000 [3:04:35<13:40:21, 11.90s/it, loss=0.7097, lr=9.84e-06]Steps:  17%|█▋        | 865/5000 [3:04:35<13:40:21, 11.90s/it, loss=0.7035, lr=9.84e-06]Steps:  17%|█▋        | 866/5000 [3:04:47<13:38:35, 11.88s/it, loss=0.7035, lr=9.84e-06]Steps:  17%|█▋        | 866/5000 [3:04:47<13:38:35, 11.88s/it, loss=0.4020, lr=9.84e-06]Steps:  17%|█▋        | 867/5000 [3:04:59<13:38:53, 11.89s/it, loss=0.4020, lr=9.84e-06]Steps:  17%|█▋        | 867/5000 [3:04:59<13:38:53, 11.89s/it, loss=1.1599, lr=9.84e-06]Steps:  17%|█▋        | 868/5000 [3:05:11<13:40:11, 11.91s/it, loss=1.1599, lr=9.84e-06]Steps:  17%|█▋        | 868/5000 [3:05:11<13:40:11, 11.91s/it, loss=0.6133, lr=9.84e-06]Steps:  17%|█▋        | 869/5000 [3:05:23<13:38:29, 11.89s/it, loss=0.6133, lr=9.84e-06]Steps:  17%|█▋        | 869/5000 [3:05:23<13:38:29, 11.89s/it, loss=0.7262, lr=9.84e-06]Steps:  17%|█▋        | 870/5000 [3:05:35<13:38:22, 11.89s/it, loss=0.7262, lr=9.84e-06]Steps:  17%|█▋        | 870/5000 [3:05:35<13:38:22, 11.89s/it, loss=0.4945, lr=9.83e-06]
[Step 870] Training Debug Info:
  Loss: 1.198424
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: -0.0045, std: 0.9258
  Noise mean: -0.0036, std: 1.0000
  Target mean: 0.0010, std: 1.3672
  Model pred mean: 0.0091, std: 0.8086
  Sigmas: [0.1953125]... (timesteps: [195.0])

[Step 870] Training Debug Info:
  Loss: 0.388794
  Latent shape: torch.Size([1, 32, 108, 78]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0413, std: 0.9492
  Noise mean: -0.0003, std: 0.9961
  Target mean: -0.0415, std: 1.3750
  Model pred mean: -0.0413, std: 1.2266
  Sigmas: [0.90625]... (timesteps: [908.0])

[Step 870] Training Debug Info:
  Loss: 1.105028
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0066, std: 0.8633
  Noise mean: 0.0011, std: 1.0000
  Target mean: -0.0054, std: 1.3203
  Model pred mean: 0.0002, std: 0.7969
  Sigmas: [0.05810546875]... (timesteps: [58.0])

[Step 870] Training Debug Info:
  Loss: 1.026044
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0302, std: 0.9375
  Noise mean: 0.0012, std: 1.0000
  Target mean: -0.0289, std: 1.3672
  Model pred mean: -0.0267, std: 0.9141
  Sigmas: [0.26171875]... (timesteps: [262.0])
Steps:  17%|█▋        | 871/5000 [3:05:47<13:37:07, 11.87s/it, loss=0.4945, lr=9.83e-06]Steps:  17%|█▋        | 871/5000 [3:05:47<13:37:07, 11.87s/it, loss=1.0260, lr=9.83e-06]Steps:  17%|█▋        | 872/5000 [3:05:58<13:37:16, 11.88s/it, loss=1.0260, lr=9.83e-06]Steps:  17%|█▋        | 872/5000 [3:05:58<13:37:16, 11.88s/it, loss=0.4096, lr=9.83e-06]Steps:  17%|█▋        | 873/5000 [3:06:10<13:37:13, 11.88s/it, loss=0.4096, lr=9.83e-06]Steps:  17%|█▋        | 873/5000 [3:06:10<13:37:13, 11.88s/it, loss=0.4417, lr=9.83e-06]Steps:  17%|█▋        | 874/5000 [3:06:22<13:37:49, 11.89s/it, loss=0.4417, lr=9.83e-06]Steps:  17%|█▋        | 874/5000 [3:06:22<13:37:49, 11.89s/it, loss=0.4945, lr=9.83e-06]Steps:  18%|█▊        | 875/5000 [3:06:34<13:37:19, 11.89s/it, loss=0.4945, lr=9.83e-06]Steps:  18%|█▊        | 875/5000 [3:06:34<13:37:19, 11.89s/it, loss=0.6610, lr=9.83e-06]Steps:  18%|█▊        | 876/5000 [3:06:46<13:38:35, 11.91s/it, loss=0.6610, lr=9.83e-06]Steps:  18%|█▊        | 876/5000 [3:06:46<13:38:35, 11.91s/it, loss=0.7762, lr=9.83e-06]Steps:  18%|█▊        | 877/5000 [3:06:58<13:37:54, 11.90s/it, loss=0.7762, lr=9.83e-06]Steps:  18%|█▊        | 877/5000 [3:06:58<13:37:54, 11.90s/it, loss=1.1382, lr=9.83e-06]Steps:  18%|█▊        | 878/5000 [3:07:10<13:38:43, 11.92s/it, loss=1.1382, lr=9.83e-06]Steps:  18%|█▊        | 878/5000 [3:07:10<13:38:43, 11.92s/it, loss=1.1084, lr=9.83e-06]Steps:  18%|█▊        | 879/5000 [3:07:22<13:37:19, 11.90s/it, loss=1.1084, lr=9.83e-06]Steps:  18%|█▊        | 879/5000 [3:07:22<13:37:19, 11.90s/it, loss=0.7971, lr=9.83e-06]Steps:  18%|█▊        | 880/5000 [3:07:34<13:35:37, 11.88s/it, loss=0.7971, lr=9.83e-06]Steps:  18%|█▊        | 880/5000 [3:07:34<13:35:37, 11.88s/it, loss=1.1569, lr=9.83e-06]
[Step 880] Training Debug Info:
  Loss: 0.452661
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0102, std: 0.9258
  Noise mean: 0.0014, std: 0.9961
  Target mean: -0.0089, std: 1.3594
  Model pred mean: -0.0150, std: 1.1797
  Sigmas: [0.84375]... (timesteps: [845.0])

[Step 880] Training Debug Info:
  Loss: 0.448534
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0052, std: 0.9258
  Noise mean: -0.0016, std: 1.0000
  Target mean: -0.0068, std: 1.3594
  Model pred mean: -0.0111, std: 1.1797
  Sigmas: [0.8984375]... (timesteps: [900.0])

[Step 880] Training Debug Info:
  Loss: 0.623310
  Latent shape: torch.Size([1, 32, 54, 162]), Packed shape: torch.Size([1, 2187, 128])
  Latent mean: 0.0503, std: 0.9492
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0488, std: 1.3750
  Model pred mean: -0.0483, std: 1.1406
  Sigmas: [0.890625]... (timesteps: [891.0])

[Step 880] Training Debug Info:
  Loss: 0.366405
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0062, std: 0.9258
  Noise mean: -0.0002, std: 1.0000
  Target mean: -0.0064, std: 1.3594
  Model pred mean: -0.0054, std: 1.2109
  Sigmas: [0.796875]... (timesteps: [796.0])
Steps:  18%|█▊        | 881/5000 [3:07:46<13:38:34, 11.92s/it, loss=1.1569, lr=9.83e-06]Steps:  18%|█▊        | 881/5000 [3:07:46<13:38:34, 11.92s/it, loss=0.3664, lr=9.82e-06]Steps:  18%|█▊        | 882/5000 [3:07:58<13:39:06, 11.93s/it, loss=0.3664, lr=9.82e-06]Steps:  18%|█▊        | 882/5000 [3:07:58<13:39:06, 11.93s/it, loss=0.4979, lr=9.82e-06]Steps:  18%|█▊        | 883/5000 [3:08:10<13:38:05, 11.92s/it, loss=0.4979, lr=9.82e-06]Steps:  18%|█▊        | 883/5000 [3:08:10<13:38:05, 11.92s/it, loss=1.2061, lr=9.82e-06]Steps:  18%|█▊        | 884/5000 [3:08:21<13:36:49, 11.91s/it, loss=1.2061, lr=9.82e-06]Steps:  18%|█▊        | 884/5000 [3:08:21<13:36:49, 11.91s/it, loss=0.3810, lr=9.82e-06]Steps:  18%|█▊        | 885/5000 [3:08:33<13:38:19, 11.93s/it, loss=0.3810, lr=9.82e-06]Steps:  18%|█▊        | 885/5000 [3:08:33<13:38:19, 11.93s/it, loss=0.5859, lr=9.82e-06]Steps:  18%|█▊        | 886/5000 [3:08:45<13:37:38, 11.92s/it, loss=0.5859, lr=9.82e-06]Steps:  18%|█▊        | 886/5000 [3:08:45<13:37:38, 11.92s/it, loss=0.4736, lr=9.82e-06]Steps:  18%|█▊        | 887/5000 [3:08:57<13:37:04, 11.92s/it, loss=0.4736, lr=9.82e-06]Steps:  18%|█▊        | 887/5000 [3:08:57<13:37:04, 11.92s/it, loss=0.8992, lr=9.82e-06]Steps:  18%|█▊        | 888/5000 [3:09:09<13:38:28, 11.94s/it, loss=0.8992, lr=9.82e-06]Steps:  18%|█▊        | 888/5000 [3:09:09<13:38:28, 11.94s/it, loss=0.6201, lr=9.82e-06]Steps:  18%|█▊        | 889/5000 [3:09:21<13:37:29, 11.93s/it, loss=0.6201, lr=9.82e-06]Steps:  18%|█▊        | 889/5000 [3:09:21<13:37:29, 11.93s/it, loss=0.6776, lr=9.82e-06]Steps:  18%|█▊        | 890/5000 [3:09:33<13:37:13, 11.93s/it, loss=0.6776, lr=9.82e-06]Steps:  18%|█▊        | 890/5000 [3:09:33<13:37:13, 11.93s/it, loss=1.1027, lr=9.82e-06]
[Step 890] Training Debug Info:
  Loss: 1.148709
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0071, std: 0.9102
  Noise mean: 0.0025, std: 1.0000
  Target mean: -0.0046, std: 1.3516
  Model pred mean: -0.0103, std: 0.8242
  Sigmas: [0.1494140625]... (timesteps: [149.0])

[Step 890] Training Debug Info:
  Loss: 1.047897
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0172, std: 0.8945
  Noise mean: 0.0006, std: 1.0000
  Target mean: -0.0166, std: 1.3438
  Model pred mean: -0.0315, std: 0.8750
  Sigmas: [0.02099609375]... (timesteps: [21.0])

[Step 890] Training Debug Info:
  Loss: 0.368120
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0086, std: 0.9141
  Noise mean: 0.0004, std: 1.0000
  Target mean: -0.0082, std: 1.3516
  Model pred mean: -0.0067, std: 1.2031
  Sigmas: [0.86328125]... (timesteps: [862.0])

[Step 890] Training Debug Info:
  Loss: 0.416762
  Latent shape: torch.Size([1, 32, 84, 102]), Packed shape: torch.Size([1, 2142, 128])
  Latent mean: -0.0315, std: 0.9102
  Noise mean: -0.0001, std: 1.0000
  Target mean: 0.0312, std: 1.3516
  Model pred mean: 0.0291, std: 1.1797
  Sigmas: [0.765625]... (timesteps: [767.0])
Steps:  18%|█▊        | 891/5000 [3:09:45<13:39:19, 11.96s/it, loss=1.1027, lr=9.82e-06]Steps:  18%|█▊        | 891/5000 [3:09:45<13:39:19, 11.96s/it, loss=0.4168, lr=9.81e-06]Steps:  18%|█▊        | 892/5000 [3:09:57<13:37:30, 11.94s/it, loss=0.4168, lr=9.81e-06]Steps:  18%|█▊        | 892/5000 [3:09:57<13:37:30, 11.94s/it, loss=1.1189, lr=9.81e-06]Steps:  18%|█▊        | 893/5000 [3:10:09<13:36:38, 11.93s/it, loss=1.1189, lr=9.81e-06]Steps:  18%|█▊        | 893/5000 [3:10:09<13:36:38, 11.93s/it, loss=0.4307, lr=9.81e-06]Steps:  18%|█▊        | 894/5000 [3:10:21<13:36:04, 11.93s/it, loss=0.4307, lr=9.81e-06]Steps:  18%|█▊        | 894/5000 [3:10:21<13:36:04, 11.93s/it, loss=0.5173, lr=9.81e-06]Steps:  18%|█▊        | 895/5000 [3:10:33<13:34:36, 11.91s/it, loss=0.5173, lr=9.81e-06]Steps:  18%|█▊        | 895/5000 [3:10:33<13:34:36, 11.91s/it, loss=0.5538, lr=9.81e-06]Steps:  18%|█▊        | 896/5000 [3:10:44<13:33:15, 11.89s/it, loss=0.5538, lr=9.81e-06]Steps:  18%|█▊        | 896/5000 [3:10:44<13:33:15, 11.89s/it, loss=0.7152, lr=9.81e-06]Steps:  18%|█▊        | 897/5000 [3:10:56<13:31:51, 11.87s/it, loss=0.7152, lr=9.81e-06]Steps:  18%|█▊        | 897/5000 [3:10:56<13:31:51, 11.87s/it, loss=1.1536, lr=9.81e-06]Steps:  18%|█▊        | 898/5000 [3:11:08<13:34:17, 11.91s/it, loss=1.1536, lr=9.81e-06]Steps:  18%|█▊        | 898/5000 [3:11:08<13:34:17, 11.91s/it, loss=1.0902, lr=9.81e-06]Steps:  18%|█▊        | 899/5000 [3:11:20<13:31:31, 11.87s/it, loss=1.0902, lr=9.81e-06]Steps:  18%|█▊        | 899/5000 [3:11:20<13:31:31, 11.87s/it, loss=1.0518, lr=9.81e-06]Steps:  18%|█▊        | 900/5000 [3:11:32<13:34:22, 11.92s/it, loss=1.0518, lr=9.81e-06]Steps:  18%|█▊        | 900/5000 [3:11:32<13:34:22, 11.92s/it, loss=0.3849, lr=9.81e-06]01/22/2026 06:36:10 - INFO - __main__ - 
[Step 900] ✅ Loss in normal range (0.3849)
01/22/2026 06:36:10 - INFO - __main__ -   Loss avg (last 100): 0.7345
01/22/2026 06:36:10 - INFO - __main__ -   Loss range: [0.3664, 1.2061]

[Step 900] Training Debug Info:
  Loss: 1.107151
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0160, std: 0.9258
  Noise mean: -0.0003, std: 1.0000
  Target mean: -0.0162, std: 1.3594
  Model pred mean: -0.0187, std: 0.8594
  Sigmas: [0.1982421875]... (timesteps: [198.0])

[Step 900] Training Debug Info:
  Loss: 0.349566
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0270, std: 0.9219
  Noise mean: -0.0025, std: 0.9961
  Target mean: -0.0294, std: 1.3594
  Model pred mean: -0.0302, std: 1.2266
  Sigmas: [0.77734375]... (timesteps: [776.0])

[Step 900] Training Debug Info:
  Loss: 0.651418
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0104, std: 0.9102
  Noise mean: 0.0019, std: 0.9961
  Target mean: -0.0085, std: 1.3516
  Model pred mean: -0.0130, std: 1.0703
  Sigmas: [0.56640625]... (timesteps: [568.0])

[Step 900] Training Debug Info:
  Loss: 0.441120
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0023, std: 0.8867
  Noise mean: 0.0005, std: 1.0000
  Target mean: -0.0018, std: 1.3359
  Model pred mean: -0.0066, std: 1.1641
  Sigmas: [0.7421875]... (timesteps: [742.0])
Steps:  18%|█▊        | 901/5000 [3:11:44<13:34:21, 11.92s/it, loss=0.3849, lr=9.81e-06]Steps:  18%|█▊        | 901/5000 [3:11:44<13:34:21, 11.92s/it, loss=0.4411, lr=9.81e-06]Steps:  18%|█▊        | 902/5000 [3:11:56<13:33:16, 11.91s/it, loss=0.4411, lr=9.81e-06]Steps:  18%|█▊        | 902/5000 [3:11:56<13:33:16, 11.91s/it, loss=0.7064, lr=9.80e-06]Steps:  18%|█▊        | 903/5000 [3:12:08<13:32:55, 11.91s/it, loss=0.7064, lr=9.80e-06]Steps:  18%|█▊        | 903/5000 [3:12:08<13:32:55, 11.91s/it, loss=1.0711, lr=9.80e-06]Steps:  18%|█▊        | 904/5000 [3:12:20<13:32:07, 11.90s/it, loss=1.0711, lr=9.80e-06]Steps:  18%|█▊        | 904/5000 [3:12:20<13:32:07, 11.90s/it, loss=0.7445, lr=9.80e-06]Steps:  18%|█▊        | 905/5000 [3:12:32<13:30:03, 11.87s/it, loss=0.7445, lr=9.80e-06]Steps:  18%|█▊        | 905/5000 [3:12:32<13:30:03, 11.87s/it, loss=0.9888, lr=9.80e-06]Steps:  18%|█▊        | 906/5000 [3:12:43<13:29:58, 11.87s/it, loss=0.9888, lr=9.80e-06]Steps:  18%|█▊        | 906/5000 [3:12:43<13:29:58, 11.87s/it, loss=0.9303, lr=9.80e-06]Steps:  18%|█▊        | 907/5000 [3:12:55<13:29:57, 11.87s/it, loss=0.9303, lr=9.80e-06]Steps:  18%|█▊        | 907/5000 [3:12:55<13:29:57, 11.87s/it, loss=1.0632, lr=9.80e-06]Steps:  18%|█▊        | 908/5000 [3:13:07<13:30:39, 11.89s/it, loss=1.0632, lr=9.80e-06]Steps:  18%|█▊        | 908/5000 [3:13:07<13:30:39, 11.89s/it, loss=0.5030, lr=9.80e-06]Steps:  18%|█▊        | 909/5000 [3:13:19<13:33:12, 11.93s/it, loss=0.5030, lr=9.80e-06]Steps:  18%|█▊        | 909/5000 [3:13:19<13:33:12, 11.93s/it, loss=1.1755, lr=9.80e-06]Steps:  18%|█▊        | 910/5000 [3:13:31<13:32:14, 11.92s/it, loss=1.1755, lr=9.80e-06]Steps:  18%|█▊        | 910/5000 [3:13:31<13:32:14, 11.92s/it, loss=0.9168, lr=9.80e-06]
[Step 910] Training Debug Info:
  Loss: 0.471702
  Latent shape: torch.Size([1, 32, 84, 108]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0117, std: 0.9414
  Noise mean: 0.0007, std: 1.0000
  Target mean: 0.0123, std: 1.3750
  Model pred mean: 0.0023, std: 1.1719
  Sigmas: [0.92578125]... (timesteps: [926.0])

[Step 910] Training Debug Info:
  Loss: 0.590155
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0165, std: 0.9766
  Noise mean: -0.0004, std: 1.0000
  Target mean: 0.0160, std: 1.3984
  Model pred mean: 0.0140, std: 1.1641
  Sigmas: [0.61328125]... (timesteps: [615.0])

[Step 910] Training Debug Info:
  Loss: 0.575125
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0549, std: 0.9062
  Noise mean: -0.0019, std: 1.0000
  Target mean: -0.0569, std: 1.3516
  Model pred mean: -0.0557, std: 1.1172
  Sigmas: [0.55859375]... (timesteps: [557.0])

[Step 910] Training Debug Info:
  Loss: 1.182316
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0049, std: 0.8633
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0054, std: 1.3203
  Model pred mean: -0.0070, std: 0.7500
  Sigmas: [0.2578125]... (timesteps: [258.0])
Steps:  18%|█▊        | 911/5000 [3:13:43<13:32:33, 11.92s/it, loss=0.9168, lr=9.80e-06]Steps:  18%|█▊        | 911/5000 [3:13:43<13:32:33, 11.92s/it, loss=1.1823, lr=9.80e-06]Steps:  18%|█▊        | 912/5000 [3:13:55<13:34:02, 11.95s/it, loss=1.1823, lr=9.80e-06]Steps:  18%|█▊        | 912/5000 [3:13:55<13:34:02, 11.95s/it, loss=1.0735, lr=9.79e-06]Steps:  18%|█▊        | 913/5000 [3:14:07<13:35:02, 11.97s/it, loss=1.0735, lr=9.79e-06]Steps:  18%|█▊        | 913/5000 [3:14:07<13:35:02, 11.97s/it, loss=0.5009, lr=9.79e-06]Steps:  18%|█▊        | 914/5000 [3:14:19<13:34:27, 11.96s/it, loss=0.5009, lr=9.79e-06]Steps:  18%|█▊        | 914/5000 [3:14:19<13:34:27, 11.96s/it, loss=0.4838, lr=9.79e-06]Steps:  18%|█▊        | 915/5000 [3:14:31<13:35:09, 11.97s/it, loss=0.4838, lr=9.79e-06]Steps:  18%|█▊        | 915/5000 [3:14:31<13:35:09, 11.97s/it, loss=0.3620, lr=9.79e-06]Steps:  18%|█▊        | 916/5000 [3:14:43<13:34:40, 11.97s/it, loss=0.3620, lr=9.79e-06]Steps:  18%|█▊        | 916/5000 [3:14:43<13:34:40, 11.97s/it, loss=0.8394, lr=9.79e-06]Steps:  18%|█▊        | 917/5000 [3:14:55<13:36:44, 12.00s/it, loss=0.8394, lr=9.79e-06]Steps:  18%|█▊        | 917/5000 [3:14:55<13:36:44, 12.00s/it, loss=1.1789, lr=9.79e-06]Steps:  18%|█▊        | 918/5000 [3:15:07<13:38:15, 12.03s/it, loss=1.1789, lr=9.79e-06]Steps:  18%|█▊        | 918/5000 [3:15:07<13:38:15, 12.03s/it, loss=0.3791, lr=9.79e-06]Steps:  18%|█▊        | 919/5000 [3:15:19<13:36:41, 12.01s/it, loss=0.3791, lr=9.79e-06]Steps:  18%|█▊        | 919/5000 [3:15:19<13:36:41, 12.01s/it, loss=0.4460, lr=9.79e-06]Steps:  18%|█▊        | 920/5000 [3:15:31<13:32:34, 11.95s/it, loss=0.4460, lr=9.79e-06]Steps:  18%|█▊        | 920/5000 [3:15:31<13:32:34, 11.95s/it, loss=0.3971, lr=9.79e-06]
[Step 920] Training Debug Info:
  Loss: 0.948555
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: -0.0053, std: 0.9180
  Noise mean: 0.0006, std: 1.0000
  Target mean: 0.0060, std: 1.3594
  Model pred mean: 0.0036, std: 0.9492
  Sigmas: [0.388671875]... (timesteps: [389.0])

[Step 920] Training Debug Info:
  Loss: 1.109416
  Latent shape: torch.Size([1, 32, 48, 180]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0234, std: 0.9023
  Noise mean: -0.0008, std: 1.0000
  Target mean: -0.0243, std: 1.3438
  Model pred mean: -0.0256, std: 0.8438
  Sigmas: [0.1767578125]... (timesteps: [177.0])

[Step 920] Training Debug Info:
  Loss: 0.683193
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0134, std: 0.9023
  Noise mean: -0.0011, std: 1.0000
  Target mean: 0.0123, std: 1.3438
  Model pred mean: 0.0120, std: 1.0625
  Sigmas: [0.51171875]... (timesteps: [513.0])

[Step 920] Training Debug Info:
  Loss: 0.687108
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0033, std: 0.9219
  Noise mean: -0.0005, std: 1.0000
  Target mean: -0.0039, std: 1.3594
  Model pred mean: -0.0036, std: 1.0781
  Sigmas: [0.50390625]... (timesteps: [505.0])
Steps:  18%|█▊        | 921/5000 [3:15:43<13:31:53, 11.94s/it, loss=0.3971, lr=9.79e-06]Steps:  18%|█▊        | 921/5000 [3:15:43<13:31:53, 11.94s/it, loss=0.6871, lr=9.79e-06]Steps:  18%|█▊        | 922/5000 [3:15:55<13:30:05, 11.92s/it, loss=0.6871, lr=9.79e-06]Steps:  18%|█▊        | 922/5000 [3:15:55<13:30:05, 11.92s/it, loss=0.4699, lr=9.78e-06]Steps:  18%|█▊        | 923/5000 [3:16:07<13:33:04, 11.97s/it, loss=0.4699, lr=9.78e-06]Steps:  18%|█▊        | 923/5000 [3:16:07<13:33:04, 11.97s/it, loss=0.6400, lr=9.78e-06]Steps:  18%|█▊        | 924/5000 [3:16:19<13:31:36, 11.95s/it, loss=0.6400, lr=9.78e-06]Steps:  18%|█▊        | 924/5000 [3:16:19<13:31:36, 11.95s/it, loss=0.4359, lr=9.78e-06]Steps:  18%|█▊        | 925/5000 [3:16:31<13:32:52, 11.97s/it, loss=0.4359, lr=9.78e-06]Steps:  18%|█▊        | 925/5000 [3:16:31<13:32:52, 11.97s/it, loss=1.1719, lr=9.78e-06]Steps:  19%|█▊        | 926/5000 [3:16:43<13:32:18, 11.96s/it, loss=1.1719, lr=9.78e-06]Steps:  19%|█▊        | 926/5000 [3:16:43<13:32:18, 11.96s/it, loss=0.4402, lr=9.78e-06]Steps:  19%|█▊        | 927/5000 [3:16:55<13:34:05, 11.99s/it, loss=0.4402, lr=9.78e-06]Steps:  19%|█▊        | 927/5000 [3:16:55<13:34:05, 11.99s/it, loss=0.4493, lr=9.78e-06]Steps:  19%|█▊        | 928/5000 [3:17:07<13:30:47, 11.95s/it, loss=0.4493, lr=9.78e-06]Steps:  19%|█▊        | 928/5000 [3:17:07<13:30:47, 11.95s/it, loss=0.6952, lr=9.78e-06]Steps:  19%|█▊        | 929/5000 [3:17:19<13:32:09, 11.97s/it, loss=0.6952, lr=9.78e-06]Steps:  19%|█▊        | 929/5000 [3:17:19<13:32:09, 11.97s/it, loss=0.7307, lr=9.78e-06]Steps:  19%|█▊        | 930/5000 [3:17:30<13:31:17, 11.96s/it, loss=0.7307, lr=9.78e-06]Steps:  19%|█▊        | 930/5000 [3:17:30<13:31:17, 11.96s/it, loss=0.4212, lr=9.78e-06]
[Step 930] Training Debug Info:
  Loss: 1.051618
  Latent shape: torch.Size([1, 32, 66, 126]), Packed shape: torch.Size([1, 2079, 128])
  Latent mean: -0.0228, std: 0.9023
  Noise mean: -0.0025, std: 1.0000
  Target mean: 0.0203, std: 1.3438
  Model pred mean: 0.0250, std: 0.8789
  Sigmas: [0.02294921875]... (timesteps: [23.0])

[Step 930] Training Debug Info:
  Loss: 1.183393
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0124, std: 0.8867
  Noise mean: 0.0040, std: 1.0000
  Target mean: 0.0164, std: 1.3359
  Model pred mean: 0.0134, std: 0.7812
  Sigmas: [0.212890625]... (timesteps: [213.0])

[Step 930] Training Debug Info:
  Loss: 0.632755
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0193, std: 0.8789
  Noise mean: -0.0012, std: 1.0000
  Target mean: 0.0181, std: 1.3281
  Model pred mean: 0.0250, std: 1.0625
  Sigmas: [0.578125]... (timesteps: [579.0])

[Step 930] Training Debug Info:
  Loss: 1.083972
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: 0.0108, std: 0.9492
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0093, std: 1.3750
  Model pred mean: -0.0089, std: 0.9102
  Sigmas: [0.06494140625]... (timesteps: [65.0])
Steps:  19%|█▊        | 931/5000 [3:17:42<13:30:04, 11.95s/it, loss=0.4212, lr=9.78e-06]Steps:  19%|█▊        | 931/5000 [3:17:42<13:30:04, 11.95s/it, loss=1.0840, lr=9.78e-06]Steps:  19%|█▊        | 932/5000 [3:17:54<13:30:10, 11.95s/it, loss=1.0840, lr=9.78e-06]Steps:  19%|█▊        | 932/5000 [3:17:54<13:30:10, 11.95s/it, loss=1.0777, lr=9.77e-06]Steps:  19%|█▊        | 933/5000 [3:18:06<13:29:56, 11.95s/it, loss=1.0777, lr=9.77e-06]Steps:  19%|█▊        | 933/5000 [3:18:06<13:29:56, 11.95s/it, loss=0.5132, lr=9.77e-06]Steps:  19%|█▊        | 934/5000 [3:18:18<13:27:47, 11.92s/it, loss=0.5132, lr=9.77e-06]Steps:  19%|█▊        | 934/5000 [3:18:18<13:27:47, 11.92s/it, loss=0.4846, lr=9.77e-06]Steps:  19%|█▊        | 935/5000 [3:18:30<13:27:32, 11.92s/it, loss=0.4846, lr=9.77e-06]Steps:  19%|█▊        | 935/5000 [3:18:30<13:27:32, 11.92s/it, loss=0.5886, lr=9.77e-06]Steps:  19%|█▊        | 936/5000 [3:18:42<13:27:38, 11.92s/it, loss=0.5886, lr=9.77e-06]Steps:  19%|█▊        | 936/5000 [3:18:42<13:27:38, 11.92s/it, loss=0.9223, lr=9.77e-06]Steps:  19%|█▊        | 937/5000 [3:18:54<13:32:01, 11.99s/it, loss=0.9223, lr=9.77e-06]Steps:  19%|█▊        | 937/5000 [3:18:54<13:32:01, 11.99s/it, loss=1.1551, lr=9.77e-06]Steps:  19%|█▉        | 938/5000 [3:19:06<13:31:13, 11.98s/it, loss=1.1551, lr=9.77e-06]Steps:  19%|█▉        | 938/5000 [3:19:06<13:31:13, 11.98s/it, loss=0.4906, lr=9.77e-06]Steps:  19%|█▉        | 939/5000 [3:19:18<13:30:44, 11.98s/it, loss=0.4906, lr=9.77e-06]Steps:  19%|█▉        | 939/5000 [3:19:18<13:30:44, 11.98s/it, loss=0.4249, lr=9.77e-06]Steps:  19%|█▉        | 940/5000 [3:19:30<13:28:01, 11.94s/it, loss=0.4249, lr=9.77e-06]Steps:  19%|█▉        | 940/5000 [3:19:30<13:28:01, 11.94s/it, loss=0.7494, lr=9.77e-06]
[Step 940] Training Debug Info:
  Loss: 0.749219
  Latent shape: torch.Size([1, 32, 114, 78]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: 0.0060, std: 0.9375
  Noise mean: 0.0023, std: 1.0000
  Target mean: -0.0037, std: 1.3672
  Model pred mean: -0.0051, std: 1.0703
  Sigmas: [0.4296875]... (timesteps: [430.0])

[Step 940] Training Debug Info:
  Loss: 1.099748
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0066, std: 0.9258
  Noise mean: 0.0050, std: 1.0000
  Target mean: 0.0117, std: 1.3672
  Model pred mean: 0.0070, std: 0.8750
  Sigmas: [0.244140625]... (timesteps: [244.0])

[Step 940] Training Debug Info:
  Loss: 1.134585
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0190, std: 0.9727
  Noise mean: -0.0023, std: 1.0000
  Target mean: -0.0214, std: 1.3906
  Model pred mean: -0.0175, std: 0.8945
  Sigmas: [0.10986328125]... (timesteps: [110.0])

[Step 940] Training Debug Info:
  Loss: 0.767125
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: -0.0398, std: 0.9258
  Noise mean: -0.0032, std: 1.0000
  Target mean: 0.0366, std: 1.3594
  Model pred mean: 0.0413, std: 1.0469
  Sigmas: [0.453125]... (timesteps: [453.0])
Steps:  19%|█▉        | 941/5000 [3:19:42<13:28:12, 11.95s/it, loss=0.7494, lr=9.77e-06]Steps:  19%|█▉        | 941/5000 [3:19:42<13:28:12, 11.95s/it, loss=0.7671, lr=9.76e-06]Steps:  19%|█▉        | 942/5000 [3:19:54<13:26:25, 11.92s/it, loss=0.7671, lr=9.76e-06]Steps:  19%|█▉        | 942/5000 [3:19:54<13:26:25, 11.92s/it, loss=0.3880, lr=9.76e-06]Steps:  19%|█▉        | 943/5000 [3:20:06<13:24:56, 11.90s/it, loss=0.3880, lr=9.76e-06]Steps:  19%|█▉        | 943/5000 [3:20:06<13:24:56, 11.90s/it, loss=1.2082, lr=9.76e-06]Steps:  19%|█▉        | 944/5000 [3:20:18<13:23:55, 11.89s/it, loss=1.2082, lr=9.76e-06]Steps:  19%|█▉        | 944/5000 [3:20:18<13:23:55, 11.89s/it, loss=0.9803, lr=9.76e-06]Steps:  19%|█▉        | 945/5000 [3:20:29<13:25:14, 11.91s/it, loss=0.9803, lr=9.76e-06]Steps:  19%|█▉        | 945/5000 [3:20:29<13:25:14, 11.91s/it, loss=1.0201, lr=9.76e-06]Steps:  19%|█▉        | 946/5000 [3:20:41<13:23:19, 11.89s/it, loss=1.0201, lr=9.76e-06]Steps:  19%|█▉        | 946/5000 [3:20:41<13:23:19, 11.89s/it, loss=0.5293, lr=9.76e-06]Steps:  19%|█▉        | 947/5000 [3:20:53<13:22:05, 11.87s/it, loss=0.5293, lr=9.76e-06]Steps:  19%|█▉        | 947/5000 [3:20:53<13:22:05, 11.87s/it, loss=0.6585, lr=9.76e-06]Steps:  19%|█▉        | 948/5000 [3:21:05<13:24:22, 11.91s/it, loss=0.6585, lr=9.76e-06]Steps:  19%|█▉        | 948/5000 [3:21:05<13:24:22, 11.91s/it, loss=0.5445, lr=9.76e-06]Steps:  19%|█▉        | 949/5000 [3:21:17<13:24:09, 11.91s/it, loss=0.5445, lr=9.76e-06]Steps:  19%|█▉        | 949/5000 [3:21:17<13:24:09, 11.91s/it, loss=0.4623, lr=9.76e-06]Steps:  19%|█▉        | 950/5000 [3:21:29<13:24:14, 11.91s/it, loss=0.4623, lr=9.76e-06]Steps:  19%|█▉        | 950/5000 [3:21:29<13:24:14, 11.91s/it, loss=0.3698, lr=9.76e-06]
[Step 950] Training Debug Info:
  Loss: 0.457927
  Latent shape: torch.Size([1, 32, 72, 126]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0173, std: 0.9375
  Noise mean: 0.0042, std: 0.9961
  Target mean: -0.0131, std: 1.3672
  Model pred mean: -0.0146, std: 1.1953
  Sigmas: [0.66015625]... (timesteps: [662.0])

[Step 950] Training Debug Info:
  Loss: 0.445743
  Latent shape: torch.Size([1, 32, 60, 150]), Packed shape: torch.Size([1, 2250, 128])
  Latent mean: -0.0166, std: 0.9023
  Noise mean: -0.0045, std: 1.0000
  Target mean: 0.0120, std: 1.3516
  Model pred mean: 0.0153, std: 1.1719
  Sigmas: [0.8984375]... (timesteps: [900.0])

[Step 950] Training Debug Info:
  Loss: 1.125984
  Latent shape: torch.Size([1, 32, 96, 96]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0067, std: 0.9180
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0066, std: 1.3516
  Model pred mean: -0.0037, std: 0.8398
  Sigmas: [0.1865234375]... (timesteps: [187.0])

[Step 950] Training Debug Info:
  Loss: 0.753966
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0654, std: 0.9375
  Noise mean: -0.0010, std: 1.0000
  Target mean: -0.0664, std: 1.3672
  Model pred mean: -0.0297, std: 1.0625
  Sigmas: [0.96875]... (timesteps: [970.0])
Steps:  19%|█▉        | 951/5000 [3:21:41<13:24:22, 11.92s/it, loss=0.3698, lr=9.76e-06]Steps:  19%|█▉        | 951/5000 [3:21:41<13:24:22, 11.92s/it, loss=0.7540, lr=9.75e-06]Steps:  19%|█▉        | 952/5000 [3:21:53<13:24:55, 11.93s/it, loss=0.7540, lr=9.75e-06]Steps:  19%|█▉        | 952/5000 [3:21:53<13:24:55, 11.93s/it, loss=1.2174, lr=9.75e-06]Steps:  19%|█▉        | 953/5000 [3:22:05<13:25:32, 11.94s/it, loss=1.2174, lr=9.75e-06]Steps:  19%|█▉        | 953/5000 [3:22:05<13:25:32, 11.94s/it, loss=0.4210, lr=9.75e-06]Steps:  19%|█▉        | 954/5000 [3:22:17<13:28:19, 11.99s/it, loss=0.4210, lr=9.75e-06]Steps:  19%|█▉        | 954/5000 [3:22:17<13:28:19, 11.99s/it, loss=0.8778, lr=9.75e-06]Steps:  19%|█▉        | 955/5000 [3:22:29<13:27:23, 11.98s/it, loss=0.8778, lr=9.75e-06]Steps:  19%|█▉        | 955/5000 [3:22:29<13:27:23, 11.98s/it, loss=0.5643, lr=9.75e-06]Steps:  19%|█▉        | 956/5000 [3:22:41<13:25:59, 11.96s/it, loss=0.5643, lr=9.75e-06]Steps:  19%|█▉        | 956/5000 [3:22:41<13:25:59, 11.96s/it, loss=1.1228, lr=9.75e-06]Steps:  19%|█▉        | 957/5000 [3:22:53<13:24:52, 11.94s/it, loss=1.1228, lr=9.75e-06]Steps:  19%|█▉        | 957/5000 [3:22:53<13:24:52, 11.94s/it, loss=1.0426, lr=9.75e-06]Steps:  19%|█▉        | 958/5000 [3:23:05<13:24:12, 11.94s/it, loss=1.0426, lr=9.75e-06]Steps:  19%|█▉        | 958/5000 [3:23:05<13:24:12, 11.94s/it, loss=0.9867, lr=9.75e-06]Steps:  19%|█▉        | 959/5000 [3:23:17<13:23:33, 11.93s/it, loss=0.9867, lr=9.75e-06]Steps:  19%|█▉        | 959/5000 [3:23:17<13:23:33, 11.93s/it, loss=1.1345, lr=9.75e-06]Steps:  19%|█▉        | 960/5000 [3:23:28<13:22:34, 11.92s/it, loss=1.1345, lr=9.75e-06]Steps:  19%|█▉        | 960/5000 [3:23:28<13:22:34, 11.92s/it, loss=1.0102, lr=9.74e-06]
[Step 960] Training Debug Info:
  Loss: 1.125497
  Latent shape: torch.Size([1, 32, 90, 102]), Packed shape: torch.Size([1, 2295, 128])
  Latent mean: 0.0215, std: 0.9219
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0200, std: 1.3594
  Model pred mean: -0.0181, std: 0.8477
  Sigmas: [0.1767578125]... (timesteps: [177.0])

[Step 960] Training Debug Info:
  Loss: 0.622424
  Latent shape: torch.Size([1, 32, 78, 114]), Packed shape: torch.Size([1, 2223, 128])
  Latent mean: -0.0063, std: 0.9180
  Noise mean: 0.0016, std: 0.9961
  Target mean: 0.0079, std: 1.3516
  Model pred mean: 0.0081, std: 1.0938
  Sigmas: [0.55859375]... (timesteps: [558.0])

[Step 960] Training Debug Info:
  Loss: 0.817567
  Latent shape: torch.Size([1, 32, 126, 72]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0361, std: 0.9414
  Noise mean: 0.0010, std: 0.9961
  Target mean: -0.0352, std: 1.3750
  Model pred mean: -0.0344, std: 1.0234
  Sigmas: [0.38671875]... (timesteps: [386.0])

[Step 960] Training Debug Info:
  Loss: 1.111411
  Latent shape: torch.Size([1, 32, 54, 156]), Packed shape: torch.Size([1, 2106, 128])
  Latent mean: 0.0063, std: 0.8711
  Noise mean: 0.0003, std: 1.0000
  Target mean: -0.0061, std: 1.3281
  Model pred mean: -0.0041, std: 0.8008
  Sigmas: [0.27734375]... (timesteps: [278.0])
Steps:  19%|█▉        | 961/5000 [3:23:40<13:22:33, 11.92s/it, loss=1.0102, lr=9.74e-06]Steps:  19%|█▉        | 961/5000 [3:23:40<13:22:33, 11.92s/it, loss=1.1114, lr=9.74e-06]Steps:  19%|█▉        | 962/5000 [3:23:52<13:22:04, 11.92s/it, loss=1.1114, lr=9.74e-06]Steps:  19%|█▉        | 962/5000 [3:23:52<13:22:04, 11.92s/it, loss=0.4395, lr=9.74e-06]Steps:  19%|█▉        | 963/5000 [3:24:04<13:23:06, 11.94s/it, loss=0.4395, lr=9.74e-06]Steps:  19%|█▉        | 963/5000 [3:24:04<13:23:06, 11.94s/it, loss=1.2342, lr=9.74e-06]Steps:  19%|█▉        | 964/5000 [3:24:16<13:21:29, 11.92s/it, loss=1.2342, lr=9.74e-06]Steps:  19%|█▉        | 964/5000 [3:24:16<13:21:29, 11.92s/it, loss=0.3774, lr=9.74e-06]Steps:  19%|█▉        | 965/5000 [3:24:28<13:21:32, 11.92s/it, loss=0.3774, lr=9.74e-06]Steps:  19%|█▉        | 965/5000 [3:24:28<13:21:32, 11.92s/it, loss=1.1082, lr=9.74e-06]Steps:  19%|█▉        | 966/5000 [3:24:40<13:19:39, 11.89s/it, loss=1.1082, lr=9.74e-06]Steps:  19%|█▉        | 966/5000 [3:24:40<13:19:39, 11.89s/it, loss=0.7794, lr=9.74e-06]Steps:  19%|█▉        | 967/5000 [3:24:52<13:19:52, 11.90s/it, loss=0.7794, lr=9.74e-06]Steps:  19%|█▉        | 967/5000 [3:24:52<13:19:52, 11.90s/it, loss=0.5469, lr=9.74e-06]Steps:  19%|█▉        | 968/5000 [3:25:04<13:21:53, 11.93s/it, loss=0.5469, lr=9.74e-06]Steps:  19%|█▉        | 968/5000 [3:25:04<13:21:53, 11.93s/it, loss=1.1313, lr=9.74e-06]Steps:  19%|█▉        | 969/5000 [3:25:16<13:19:28, 11.90s/it, loss=1.1313, lr=9.74e-06]Steps:  19%|█▉        | 969/5000 [3:25:16<13:19:28, 11.90s/it, loss=1.0845, lr=9.73e-06]Steps:  19%|█▉        | 970/5000 [3:25:28<13:20:14, 11.91s/it, loss=1.0845, lr=9.73e-06]Steps:  19%|█▉        | 970/5000 [3:25:28<13:20:14, 11.91s/it, loss=0.7083, lr=9.73e-06]
[Step 970] Training Debug Info:
  Loss: 0.765843
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0233, std: 0.9727
  Noise mean: -0.0018, std: 1.0000
  Target mean: -0.0251, std: 1.3984
  Model pred mean: -0.0270, std: 1.0859
  Sigmas: [0.435546875]... (timesteps: [435.0])

[Step 970] Training Debug Info:
  Loss: 1.060088
  Latent shape: torch.Size([1, 32, 66, 132]), Packed shape: torch.Size([1, 2178, 128])
  Latent mean: 0.0654, std: 0.9531
  Noise mean: 0.0001, std: 1.0000
  Target mean: -0.0649, std: 1.3828
  Model pred mean: -0.0654, std: 0.9180
  Sigmas: [0.0439453125]... (timesteps: [44.0])

[Step 970] Training Debug Info:
  Loss: 1.116062
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: -0.0214, std: 0.8945
  Noise mean: 0.0005, std: 1.0000
  Target mean: 0.0219, std: 1.3438
  Model pred mean: 0.0195, std: 0.8242
  Sigmas: [0.06689453125]... (timesteps: [67.0])

[Step 970] Training Debug Info:
  Loss: 0.510976
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0153, std: 0.9102
  Noise mean: 0.0015, std: 1.0000
  Target mean: -0.0138, std: 1.3516
  Model pred mean: -0.0172, std: 1.1406
  Sigmas: [0.6171875]... (timesteps: [616.0])
Steps:  19%|█▉        | 971/5000 [3:25:39<13:19:41, 11.91s/it, loss=0.7083, lr=9.73e-06]Steps:  19%|█▉        | 971/5000 [3:25:39<13:19:41, 11.91s/it, loss=0.5110, lr=9.73e-06]Steps:  19%|█▉        | 972/5000 [3:25:51<13:21:25, 11.94s/it, loss=0.5110, lr=9.73e-06]Steps:  19%|█▉        | 972/5000 [3:25:51<13:21:25, 11.94s/it, loss=0.4778, lr=9.73e-06]Steps:  19%|█▉        | 973/5000 [3:26:03<13:20:43, 11.93s/it, loss=0.4778, lr=9.73e-06]Steps:  19%|█▉        | 973/5000 [3:26:03<13:20:43, 11.93s/it, loss=0.9111, lr=9.73e-06]Steps:  19%|█▉        | 974/5000 [3:26:15<13:18:54, 11.91s/it, loss=0.9111, lr=9.73e-06]Steps:  19%|█▉        | 974/5000 [3:26:15<13:18:54, 11.91s/it, loss=0.4835, lr=9.73e-06]Steps:  20%|█▉        | 975/5000 [3:26:27<13:19:43, 11.92s/it, loss=0.4835, lr=9.73e-06]Steps:  20%|█▉        | 975/5000 [3:26:27<13:19:43, 11.92s/it, loss=0.3859, lr=9.73e-06]Steps:  20%|█▉        | 976/5000 [3:26:39<13:19:11, 11.92s/it, loss=0.3859, lr=9.73e-06]Steps:  20%|█▉        | 976/5000 [3:26:39<13:19:11, 11.92s/it, loss=0.3840, lr=9.73e-06]Steps:  20%|█▉        | 977/5000 [3:26:51<13:18:52, 11.91s/it, loss=0.3840, lr=9.73e-06]Steps:  20%|█▉        | 977/5000 [3:26:51<13:18:52, 11.91s/it, loss=1.1585, lr=9.73e-06]Steps:  20%|█▉        | 978/5000 [3:27:03<13:20:31, 11.94s/it, loss=1.1585, lr=9.73e-06]Steps:  20%|█▉        | 978/5000 [3:27:03<13:20:31, 11.94s/it, loss=0.5822, lr=9.72e-06]Steps:  20%|█▉        | 979/5000 [3:27:15<13:19:56, 11.94s/it, loss=0.5822, lr=9.72e-06]Steps:  20%|█▉        | 979/5000 [3:27:15<13:19:56, 11.94s/it, loss=0.6408, lr=9.72e-06]Steps:  20%|█▉        | 980/5000 [3:27:27<13:20:43, 11.95s/it, loss=0.6408, lr=9.72e-06]Steps:  20%|█▉        | 980/5000 [3:27:27<13:20:43, 11.95s/it, loss=0.8420, lr=9.72e-06]
[Step 980] Training Debug Info:
  Loss: 1.103895
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0025, std: 0.9219
  Noise mean: -0.0007, std: 1.0000
  Target mean: -0.0032, std: 1.3594
  Model pred mean: -0.0004, std: 0.8633
  Sigmas: [0.06494140625]... (timesteps: [65.0])

[Step 980] Training Debug Info:
  Loss: 1.065482
  Latent shape: torch.Size([1, 32, 72, 120]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0242, std: 0.9453
  Noise mean: -0.0017, std: 1.0000
  Target mean: -0.0260, std: 1.3750
  Model pred mean: -0.0264, std: 0.9141
  Sigmas: [0.271484375]... (timesteps: [272.0])

[Step 980] Training Debug Info:
  Loss: 1.026313
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0420, std: 0.9141
  Noise mean: 0.0028, std: 1.0000
  Target mean: -0.0391, std: 1.3594
  Model pred mean: -0.0366, std: 0.9062
  Sigmas: [0.010986328125]... (timesteps: [11.0])

[Step 980] Training Debug Info:
  Loss: 0.463813
  Latent shape: torch.Size([1, 32, 48, 186]), Packed shape: torch.Size([1, 2232, 128])
  Latent mean: 0.0009, std: 0.9219
  Noise mean: -0.0046, std: 1.0000
  Target mean: -0.0055, std: 1.3594
  Model pred mean: 0.0005, std: 1.1875
  Sigmas: [0.7109375]... (timesteps: [712.0])
Steps:  20%|█▉        | 981/5000 [3:27:39<13:22:18, 11.98s/it, loss=0.8420, lr=9.72e-06]Steps:  20%|█▉        | 981/5000 [3:27:39<13:22:18, 11.98s/it, loss=0.4638, lr=9.72e-06]Steps:  20%|█▉        | 982/5000 [3:27:51<13:24:59, 12.02s/it, loss=0.4638, lr=9.72e-06]Steps:  20%|█▉        | 982/5000 [3:27:51<13:24:59, 12.02s/it, loss=0.4548, lr=9.72e-06]Steps:  20%|█▉        | 983/5000 [3:28:03<13:23:42, 12.00s/it, loss=0.4548, lr=9.72e-06]Steps:  20%|█▉        | 983/5000 [3:28:03<13:23:42, 12.00s/it, loss=1.1876, lr=9.72e-06]Steps:  20%|█▉        | 984/5000 [3:28:15<13:21:31, 11.97s/it, loss=1.1876, lr=9.72e-06]Steps:  20%|█▉        | 984/5000 [3:28:15<13:21:31, 11.97s/it, loss=0.4987, lr=9.72e-06]Steps:  20%|█▉        | 985/5000 [3:28:27<13:18:50, 11.94s/it, loss=0.4987, lr=9.72e-06]Steps:  20%|█▉        | 985/5000 [3:28:27<13:18:50, 11.94s/it, loss=1.0576, lr=9.72e-06]Steps:  20%|█▉        | 986/5000 [3:28:39<13:18:00, 11.93s/it, loss=1.0576, lr=9.72e-06]Steps:  20%|█▉        | 986/5000 [3:28:39<13:18:00, 11.93s/it, loss=0.9147, lr=9.71e-06]Steps:  20%|█▉        | 987/5000 [3:28:51<13:17:08, 11.92s/it, loss=0.9147, lr=9.71e-06]Steps:  20%|█▉        | 987/5000 [3:28:51<13:17:08, 11.92s/it, loss=0.4690, lr=9.71e-06]Steps:  20%|█▉        | 988/5000 [3:29:03<13:19:10, 11.95s/it, loss=0.4690, lr=9.71e-06]Steps:  20%|█▉        | 988/5000 [3:29:03<13:19:10, 11.95s/it, loss=0.4641, lr=9.71e-06]Steps:  20%|█▉        | 989/5000 [3:29:15<13:22:47, 12.01s/it, loss=0.4641, lr=9.71e-06]Steps:  20%|█▉        | 989/5000 [3:29:15<13:22:47, 12.01s/it, loss=0.6135, lr=9.71e-06]Steps:  20%|█▉        | 990/5000 [3:29:27<13:22:01, 12.00s/it, loss=0.6135, lr=9.71e-06]Steps:  20%|█▉        | 990/5000 [3:29:27<13:22:01, 12.00s/it, loss=0.4441, lr=9.71e-06]
[Step 990] Training Debug Info:
  Loss: 0.658435
  Latent shape: torch.Size([1, 32, 48, 192]), Packed shape: torch.Size([1, 2304, 128])
  Latent mean: 0.0079, std: 0.8984
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0061, std: 1.3438
  Model pred mean: -0.0065, std: 1.0781
  Sigmas: [0.5234375]... (timesteps: [524.0])

[Step 990] Training Debug Info:
  Loss: 0.530928
  Latent shape: torch.Size([1, 32, 60, 144]), Packed shape: torch.Size([1, 2160, 128])
  Latent mean: 0.0236, std: 0.9219
  Noise mean: -0.0018, std: 1.0000
  Target mean: -0.0254, std: 1.3594
  Model pred mean: -0.0229, std: 1.1406
  Sigmas: [0.59375]... (timesteps: [594.0])

[Step 990] Training Debug Info:
  Loss: 0.491612
  Latent shape: torch.Size([1, 32, 54, 168]), Packed shape: torch.Size([1, 2268, 128])
  Latent mean: 0.0210, std: 0.8984
  Noise mean: 0.0026, std: 1.0000
  Target mean: -0.0184, std: 1.3438
  Model pred mean: -0.0204, std: 1.1406
  Sigmas: [0.703125]... (timesteps: [705.0])

[Step 990] Training Debug Info:
  Loss: 1.065965
  Latent shape: torch.Size([1, 32, 66, 138]), Packed shape: torch.Size([1, 2277, 128])
  Latent mean: 0.0215, std: 0.9102
  Noise mean: 0.0018, std: 1.0000
  Target mean: -0.0197, std: 1.3516
  Model pred mean: -0.0141, std: 0.8828
  Sigmas: [0.0380859375]... (timesteps: [38.0])
Steps:  20%|█▉        | 991/5000 [3:29:39<13:19:09, 11.96s/it, loss=0.4441, lr=9.71e-06]Steps:  20%|█▉        | 991/5000 [3:29:39<13:19:09, 11.96s/it, loss=1.0660, lr=9.71e-06]Steps:  20%|█▉        | 992/5000 [3:29:50<13:17:04, 11.93s/it, loss=1.0660, lr=9.71e-06]Steps:  20%|█▉        | 992/5000 [3:29:50<13:17:04, 11.93s/it, loss=0.6835, lr=9.71e-06]Steps:  20%|█▉        | 993/5000 [3:30:02<13:16:50, 11.93s/it, loss=0.6835, lr=9.71e-06]Steps:  20%|█▉        | 993/5000 [3:30:02<13:16:50, 11.93s/it, loss=0.5511, lr=9.71e-06]Steps:  20%|█▉        | 994/5000 [3:30:14<13:15:51, 11.92s/it, loss=0.5511, lr=9.71e-06]Steps:  20%|█▉        | 994/5000 [3:30:14<13:15:51, 11.92s/it, loss=1.1132, lr=9.71e-06]Steps:  20%|█▉        | 995/5000 [3:30:26<13:13:48, 11.89s/it, loss=1.1132, lr=9.71e-06]Steps:  20%|█▉        | 995/5000 [3:30:26<13:13:48, 11.89s/it, loss=0.9610, lr=9.70e-06]Steps:  20%|█▉        | 996/5000 [3:30:38<13:14:00, 11.90s/it, loss=0.9610, lr=9.70e-06]Steps:  20%|█▉        | 996/5000 [3:30:38<13:14:00, 11.90s/it, loss=0.9268, lr=9.70e-06]Steps:  20%|█▉        | 997/5000 [3:30:50<13:13:16, 11.89s/it, loss=0.9268, lr=9.70e-06]Steps:  20%|█▉        | 997/5000 [3:30:50<13:13:16, 11.89s/it, loss=0.3764, lr=9.70e-06]Steps:  20%|█▉        | 998/5000 [3:31:02<13:14:08, 11.91s/it, loss=0.3764, lr=9.70e-06]Steps:  20%|█▉        | 998/5000 [3:31:02<13:14:08, 11.91s/it, loss=0.6429, lr=9.70e-06]Steps:  20%|█▉        | 999/5000 [3:31:14<13:16:45, 11.95s/it, loss=0.6429, lr=9.70e-06]Steps:  20%|█▉        | 999/5000 [3:31:14<13:16:45, 11.95s/it, loss=0.5747, lr=9.70e-06]Steps:  20%|██        | 1000/5000 [3:31:26<13:14:29, 11.92s/it, loss=0.5747, lr=9.70e-06]Steps:  20%|██        | 1000/5000 [3:31:26<13:14:29, 11.92s/it, loss=0.9149, lr=9.70e-06]01/22/2026 06:56:04 - INFO - __main__ - 
[Step 1000] ✅ Loss in normal range (0.9149)
01/22/2026 06:56:04 - INFO - __main__ -   Loss avg (last 100): 0.7435
01/22/2026 06:56:04 - INFO - __main__ -   Loss range: [0.3620, 1.2342]
Configuration saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-1000/transformer/config.json
Model weights saved in /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-1000/transformer/diffusion_pytorch_model.safetensors
01/22/2026 06:56:47 - INFO - __main__ - Saved checkpoint to /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-1000
01/22/2026 06:56:47 - INFO - accelerate.accelerator - Saving current state to /home/v-yuxluo/WORK_local/ArXivQwenImage/output/flux2klein_fulltune_5000/checkpoint-1000/accelerator
01/22/2026 06:56:47 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
Traceback (most recent call last):
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/1: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 829, in <module>
    main()
  File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 711, in main
    save_model_checkpoint(
  File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 228, in save_model_checkpoint
    accelerator.save_state(os.path.join(save_path, "accelerator"))
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/accelerate/accelerator.py", line 3660, in save_state
    model.save_checkpoint(output_dir, ckpt_id, **save_model_func_kwargs)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 3673, in save_checkpoint
    self._save_zero_checkpoint(save_dir, tag)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 4064, in _save_zero_checkpoint
    self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 32, in save
    torch.save(state_dict, path, _use_new_zipfile_serialization=self.zipfile_serialization)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 690, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 8768 vs 8660
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
[rank0]:     _save(
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
[rank0]:     zip_file.write_record(name, storage, num_bytes)
[rank0]: RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/1: file write failed

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 829, in <module>
[rank0]:     main()
[rank0]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 711, in main
[rank0]:     save_model_checkpoint(
[rank0]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 228, in save_model_checkpoint
[rank0]:     accelerator.save_state(os.path.join(save_path, "accelerator"))
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/accelerate/accelerator.py", line 3660, in save_state
[rank0]:     model.save_checkpoint(output_dir, ckpt_id, **save_model_func_kwargs)
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 3673, in save_checkpoint
[rank0]:     self._save_zero_checkpoint(save_dir, tag)
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 4064, in _save_zero_checkpoint
[rank0]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 32, in save
[rank0]:     torch.save(state_dict, path, _use_new_zipfile_serialization=self.zipfile_serialization)
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 690, in __exit__
[rank0]:     self.file_like.write_end_of_file()
[rank0]: RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 8768 vs 8660
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
[rank3]:     _save(
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
[rank3]:     zip_file.write_record(name, storage, num_bytes)
[rank3]: RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/3: file write failed

[rank3]: During handling of the above exception, another exception occurred:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 829, in <module>
[rank3]:     main()
[rank3]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 711, in main
[rank3]:     save_model_checkpoint(
[rank3]:   File "/home/v-yuxluo/WORK_local/ArXivQwenImage/train_OpenSciDraw_fulltune.py", line 228, in save_model_checkpoint
[rank3]:     accelerator.save_state(os.path.join(save_path, "accelerator"))
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/accelerate/accelerator.py", line 3660, in save_state
[rank3]:     model.save_checkpoint(output_dir, ckpt_id, **save_model_func_kwargs)
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 3673, in save_checkpoint
[rank3]:     self._save_zero_checkpoint(save_dir, tag)
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 4064, in _save_zero_checkpoint
[rank3]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 32, in save
[rank3]:     torch.save(state_dict, path, _use_new_zipfile_serialization=self.zipfile_serialization)
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
[rank3]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank3]:   File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/torch/serialization.py", line 690, in __exit__
[rank3]:     self.file_like.write_end_of_file()
[rank3]: RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 18157168512 vs 18157168344
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x145c4bce7be0>
Traceback (most recent call last):
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 186, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 103, in put
    with open(self.file_path + ".tmp", 'wb') as handle:
OSError: [Errno 28] No space left on device
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mflux2klein_9b_fulltune_5000steps[0m at: [34mhttps://microsoft-research.wandb.io/v-yuxluo/Flux2Klein-FullTune/runs/k5e53u6r[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260122_032439-k5e53u6r/logs[0m
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x14a603c2fbe0>
Traceback (most recent call last):
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 480, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/home/v-yuxluo/miniconda3/envs/flux2/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 457, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_k