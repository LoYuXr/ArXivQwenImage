import os
import pandas as pd
import numpy as np
import torch
import itertools
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import pyarrow.parquet as pq
from OpenSciDraw.registry import DATASETS
from torch.utils.data import Dataset, Sampler
import torch.distributed as dist
import math
import time
import random






@DATASETS.register_module()
class ArXiVParquetDatasetV2(Dataset):
    def __init__(self, base_dir, parquet_base_path, num_workers=16, 
                 num_train_examples=1000000, debug_mode=False, is_main_process=False, stat_data=False):
        self.base_path = Path(base_dir)
        self.data_base_path = self.base_path / parquet_base_path
        
        ## é¦–å…ˆè¯»å…¥æ‰€æœ‰çš„.parquetï¼Œç°åœ¨ä¸å¤§ï¼Œä¸éœ€è¦è¯»snapshot
        print(f"ğŸ” Building metadata from all parquet files in {self.data_base_path}...")
        year_dirs = sorted([d for d in self.data_base_path.iterdir() if d.is_dir()])
        all_paths = []
        for y_dir in year_dirs:
            all_paths.extend(sorted(y_dir.glob("*.parquet")))
        if debug_mode: all_paths = all_paths[:200]
        
        print(f"â³ Loading/parsing metadata (parquet: path only) from {len(all_paths)} parquet files...")
        df = self._parallel_load_parquet(all_paths, max_workers=num_workers, num_train_examples=num_train_examples)
        self.meta_df = df
        
        print(f"âœ… Loaded {len(self.meta_df)} samples.")
        
        self._filter_small_buckets(batch_size=8, num_replicas=4)
        
        if stat_data and is_main_process:
            print(f"ğŸ“Š Data Statistics:")
            bucket_counts = self.meta_df.groupby(['bucket_h', 'bucket_w']).size().reset_index(name='counts')
            print(bucket_counts)
            total_samples = len(self.meta_df)
            for _, row in bucket_counts.iterrows():
                h, w, count = row['bucket_h'], row['bucket_w'], row['counts']
                print(f" - Resolution {w}x{h}: {count} samples ({count/total_samples*100:.2f}%)")
                
    def __len__(self):
        return len(self.meta_df)
        
        
    def _parallel_load_parquet(
                        self, 
                        paths, 
                        max_workers, 
                        num_train_examples, 
                        default_key=["caption", "cache_path", "latent_shape", "text_embeds_shape", "bucket_w", "bucket_h", "aspect_ratio"]
                        ):
        meta_list = []
        def load_one_file(path):
            try:
                pf = pq.ParquetFile(path)
                # è¯»å– schema è·å–çœŸæ­£çš„ top-level åˆ—å (pf.schema.names ä¼šè¿”å›åµŒå¥—å­—æ®µåå¦‚ 'element')
                available_columns = [field.name for field in pf.schema_arrow]
                
                # å¦‚æœ text_embeds_shape ä¸å­˜åœ¨ä½† prompt_embeds_shape å­˜åœ¨ï¼Œåšæ›¿æ¢
                columns_to_read = default_key.copy()
                rename_map = {}
                
                if 'text_embeds_shape' not in available_columns and 'prompt_embeds_shape' in available_columns:
                    columns_to_read = [c if c != 'text_embeds_shape' else 'prompt_embeds_shape' for c in columns_to_read]
                    rename_map['prompt_embeds_shape'] = 'text_embeds_shape'
                
                df = pf.read(columns=columns_to_read).to_pandas()
                
                # é‡å‘½ååˆ—ï¼Œç»Ÿä¸€ä¸º text_embeds_shape
                if rename_map:
                    df = df.rename(columns=rename_map)
                
                df['source_file'] = str(path)
                df['local_index'] = range(len(df))
                return df
            except Exception as e:
                return f"Error: {path} | {str(e)}"
            
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_path = {executor.submit(load_one_file, p): p for p in paths}
            for future in tqdm(as_completed(future_to_path), total=len(paths), desc="Scanning Parquet Files"):
                result = future.result()
                if isinstance(result, pd.DataFrame):
                    meta_list.append(result)
                    
        return pd.concat(meta_list, ignore_index=True).iloc[:num_train_examples]
    
    def _filter_small_buckets(self, batch_size, num_replicas):
        # ç»Ÿè®¡æ¯ä¸ªæ¡¶çš„æ ·æœ¬æ•°
        counts = self.meta_df.groupby(['bucket_h', 'bucket_w']).indices
        valid_indices = []
        
        for bucket_key, indices in counts.items():
            # è®¡ç®—åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè¿™ä¸ªæ¡¶èƒ½å‡‘å‡ºå¤šå°‘ä¸ªå®Œæ•´çš„ Batch
            # æ¯ä¸ªæ¡¶è‡³å°‘éœ€è¦ï¼šbatch_size * num_replicas * 2 ä¸ªæ ·æœ¬æ‰èƒ½ä¿è¯æ¯å¼ å¡éƒ½èƒ½åˆ†åˆ°
            total_needed = batch_size * num_replicas * 2
            if len(indices) >= total_needed:
                # åªæœ‰æ ·æœ¬æ•°è¶³å¤Ÿçš„æ¡¶æ‰ä¿ç•™
                valid_indices.extend(indices)
        
        # æ›´æ–° meta_dfï¼Œåªä¿ç•™æœ‰æ•ˆæ ·æœ¬
        self.meta_df = self.meta_df.iloc[valid_indices].reset_index(drop=True)
        print(f"Filtered dataset: {len(self.meta_df)} samples remaining.")
        
        #

    def _read_sample_npz(self, npz_path):
        #tbd
        cache_latent_path = self.data_base_path / npz_path
        try:
            with  np.load(cache_latent_path, allow_pickle=True) as npz_data:
                
                latents_np = npz_data['latents'].astype(np.float32)
                
                # Support both 'text_embeds' (QwenImage) and 'prompt_embeds' (Flux2Klein)
                if 'text_embeds' in npz_data:
                    text_embeds_np = npz_data['text_embeds'].astype(np.float16)
                elif 'prompt_embeds' in npz_data:
                    text_embeds_np = npz_data['prompt_embeds'].astype(np.float16)
                else:
                    raise KeyError("Neither 'text_embeds' nor 'prompt_embeds' found in npz file")
                
                # Support both text_mask (QwenImage) and text_ids (Flux2Klein)
                # Priority: text_mask > text_ids > create default
                if 'text_mask' in npz_data:
                    text_mask = npz_data['text_mask'].astype(np.int8)
                    text_ids = None
                elif 'text_ids' in npz_data:
                    # Flux2Klein uses text_ids for RoPE position encoding
                    text_ids = npz_data['text_ids'].astype(np.float16)
                    # Create text_mask as all ones (all tokens valid)
                    seq_len = text_embeds_np.shape[0]
                    text_mask = np.ones((seq_len,), dtype=np.int8)
                else:
                    # Fallback: create default mask
                    seq_len = text_embeds_np.shape[0]
                    text_mask = np.ones((seq_len,), dtype=np.int8)
                    text_ids = None
                
                return latents_np, text_embeds_np, text_mask, text_ids
        except Exception as e:
            print(f"âŒ Error reading npz {npz_path}: {e}")
            return None, None, None, None
        
    def get_data_info(self, index):
        index = index % len(self.meta_df)
        sample = self.meta_df.iloc[index]
        return sample
        

    def __getitem__(self, index):
        meta_row = self.get_data_info(index)
        latents, text_embeds, text_mask, text_ids = self._read_sample_npz(
            meta_row['cache_path']
        )
        if latents is None or text_embeds is None or text_mask is None:
            print(f"âŒ Failed to load sample at index {index}. Use a zero latent and embed as fallback.")
            latents = np.zeros(tuple(map(int, meta_row['latent_shape'])), dtype=np.float32)
            # è¡¥ä¸ï¼š
            text_embeds = np.zeros(tuple(map(int, meta_row['text_embeds_shape'])), dtype=np.float16)
            text_mask = np.zeros((text_embeds.shape[0],), dtype=np.int8)
            text_ids = None
            
        
        latents = torch.from_numpy(latents).reshape(list(map(int, meta_row['latent_shape'])))
        expected = int(np.prod(meta_row['text_embeds_shape']))
        actual = text_embeds.size

        if actual != expected:
            # print(
            #     f"[BAD SAMPLE]\n"
            #     f"cache_path={meta_row['cache_path']}\n"
            #     f"text_embeds.size={actual}\n"
            #     f"text_embeds_shape(meta)={meta_row['text_embeds_shape']}"
            # )
            text_embeds = torch.from_numpy(text_embeds)
            L_embed = text_embeds.shape[0]
            L_mask = text_mask.shape[0]
            L = min(L_embed, L_mask)

            if L_embed != L_mask:
                print(
                    f"[TRIM TEXT]\n"
                    f"cache_path={meta_row['cache_path']}\n"
                    f"L_embed={L_embed}, L_mask={L_mask} -> use L={L}"
                )
                
                text_embeds = text_embeds[:L]
                text_mask = text_mask[:L]   ###LYX HINT å…ˆæ‰“ä¸Šè¡¥ä¸ï¼ï¼ï¼ ç°åœ¨è¿˜é”™ç€å‘¢ï¼Œæ•°æ®çš„åˆ¶ä½œï¼
                if text_ids is not None:
                    text_ids = text_ids[:L]
        
        else:
    
            text_embeds = torch.from_numpy(text_embeds).reshape(list(map(int, meta_row['text_embeds_shape'])))
        text_mask = torch.from_numpy(text_mask)
        
        result = {
            "latents": latents,
            "text_embeds": text_embeds,
            "text_mask": text_mask,
            "bucket_size": (meta_row['bucket_h'], meta_row['bucket_w']),
            "aspect_ratio": meta_row['aspect_ratio'],
            "caption": meta_row['caption']
        }
        
        # Add text_ids for Flux2Klein (RoPE position encoding)
        if text_ids is not None:
            result["text_ids"] = torch.from_numpy(text_ids)
        
        return result

    def collate_fn(self, batch):
        from torch.nn.utils.rnn import pad_sequence
        latents = torch.stack([x['latents'] for x in batch])
        embeds_list = [x['text_embeds'] for x in batch]
        masks_list = [x['text_mask'] for x in batch]
        padded_embeds = pad_sequence(embeds_list, batch_first=True, padding_value=0)
        padded_masks = pad_sequence(masks_list, batch_first=True, padding_value=0)

        result = {
            "latents": latents,
            "text_embeds": padded_embeds,
            "text_mask": padded_masks,
            "captions": [x['caption'] for x in batch],
            "bucket_size": batch[0]['bucket_size'],
            "aspect_ratio": batch[0]['aspect_ratio'],
        }
        
        # Add text_ids for Flux2Klein if present
        if 'text_ids' in batch[0] and batch[0]['text_ids'] is not None:
            text_ids_list = [x['text_ids'] for x in batch]
            padded_text_ids = pad_sequence(text_ids_list, batch_first=True, padding_value=0)
            result["text_ids"] = padded_text_ids
        
        return result

    
@DATASETS.register_module()
class DistributedBucketSamplerV2(Sampler):
    def __init__(self, dataset, batch_size, num_replicas=None, rank=None, drop_last=True, shuffle=True, seed=42):
        super().__init__(dataset)
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_replicas = num_replicas or (dist.get_world_size() if dist.is_initialized() else 1)
        self.rank = rank if rank is not None else (dist.get_rank() if dist.is_initialized() else 0)
        self.drop_last = drop_last
        self.shuffle = shuffle
        self.seed = seed  # Base seed for reproducibility
        self.epoch = 0

        self.groups = self.dataset.meta_df.groupby(['bucket_h', 'bucket_w']).indices

    def __iter__(self):
        # --- 3. åˆ†å¸ƒå¼åŒæ­¥æ ¸å¿ƒï¼šæ‰€æœ‰è¿›ç¨‹å¿…é¡»å…±äº«åŒä¸€ä¸ª RNG ---
        # ä½¿ç”¨ seed + epoch æ¥ç¡®ä¿å¯å¤ç°æ€§
        combined_seed = self.seed + self.epoch
        g = torch.Generator()
        g.manual_seed(combined_seed)
        rng = random.Random(combined_seed)  # ä½¿ç”¨ Python çš„ random ä¿è¯ shuffling ä¸€è‡´

        all_batch_lists = []
        
        # æ’åº keys ä¿è¯æ‰€æœ‰å¡éå†æ¡¶çš„é¡ºåºç»å¯¹ä¸€è‡´
        sorted_bucket_keys = sorted(self.groups.keys())

        for bucket_key in sorted_bucket_keys:
            indices = self.groups[bucket_key].tolist()
            
            if self.shuffle:
                rng.shuffle(indices) # å…¨å±€ç»Ÿä¸€æ‰“ä¹±æ¡¶å†…æ ·æœ¬
            
            # è¡¥é½é€»è¾‘ï¼šè®©æ¯ä¸ªæ¡¶éƒ½èƒ½è¢« world_size * batch_size æ•´é™¤ï¼ˆé’ˆå¯¹ drop_last=Trueï¼‰
            # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ä¿è¯å„å¡çœ‹åˆ°çš„ Batch æ•°é‡å®Œå…¨ç›¸ç­‰
            if self.drop_last:
                total_per_bucket = (len(indices) // (self.num_replicas * self.batch_size)) * (self.num_replicas * self.batch_size)
                indices = indices[:total_per_bucket]
            else:
                total_per_bucket = int(math.ceil(len(indices) / (self.num_replicas * self.batch_size))) * (self.num_replicas * self.batch_size)
                # å¾ªç¯è¡¥é½
                indices += indices[:(total_per_bucket - len(indices))]

            # åˆ†å‘åˆ°å½“å‰ Rank (ä¾‹å¦‚ 4å¡ï¼ŒRank 0 æ‹¿ 0, 4, 8...)
            # ä½†æ³¨æ„ï¼šæˆ‘ä»¬è¦å…ˆç»„æˆæ‰€æœ‰çš„ batchï¼Œå†åˆ†é…ï¼Œé˜²æ­¢è·¨æ¡¶
            bucket_batches = []
            for i in range(0, len(indices), self.batch_size * self.num_replicas):
                # è¿™ä¸€å—åŒ…å«äº†æ‰€æœ‰å¡åœ¨å½“å‰ä½ç½®çš„ batch
                chunk = indices[i : i + self.batch_size * self.num_replicas]
                # å½“å‰å¡å–è‡ªå·±é‚£ä¸€ä»½
                my_batch = chunk[self.rank * self.batch_size : (self.rank + 1) * self.batch_size]
                if len(my_batch) == self.batch_size:
                    bucket_batches.append(my_batch)
            
            all_batch_lists.extend(bucket_batches)

        # --- 4. æ¡¶é—´æ‰“ä¹±åŒæ­¥ ---
        # å¿…é¡»æ‰€æœ‰å¡å¯¹ batch åºåˆ—è¿›è¡Œå®Œå…¨ç›¸åŒçš„æ‰“ä¹±ï¼Œå¦åˆ™ä¼šå› ä¸ºåˆ†è¾¨ç‡é¡ºåºä¸åŒå¯¼è‡´æ­»é”
        if self.shuffle:
            rng.shuffle(all_batch_lists)
            
        return iter(all_batch_lists)

    def __len__(self):
        # æ­¤å¤„çš„è®¡ç®—é€»è¾‘å¿…é¡»ä¸ __iter__ ä¸¥ä¸åˆç¼
        total_batches = 0
        for bucket_key in self.groups:
            indices = self.groups[bucket_key]
            num_samples_per_replica = len(indices) // self.num_replicas
            if self.drop_last:
                total_batches += num_samples_per_replica // self.batch_size
            else:
                total_batches += int(math.ceil(num_samples_per_replica / self.batch_size))
        return total_batches

    def set_epoch(self, epoch):
        self.epoch = epoch