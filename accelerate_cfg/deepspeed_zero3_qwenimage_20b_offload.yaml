# DeepSpeed ZeRO-3 Configuration for QwenImage 20B with FULL CPU Offloading
# Use this if the basic ZeRO-3 config runs out of memory
# 
# This configuration offloads BOTH optimizer states AND parameters to CPU
# Trading training speed for memory efficiency
#
# Performance Impact:
# - ~2-3x slower than GPU-only training
# - But enables training on limited GPU memory
#
# Recommended for:
# - 4x 40GB A100 or smaller GPUs
# - When you need to fit larger batch sizes
#
# Usage:
#   accelerate launch --config_file accelerate_cfg/deepspeed_zero3_qwenimage_20b_offload.yaml \
#       train_OpenSciDraw_fulltune.py configs/260126/qwenimage_fulltune_local.py

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: DEEPSPEED
downcast_bf16: 'no'
enable_cpu_affinity: false
gpu_ids: 0,1,2,3
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

deepspeed_config:
  # ZeRO Stage 3: Full sharding
  zero_stage: 3
  
  # ZeRO-3 options
  zero3_init_flag: true
  zero3_save_16bit_model: true
  
  # Gradient settings
  gradient_accumulation_steps: 4
  gradient_clipping: 1.0
  
  # Batch size
  train_micro_batch_size_per_gpu: 1
  
  # FULL CPU Offloading: Both optimizer and parameters
  offload_optimizer_device: cpu
  offload_optimizer_pin_memory: true
  offload_param_device: cpu
  offload_param_pin_memory: true
