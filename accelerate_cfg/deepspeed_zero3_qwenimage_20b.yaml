# DeepSpeed ZeRO-3 Configuration for QwenImage 20B Full Fine-tuning
# Optimized for 4x 80GB A100 GPUs with CPU offloading
# 
# Memory Estimation for 20B model:
# - Model parameters: 20B * 2 bytes (BF16) = 40GB
# - Gradients: 20B * 2 bytes = 40GB
# - Optimizer states (AdamW): 20B * 4 bytes * 2 = 160GB
# Total without sharding: ~240GB
# With ZeRO-3 (4 GPUs): ~60GB per GPU (still too large)
# With CPU offload: Fits in GPU memory with room for activations
#
# Usage:
#   accelerate launch --config_file accelerate_cfg/deepspeed_zero3_qwenimage_20b.yaml \
#       train_OpenSciDraw_fulltune.py configs/260126/qwenimage_fulltune_local.py

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: DEEPSPEED
downcast_bf16: 'no'
enable_cpu_affinity: false
gpu_ids: 0,1,2,3
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

deepspeed_config:
  # ZeRO Stage 3: Full parameter, gradient, and optimizer state sharding
  zero_stage: 3
  
  # ZeRO-3 specific options
  zero3_init_flag: true
  zero3_save_16bit_model: true
  
  # Gradient settings
  gradient_accumulation_steps: 4
  gradient_clipping: 1.0
  
  # Batch size per GPU (actual batch size)
  train_micro_batch_size_per_gpu: 1
  
  # CPU Offloading: Critical for 20B model on 4x A100
  # Offload optimizer states to CPU to free GPU memory
  offload_optimizer_device: cpu
  offload_optimizer_pin_memory: true
  
  # Offload parameters to CPU (optional, use if OOM with optimizer offload alone)
  # This trades speed for memory - enable only if needed
  offload_param_device: none  # Set to 'cpu' if OOM
  
  # Communication optimization
  # reduce_bucket_size: 500000000  # 500MB for gradient reduction
  # stage3_prefetch_bucket_size: 200000000  # 200MB for prefetching
  # stage3_param_persistence_threshold: 100000  # Keep small params in GPU
